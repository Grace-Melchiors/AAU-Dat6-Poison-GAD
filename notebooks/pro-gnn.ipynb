{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/DSE-MSU/DeepRobust/blob/master/deeprobust/graph/utils.py\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.sparse as ts\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    \"\"\"Return accuracy of output compared to labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output : torch.Tensor\n",
    "        output from model\n",
    "    labels : torch.Tensor or numpy.array\n",
    "        node labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        accuracy\n",
    "    \"\"\"\n",
    "    if not hasattr(labels, '__len__'):\n",
    "        labels = [labels]\n",
    "    if type(labels) is not torch.Tensor:\n",
    "        labels = torch.LongTensor(labels)\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/DSE-MSU/DeepRobust/blob/master/deeprobust/graph/defense/pgd.py\n",
    "\n",
    "from torch.optim.sgd import SGD\n",
    "from torch.optim.optimizer import required\n",
    "from torch.optim import Optimizer\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "class PGD(Optimizer):\n",
    "    \"\"\"Proximal gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : iterable\n",
    "        iterable of parameters to optimize or dicts defining parameter groups\n",
    "    proxs : iterable\n",
    "        iterable of proximal operators\n",
    "    alpha : iterable\n",
    "        iterable of coefficients for proximal gradient descent\n",
    "    lr : float\n",
    "        learning rate\n",
    "    momentum : float\n",
    "        momentum factor (default: 0)\n",
    "    weight_decay : float\n",
    "        weight decay (L2 penalty) (default: 0)\n",
    "    dampening : float\n",
    "        dampening for momentum (default: 0)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, proxs, alphas, lr=required, momentum=0, dampening=0, weight_decay=0):\n",
    "        defaults = dict(lr=lr, momentum=0, dampening=0,\n",
    "                        weight_decay=0, nesterov=False)\n",
    "        \n",
    "        #Added such that it can be accessed in setstate, which previously gave errors\n",
    "        self.proxs = proxs\n",
    "        self.alphas = alphas\n",
    "        #End of added\n",
    "\n",
    "        super(PGD, self).__init__(params, defaults)\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('proxs', proxs)\n",
    "            group.setdefault('alphas', alphas)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(PGD, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('nesterov', False)\n",
    "            group.setdefault('proxs', self.proxs)\n",
    "            group.setdefault('alphas', self.alphas)\n",
    "\n",
    "    def step(self, delta=0, closure=None):\n",
    "         for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            dampening = group['dampening']\n",
    "            nesterov = group['nesterov']\n",
    "            proxs = group['proxs']\n",
    "            alphas = group['alphas']\n",
    "\n",
    "            # apply the proximal operator to each parameter in a group\n",
    "            for param in group['params']:\n",
    "                for prox_operator, alpha in zip(proxs, alphas):\n",
    "                    # param.data.add_(lr, -param.grad.data)\n",
    "                    # param.data.add_(delta)\n",
    "                    param.data = prox_operator(param.data, alpha=alpha*lr)\n",
    "\n",
    "\n",
    "class ProxOperators():\n",
    "    \"\"\"Proximal Operators.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nuclear_norm = None\n",
    "\n",
    "    def prox_l1(self, data, alpha):\n",
    "        \"\"\"Proximal operator for l1 norm.\n",
    "        \"\"\"\n",
    "        data = torch.mul(torch.sign(data), torch.clamp(torch.abs(data)-alpha, min=0))\n",
    "        return data\n",
    "\n",
    "    def prox_nuclear(self, data, alpha):\n",
    "        \"\"\"Proximal operator for nuclear norm (trace norm).\n",
    "        \"\"\"\n",
    "        device = data.device\n",
    "        U, S, V = np.linalg.svd(data.cpu())\n",
    "        U, S, V = torch.FloatTensor(U).to(device), torch.FloatTensor(S).to(device), torch.FloatTensor(V).to(device)\n",
    "        self.nuclear_norm = S.sum()\n",
    "        # print(\"nuclear norm: %.4f\" % self.nuclear_norm)\n",
    "\n",
    "        diag_S = torch.diag(torch.clamp(S-alpha, min=0))\n",
    "        return torch.matmul(torch.matmul(U, diag_S), V)\n",
    "\n",
    "    def prox_nuclear_truncated_2(self, data, alpha, k=50):\n",
    "        device = data.device\n",
    "        import tensorly as tl\n",
    "        tl.set_backend('pytorch')\n",
    "        U, S, V = tl.truncated_svd(data.cpu(), n_eigenvecs=k)\n",
    "        U, S, V = torch.FloatTensor(U).to(device), torch.FloatTensor(S).to(device), torch.FloatTensor(V).to(device)\n",
    "        self.nuclear_norm = S.sum()\n",
    "        # print(\"nuclear norm: %.4f\" % self.nuclear_norm)\n",
    "\n",
    "        S = torch.clamp(S-alpha, min=0)\n",
    "\n",
    "        # diag_S = torch.diag(torch.clamp(S-alpha, min=0))\n",
    "        # U = torch.spmm(U, diag_S)\n",
    "        # V = torch.matmul(U, V)\n",
    "\n",
    "        # make diag_S sparse matrix\n",
    "        indices = torch.tensor((range(0, len(S)), range(0, len(S)))).to(device)\n",
    "        values = S\n",
    "        diag_S = torch.sparse.FloatTensor(indices, values, torch.Size((len(S), len(S))))\n",
    "        V = torch.spmm(diag_S, V)\n",
    "        V = torch.matmul(U, V)\n",
    "        return V\n",
    "\n",
    "    def prox_nuclear_truncated(self, data, alpha, k=50):\n",
    "        device = data.device\n",
    "        indices = torch.nonzero(data).t()\n",
    "        values = data[indices[0], indices[1]] # modify this based on dimensionality\n",
    "        data_sparse = sp.csr_matrix((values.cpu().numpy(), indices.cpu().numpy()))\n",
    "        U, S, V = sp.linalg.svds(data_sparse, k=k)\n",
    "        U, S, V = torch.FloatTensor(U).to(device), torch.FloatTensor(S).to(device), torch.FloatTensor(V).to(device)\n",
    "        self.nuclear_norm = S.sum()\n",
    "        diag_S = torch.diag(torch.clamp(S-alpha, min=0))\n",
    "        return torch.matmul(torch.matmul(U, diag_S), V)\n",
    "\n",
    "    def prox_nuclear_cuda(self, data, alpha):\n",
    "\n",
    "        device = data.device\n",
    "        U, S, V = torch.svd(data)\n",
    "        # self.nuclear_norm = S.sum()\n",
    "        # print(f\"rank = {len(S.nonzero())}\")\n",
    "        self.nuclear_norm = S.sum()\n",
    "        S = torch.clamp(S-alpha, min=0)\n",
    "        indices = torch.tensor([range(0, U.shape[0]),range(0, U.shape[0])]).to(device)\n",
    "        values = S\n",
    "        diag_S = torch.sparse.FloatTensor(indices, values, torch.Size(U.shape))\n",
    "        # diag_S = torch.diag(torch.clamp(S-alpha, min=0))\n",
    "        # print(f\"rank_after = {len(diag_S.nonzero())}\")\n",
    "        V = torch.spmm(diag_S, V.t_())\n",
    "        V = torch.matmul(U, V)\n",
    "        return V\n",
    "\n",
    "prox_operators = ProxOperators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/DSE-MSU/DeepRobust/blob/master/deeprobust/graph/defense/prognn.py\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "from typing import Tuple\n",
    "\n",
    "def loss_func(adj: torch.Tensor, A_hat: torch.Tensor, attrs: torch.Tensor, X_hat: torch.Tensor, alpha: float) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    diff_attribute = torch.pow(X_hat - attrs, 2)\n",
    "    attribute_reconstruction_errors = torch.sqrt(torch.sum(diff_attribute, 1))\n",
    "    attribute_cost = torch.mean(attribute_reconstruction_errors)\n",
    "\n",
    "    diff_structure = torch.pow(A_hat - adj, 2)\n",
    "    structure_reconstruction_errors = torch.sqrt(torch.sum(diff_structure, 1))\n",
    "    structure_cost = torch.mean(structure_reconstruction_errors)\n",
    "\n",
    "    cost = alpha * attribute_reconstruction_errors + (1 - alpha) * structure_reconstruction_errors\n",
    "    return cost, structure_cost, attribute_cost\n",
    "\n",
    "class ProGNN:\n",
    "    \"\"\" ProGNN (Properties Graph Neural Network). See more details in Graph Structure Learning for Robust Graph Neural Networks, KDD 2020, https://arxiv.org/abs/2005.10203.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model:\n",
    "        model: The backbone GNN model in ProGNN\n",
    "    args:\n",
    "        model configs\n",
    "    device: str\n",
    "        'cpu' or 'cuda'.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    See details in https://github.com/ChandlerBang/Pro-GNN.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, args, device):\n",
    "        self.device = device\n",
    "        self.args = args\n",
    "        self.best_val_acc = 0\n",
    "        self.best_val_loss = 10\n",
    "        self.best_graph = None\n",
    "        self.weights = None\n",
    "        self.estimator = None\n",
    "        self.model = model.to(device)\n",
    "\n",
    "    def fit(self, features, adj, labels, top_k = 10, **kwargs):\n",
    "        \"\"\"Train Pro-GNN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        features :\n",
    "            node features\n",
    "        adj :\n",
    "            the adjacency matrix. The format is torch.tensor\n",
    "        labels :\n",
    "            node labels\n",
    "        top_k :\n",
    "            the number of top k nodes to get\n",
    "        \"\"\"\n",
    "        args = self.args\n",
    "\n",
    "        # self.optimizer = optim.Adam(self.model.parameters(),\n",
    "        #                        lr=args.lr, weight_decay=args.weight_decay)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=args.lr) #Our other versions do not use weight decay\n",
    "\n",
    "\n",
    "        estimator = EstimateAdj(adj, symmetric=args.symmetric, device=self.device).to(self.device)\n",
    "        self.estimator = estimator\n",
    "        self.optimizer_adj = optim.SGD(estimator.parameters(),\n",
    "                              momentum=0.9, lr=args.lr_adj)\n",
    "\n",
    "        self.optimizer_l1 = PGD(estimator.parameters(),\n",
    "                        proxs=[prox_operators.prox_l1],\n",
    "                        lr=args.lr_adj, alphas=[args.alpha])\n",
    "\n",
    "\n",
    "        # warnings.warn(\"If you find the nuclear proximal operator runs too slow on Pubmed, you can  uncomment line 67-71 and use prox_nuclear_cuda to perform the proximal on gpu.\")\n",
    "        # if args.dataset == \"pubmed\":\n",
    "        #     self.optimizer_nuclear = PGD(estimator.parameters(),\n",
    "        #               proxs=[prox_operators.prox_nuclear_cuda],\n",
    "        #               lr=args.lr_adj, alphas=[args.beta])\n",
    "        # else:\n",
    "        warnings.warn(\"If you find the nuclear proximal operator runs too slow, you can modify line 77 to use prox_operators.prox_nuclear_cuda instead of prox_operators.prox_nuclear to perform the proximal on GPU. See details in https://github.com/ChandlerBang/Pro-GNN/issues/1\")\n",
    "        self.optimizer_nuclear = PGD(estimator.parameters(),\n",
    "                  proxs=[prox_operators.prox_nuclear],\n",
    "                  lr=args.lr_adj, alphas=[args.beta])\n",
    "\n",
    "        # Train model\n",
    "        t_total = time.time()\n",
    "        for epoch in range(args.epochs):\n",
    "            if args.only_gcn:\n",
    "                self.train_DOMINANT(epoch, estimator.estimated_adj)\n",
    "                # self.train_gcn(epoch, features, estimator.estimated_adj,\n",
    "                #         labels, idx_train, idx_val)\n",
    "            else:\n",
    "                for i in range(int(args.outer_steps)):\n",
    "                    self.train_adj(epoch, features, adj)\n",
    "\n",
    "                for i in range(int(args.inner_steps)):\n",
    "                    self.train_DOMINANT(epoch, estimator.estimated_adj)\n",
    "                    # self.train_gcn(epoch, features, estimator.estimated_adj,\n",
    "                    #         labels, idx_train, idx_val)\n",
    "\n",
    "        # Get evals from DOMINANT model\n",
    "        self.eval()\n",
    "        A_hat, X_hat = self.forward(self.attrs, self.edge_index)\n",
    "        loss, struct_loss, feat_loss = loss_func(self.adj_label, A_hat, self.attrs, X_hat, args.alpha)\n",
    "        self.score = loss.detach().cpu().numpy()\n",
    "\n",
    "        # Identify and store the IDs of the nodes with the top K highest anomaly scores\n",
    "        \n",
    "        # Convert the anomaly scores to a PyTorch tensor\n",
    "        scores_tensor = torch.tensor(self.score)\n",
    "        # Use torch.topk to find the top K scores and their indices\n",
    "        topk_scores, topk_indices = torch.topk(scores_tensor, top_k, largest=True)\n",
    "        # Convert the indices and scores to lists and store them\n",
    "        top_k_AS_indices = topk_indices.tolist()\n",
    "        top_k_AS_scores = topk_scores.tolist()\n",
    "        self.model.top_k_AS = top_k_AS_indices\n",
    "        # Print the node IDs and their corresponding anomaly scores\n",
    "        if args.debug:\n",
    "            print(f\"Top {top_k} highest anomaly scores' node IDs and scores:\")\n",
    "            for idx, score in zip(top_k_AS_indices, top_k_AS_scores):\n",
    "                print(f\"Node ID: {idx}, Anomaly Score: {score}\")\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "        print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "        print(args)\n",
    "\n",
    "        # Testing\n",
    "        # print(\"picking the best model according to validation performance\")\n",
    "        # self.model.load_state_dict(self.weights)\n",
    "\n",
    "\n",
    "    def train_DOMINANT(self, epoch, estimated_adj):\n",
    "        args = self.args\n",
    "        if args.debug:\n",
    "            print(\"\\n=== train_model ===\")\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        edge_index = estimated_adj.clone().detach().to_sparse()\n",
    "\n",
    "        A_hat, X_hat = self.model.forward(self.model.attrs, edge_index)\n",
    "        loss, struct_loss, feat_loss = loss_func(estimated_adj, A_hat, self.model.attrs, X_hat, args.alpha)\n",
    "        loss = torch.mean(loss)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        if args.debug:\n",
    "            print(f\"Epoch: {epoch:04d}, train_loss={loss.item():.5f}, \"\n",
    "                f\"train/struct_loss={struct_loss.item():.5f}, train/feat_loss={feat_loss.item():.5f}\")\n",
    "\n",
    "        \n",
    "    def train_gcn(self, epoch, features, adj, labels, idx_train, idx_val):\n",
    "        args = self.args\n",
    "        estimator = self.estimator\n",
    "        adj = estimator.normalize()\n",
    "\n",
    "        t = time.time()\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        output = self.model(features, adj)\n",
    "        loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "        acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "        loss_train.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        self.model.eval()\n",
    "        output = self.model(features, adj)\n",
    "\n",
    "        loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "        acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "\n",
    "        if acc_val > self.best_val_acc:\n",
    "            self.best_val_acc = acc_val\n",
    "            self.best_graph = adj.detach()\n",
    "            self.weights = deepcopy(self.model.state_dict())\n",
    "            if args.debug:\n",
    "                print('\\t=== saving current graph/gcn, best_val_acc: %s' % self.best_val_acc.item())\n",
    "\n",
    "        if loss_val < self.best_val_loss:\n",
    "            self.best_val_loss = loss_val\n",
    "            self.best_graph = adj.detach()\n",
    "            self.weights = deepcopy(self.model.state_dict())\n",
    "            if args.debug:\n",
    "                print(f'\\t=== saving current graph/gcn, best_val_loss: %s' % self.best_val_loss.item())\n",
    "\n",
    "        if args.debug:\n",
    "            if epoch % 1 == 0:\n",
    "                print('Epoch: {:04d}'.format(epoch+1),\n",
    "                      'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                      'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                      'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                      'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                      'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "\n",
    "    def train_adj(self, epoch, features, adj):\n",
    "        estimator = self.estimator\n",
    "        args = self.args\n",
    "        if args.debug:\n",
    "            print(\"\\n=== train_adj ===\")\n",
    "        t = time.time()\n",
    "        estimator.train()\n",
    "        self.optimizer_adj.zero_grad()\n",
    "\n",
    "        loss_l1 = torch.norm(estimator.estimated_adj, 1)\n",
    "        loss_fro = torch.norm(estimator.estimated_adj - adj, p='fro')\n",
    "        normalized_adj = estimator.normalize()\n",
    "\n",
    "        if args.lambda_:\n",
    "            loss_smooth_feat = self.feature_smoothing(estimator.estimated_adj, features)\n",
    "        else:\n",
    "            loss_smooth_feat = 0 * loss_l1\n",
    "\n",
    "        # output = self.model(features, normalized_adj) #Forward func\n",
    "        # loss_gcn = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "        # acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "\n",
    "        edge_index = normalized_adj.clone().detach().to_sparse()\n",
    "\n",
    "        A_hat, X_hat = self.model.forward(self.model.attrs, edge_index)\n",
    "        loss, _, _ = loss_func(self.model.adj_label, A_hat, self.model.attrs, X_hat, args.alpha)\n",
    "        loss_gcn = torch.mean(loss)\n",
    "\n",
    "\n",
    "        loss_symmetric = torch.norm(estimator.estimated_adj \\\n",
    "                        - estimator.estimated_adj.t(), p=\"fro\")\n",
    "\n",
    "        loss_diffiential =  loss_fro + args.gamma * loss_gcn + args.lambda_ * loss_smooth_feat + args.phi * loss_symmetric\n",
    "\n",
    "        loss_diffiential.backward()\n",
    "\n",
    "        self.optimizer_adj.step()\n",
    "        loss_nuclear =  0 * loss_fro\n",
    "        if args.beta != 0:\n",
    "            self.optimizer_nuclear.zero_grad()\n",
    "            self.optimizer_nuclear.step()\n",
    "            loss_nuclear = prox_operators.nuclear_norm\n",
    "\n",
    "        self.optimizer_l1.zero_grad()\n",
    "        self.optimizer_l1.step()\n",
    "\n",
    "        total_loss = loss_fro \\\n",
    "                    + args.gamma * loss_gcn \\\n",
    "                    + args.alpha * loss_l1 \\\n",
    "                    + args.beta * loss_nuclear \\\n",
    "                    + args.phi * loss_symmetric\n",
    "\n",
    "        estimator.estimated_adj.data.copy_(torch.clamp(\n",
    "                  estimator.estimated_adj.data, min=0, max=1))\n",
    "\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "\n",
    "        if args.debug:\n",
    "            if epoch % 1 == 0:\n",
    "                print('Epoch: {:04d}'.format(epoch+1),\n",
    "                      'loss_fro: {:.4f}'.format(loss_fro.item()),\n",
    "                      'loss_gcn: {:.4f}'.format(loss_gcn.item()),\n",
    "                      'loss_feat: {:.4f}'.format(loss_smooth_feat.item()),\n",
    "                      'loss_symmetric: {:.4f}'.format(loss_symmetric.item()),\n",
    "                      'delta_l1_norm: {:.4f}'.format(torch.norm(estimator.estimated_adj-adj, 1).item()),\n",
    "                      'loss_l1: {:.4f}'.format(loss_l1.item()),\n",
    "                      'loss_total: {:.4f}'.format(total_loss.item()),\n",
    "                      'loss_nuclear: {:.4f}'.format(loss_nuclear.item()))\n",
    "\n",
    "\n",
    "    def test(self, features, labels, idx_test):\n",
    "        \"\"\"Evaluate the performance of ProGNN on test set\n",
    "        \"\"\"\n",
    "        print(\"\\t=== testing ===\")\n",
    "        self.model.eval()\n",
    "        adj = self.best_graph\n",
    "        if self.best_graph is None:\n",
    "            adj = self.estimator.normalize()\n",
    "        output = self.model(features, adj)\n",
    "        loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "        acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "        print(\"\\tTest set results:\",\n",
    "              \"loss= {:.4f}\".format(loss_test.item()),\n",
    "              \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "        return acc_test.item()\n",
    "\n",
    "    def feature_smoothing(self, adj, X):\n",
    "        adj = (adj.t() + adj)/2\n",
    "        rowsum = adj.sum(1)\n",
    "        r_inv = rowsum.flatten()\n",
    "        D = torch.diag(r_inv)\n",
    "        L = D - adj\n",
    "\n",
    "        r_inv = r_inv  + 1e-3\n",
    "        r_inv = r_inv.pow(-1/2).flatten()\n",
    "        r_inv[torch.isinf(r_inv)] = 0.\n",
    "        r_mat_inv = torch.diag(r_inv)\n",
    "        # L = r_mat_inv @ L\n",
    "        L = r_mat_inv @ L @ r_mat_inv\n",
    "\n",
    "        XLXT = torch.matmul(torch.matmul(X.t(), L), X)\n",
    "        loss_smooth_feat = torch.trace(XLXT)\n",
    "        return loss_smooth_feat\n",
    "\n",
    "\n",
    "class EstimateAdj(nn.Module):\n",
    "    \"\"\"Provide a pytorch parameter matrix for estimated\n",
    "    adjacency matrix and corresponding operations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, adj, symmetric=False, device='cpu'):\n",
    "        super(EstimateAdj, self).__init__()\n",
    "        n = len(adj)\n",
    "        self.estimated_adj = nn.Parameter(torch.FloatTensor(n, n))\n",
    "        self._init_estimation(adj)\n",
    "        self.symmetric = symmetric\n",
    "        self.device = device\n",
    "\n",
    "    def _init_estimation(self, adj):\n",
    "        with torch.no_grad():\n",
    "            n = len(adj)\n",
    "            self.estimated_adj.data.copy_(adj)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.estimated_adj\n",
    "\n",
    "    def normalize(self):\n",
    "\n",
    "        if self.symmetric:\n",
    "            adj = (self.estimated_adj + self.estimated_adj.t())/2\n",
    "        else:\n",
    "            adj = self.estimated_adj\n",
    "\n",
    "        normalized_adj = self._normalize(adj + torch.eye(adj.shape[0]).to(self.device))\n",
    "        return normalized_adj\n",
    "\n",
    "    def _normalize(self, mx):\n",
    "        rowsum = mx.sum(1)\n",
    "        r_inv = rowsum.pow(-1/2).flatten()\n",
    "        r_inv[torch.isinf(r_inv)] = 0.\n",
    "        r_mat_inv = torch.diag(r_inv)\n",
    "        mx = r_mat_inv @ mx\n",
    "        mx = mx @ r_mat_inv\n",
    "        return mx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/DSE-MSU/DeepRobust/blob/master/deeprobust/graph/utils.py\n",
    "\n",
    "\n",
    "def get_train_val_test_gcn(labels, seed=None):\n",
    "    \"\"\"This setting follows gcn, where we randomly sample 20 instances for each class\n",
    "    as training data, 500 instances as validation data, 1000 instances as test data.\n",
    "    Note here we are not using fixed splits. When random seed changes, the splits\n",
    "    will also change.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : numpy.array\n",
    "        node labels\n",
    "    seed : int or None\n",
    "        random seed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    idx_train :\n",
    "        node training indices\n",
    "    idx_val :\n",
    "        node validation indices\n",
    "    idx_test :\n",
    "        node test indices\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    idx = np.arange(len(labels))\n",
    "    nclass = labels.max() + 1\n",
    "    idx_train = []\n",
    "    idx_unlabeled = []\n",
    "    for i in range(nclass):\n",
    "        labels_i = idx[labels==i]\n",
    "        labels_i = np.random.permutation(labels_i)\n",
    "        idx_train = np.hstack((idx_train, labels_i[: 20])).astype(np.int)\n",
    "        idx_unlabeled = np.hstack((idx_unlabeled, labels_i[20: ])).astype(np.int)\n",
    "\n",
    "    idx_unlabeled = np.random.permutation(idx_unlabeled)\n",
    "    idx_val = idx_unlabeled[: 500]\n",
    "    idx_test = idx_unlabeled[500: 1500]\n",
    "    return idx_train, idx_val, idx_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gey adj from https://github.com/DSE-MSU/DeepRobust/blob/master/deeprobust/graph/data/dataset.py#L221\n",
    "\n",
    "def load_npz(self, file_name, is_sparse=True):\n",
    "    with np.load(file_name) as loader:\n",
    "        # loader = dict(loader)\n",
    "        if is_sparse:\n",
    "            adj = sp.csr_matrix((loader['adj_data'], loader['adj_indices'],\n",
    "                                        loader['adj_indptr']), shape=loader['adj_shape'])\n",
    "            if 'attr_data' in loader:\n",
    "                features = sp.csr_matrix((loader['attr_data'], loader['attr_indices'],\n",
    "                                                loader['attr_indptr']), shape=loader['attr_shape'])\n",
    "            else:\n",
    "                features = None\n",
    "            labels = loader.get('labels')\n",
    "        else:\n",
    "            adj = loader['adj_data']\n",
    "            if 'attr_data' in loader:\n",
    "                features = loader['attr_data']\n",
    "            else:\n",
    "                features = None\n",
    "            labels = loader.get('labels')\n",
    "    if features is None:\n",
    "        features = np.eye(adj.shape[0])\n",
    "    features = sp.csr_matrix(features, dtype=np.float32)\n",
    "    return adj, features, labels\n",
    "\n",
    "\n",
    "def get_adj(self):\n",
    "    adj, features, labels = load_npz(data_filename)\n",
    "    adj = adj + adj.T\n",
    "    adj = adj.tolil()\n",
    "    adj[adj > 1] = 1\n",
    "\n",
    "    if self.require_lcc:\n",
    "        lcc = self.largest_connected_components(adj)\n",
    "        adj = adj[lcc][:, lcc]\n",
    "        features = features[lcc]\n",
    "        labels = labels[lcc]\n",
    "        assert adj.sum(0).A1.min() > 0, \"Graph contains singleton nodes\"\n",
    "\n",
    "    # whether to set diag=0?\n",
    "    adj.setdiag(0)\n",
    "    adj = adj.astype(\"float32\").tocsr()\n",
    "    adj.eliminate_zeros()\n",
    "\n",
    "    assert np.abs(adj - adj.T).sum() == 0, \"Input graph is not symmetric\"\n",
    "    assert adj.max() == 1 and len(np.unique(adj[adj.nonzero()].A1)) == 1, \"Graph must be unweighted\"\n",
    "\n",
    "    return adj, features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.,  ..., 1., 1., 1.])\n",
      "node_attrs\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "adj\n",
      "SparseTensor(row=tensor([   0,    0,    0,  ..., 2707, 2707, 2707]),\n",
      "             col=tensor([   8,  435,  544,  ...,  774, 1389, 2344]),\n",
      "             val=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
      "             size=(2708, 2708), nnz=6268, density=0.09%)\n",
      "labels\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "#Insert own dataset here\n",
    "\n",
    "from pygod.generator import gen_contextual_outlier, gen_structural_outlier\n",
    "from torch_geometric.datasets import AttributedGraphDataset\n",
    "from typing import List\n",
    "\n",
    "string = \"Cora\"\n",
    "#string = \"Wiki\"\n",
    "#string = \"Facebook\"\n",
    "\n",
    "dataset = AttributedGraphDataset(root = \"data/\"+string, name = string)\n",
    "\n",
    "data = dataset[0]\n",
    "\n",
    "\n",
    "amount_of_nodes = data.num_nodes\n",
    "\n",
    "seed = 123\n",
    "num_nodes_to_inject = round(amount_of_nodes * 0.05)\n",
    "num_nodes_per_clique = 15\n",
    "num_cliques = round((num_nodes_to_inject / 2) / num_nodes_per_clique)\n",
    "num_contextual_outliers = num_nodes_to_inject - num_cliques * num_nodes_per_clique\n",
    "\n",
    "data, ya = gen_contextual_outlier(data, n = num_contextual_outliers, k = 50, seed = seed) \n",
    "#n (int) – Number of nodes converting to outliers.\n",
    "#k (int) – Number of candidate nodes for each outlier node.\n",
    "\n",
    "data, ys = gen_structural_outlier(data, m = num_nodes_per_clique, n = num_cliques, seed = seed)\n",
    "#m (int) - Number nodes in the outlier cliques.\n",
    "#n (int) - Number of outlier clique\n",
    "data_classes = data.y\n",
    "\n",
    "data.y = torch.logical_or(ys, ya).long()\n",
    "y_binary: List[int] = data.y.bool()\n",
    "anomaly_list = np.where(y_binary == True)[0]  # Used for list for which nodes to hide\n",
    "\n",
    "\n",
    "\n",
    "from gad_adversarial_robustness.utils.graph_utils import prepare_graph\n",
    "\n",
    "amount_of_nodes = data.num_nodes\n",
    "labels = data_classes\n",
    "features = data.x\n",
    "_, adj, _ = prepare_graph(data) #Get adjacency matrix\n",
    "\n",
    "\n",
    "# adj, features, labels = data.adj, data.features, data.labels\n",
    "# idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1efd1552210>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Based on https://github.com/ChandlerBang/Pro-GNN/blob/master/train.py\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "import os\n",
    "#from torch_sparse import SparseTensor\n",
    "\n",
    "from gad_adversarial_robustness.gad.dominant.dominant import Dominant \n",
    "from gad_adversarial_robustness.utils.graph_utils import load_anomaly_detection_dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_torch_sparse_tensor\n",
    "\n",
    "script_dir = os.path.abspath('')\n",
    "yaml_path = os.path.join(script_dir, '..', 'configs', 'dominant_config.yaml')\n",
    "with open(yaml_path) as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "dataset_caching_path = os.path.join(script_dir, '..', '..', '..', 'data')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    config['model']['device'] = 'cuda'\n",
    "else:\n",
    "    config['model']['device'] = 'cpu'\n",
    "\n",
    "\n",
    "# Training settings\n",
    "class Args: #Has comments from what pro-gnn recommends for their own GCN system\n",
    "    def __init__(self, config):\n",
    "        self.debug = False\n",
    "        self.only_gcn = False\n",
    "        self.no_cuda = False\n",
    "        self.seed = 123\n",
    "        self.lr = config['model']['lr'] #0.01\n",
    "        self.weight_decay = 5e-4\n",
    "        self.hidden = 16\n",
    "        self.dropout = config['model']['dropout'] # 0.5\n",
    "        self.dataset = 'cora'\n",
    "        self.attack = 'meta'\n",
    "        self.ptb_rate = 0.05\n",
    "        self.epochs = config['model']['epochs'] #400\n",
    "        self.alpha = config['model']['alpha'] #5e-4\n",
    "        self.beta = 1.5\n",
    "        self.gamma = 1\n",
    "        self.lambda_ = 0\n",
    "        self.phi = 0\n",
    "        self.inner_steps = 2\n",
    "        self.outer_steps = 1\n",
    "        self.lr_adj = 0.01\n",
    "        self.symmetric = False\n",
    "\n",
    "args = Args(config)\n",
    "\n",
    "\n",
    "\n",
    "cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "if args.ptb_rate == 0:\n",
    "    attack = \"no\"\n",
    "\n",
    "\n",
    "# # Here the random seed is to split the train/val/test data,\n",
    "# # we need to set the random seed to be the same as that when you generate the perturbed graph\n",
    "# # but now change the setting from nettack to prognn which directly loads the prognn splits\n",
    "# # data = Dataset(root='/tmp/', name=args.dataset, setting='nettack', seed=15)\n",
    "# data = Dataset(root='/tmp/', name=args.dataset, setting='prognn')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make model\n",
    "\n",
    "adj, _, _, adj_label = load_anomaly_detection_dataset(data, config['model']['device'])\n",
    "\n",
    "#edge_index = torch.LongTensor(np.array(sp.coo_matrix(adj).nonzero()))\n",
    "adj_label = torch.FloatTensor(adj_label).to(config['model']['device'])\n",
    "#attrs = torch.FloatTensor(attrs)\n",
    "\n",
    "edge_index = dataset.edge_index.to(config['model']['device'])\n",
    "label = torch.Tensor(dataset.y.bool()).to(config['model']['device'])\n",
    "attrs = dataset.x.to(config['model']['device'])\n",
    "\n",
    "\n",
    "sparse_adj = to_torch_sparse_tensor(edge_index)\n",
    "\n",
    "dom_model = Dominant(feat_size=attrs.size(1), hidden_size=config['model']['hidden_dim'], dropout=config['model']['dropout'],\n",
    "                device=config['model']['device'], edge_index=sparse_adj, adj_label=adj_label, attrs=attrs, label=label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2500, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2500]])\n",
      "tensor(indices=tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
      "                       [   8,  435,  544,  ...,  774, 1389, 2344]]),\n",
      "       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
      "       size=(2708, 2708), nnz=5429, layout=torch.sparse_coo)\n",
      "torch.Size([2708, 2708])\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "\n",
    "adj = torch.Tensor(adj)\n",
    "\n",
    "print(adj)\n",
    "print(sparse_adj)\n",
    "#print(torch.tensor(adj))\n",
    "\n",
    "print(adj.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_2656\\396386561.py:91: UserWarning: If you find the nuclear proximal operator runs too slow, you can modify line 77 to use prox_operators.prox_nuclear_cuda instead of prox_operators.prox_nuclear to perform the proximal on GPU. See details in https://github.com/ChandlerBang/Pro-GNN/issues/1\n",
      "  warnings.warn(\"If you find the nuclear proximal operator runs too slow, you can modify line 77 to use prox_operators.prox_nuclear_cuda instead of prox_operators.prox_nuclear to perform the proximal on GPU. See details in https://github.com/ChandlerBang/Pro-GNN/issues/1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0001 loss_fro: 0.0000 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 37.2644 delta_l1_norm: 116.8474 loss_l1: 3406.4385 loss_total: nan loss_nuclear: 1920.1359\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0000, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0000, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0002 loss_fro: 1.3000 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 36.2567 delta_l1_norm: 232.7221 loss_l1: 3289.6260 loss_total: nan loss_nuclear: 1862.4781\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0001, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0001, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0003 loss_fro: 2.5889 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 35.2618 delta_l1_norm: 347.5591 loss_l1: 3173.7837 loss_total: nan loss_nuclear: 1805.3531\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0002, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0002, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0004 loss_fro: 3.8666 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 34.2792 delta_l1_norm: 461.2324 loss_l1: 3058.9741 loss_total: nan loss_nuclear: 1748.7937\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0003, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0003, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0005 loss_fro: 5.1327 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 33.3085 delta_l1_norm: 573.6451 loss_l1: 2945.3113 loss_total: nan loss_nuclear: 1692.8306\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0004, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0004, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0006 loss_fro: 6.3862 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 32.3494 delta_l1_norm: 684.6617 loss_l1: 2832.9360 loss_total: nan loss_nuclear: 1637.4937\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0005, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0005, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0007 loss_fro: 7.6269 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 31.4016 delta_l1_norm: 793.7583 loss_l1: 2721.9541 loss_total: nan loss_nuclear: 1582.8024\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0006, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0006, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0008 loss_fro: 8.8517 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 30.4651 delta_l1_norm: 899.4746 loss_l1: 2612.8994 loss_total: nan loss_nuclear: 1528.8337\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0007, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0007, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0009 loss_fro: 10.0518 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 29.5397 delta_l1_norm: 999.1924 loss_l1: 2507.2307 loss_total: nan loss_nuclear: 1475.6885\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0008, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0008, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0010 loss_fro: 11.2129 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 28.6254 delta_l1_norm: 1096.5458 loss_l1: 2407.5686 loss_total: nan loss_nuclear: 1423.5963\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0009, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0009, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0011 loss_fro: 12.3576 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 27.7221 delta_l1_norm: 1191.7156 loss_l1: 2310.2646 loss_total: nan loss_nuclear: 1372.4263\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0010, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0010, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0012 loss_fro: 13.4864 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 26.8301 delta_l1_norm: 1284.6217 loss_l1: 2215.1265 loss_total: nan loss_nuclear: 1322.1134\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0011, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0011, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0013 loss_fro: 14.5985 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 25.9491 delta_l1_norm: 1375.7812 loss_l1: 2122.2937 loss_total: nan loss_nuclear: 1272.6359\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0012, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0012, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0014 loss_fro: 15.6977 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 25.0792 delta_l1_norm: 1464.6904 loss_l1: 2031.2473 loss_total: nan loss_nuclear: 1223.9343\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0013, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0013, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0015 loss_fro: 16.7768 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 24.2204 delta_l1_norm: 1551.2755 loss_l1: 1942.4469 loss_total: nan loss_nuclear: 1176.3563\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0014, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0014, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0016 loss_fro: 17.8350 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 23.3727 delta_l1_norm: 1635.6226 loss_l1: 1855.9998 loss_total: nan loss_nuclear: 1129.9263\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0015, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0015, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0017 loss_fro: 18.8764 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 22.5360 delta_l1_norm: 1717.9529 loss_l1: 1771.8252 loss_total: nan loss_nuclear: 1084.3188\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0016, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0016, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0018 loss_fro: 19.9037 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 21.7102 delta_l1_norm: 1797.7026 loss_l1: 1689.6777 loss_total: nan loss_nuclear: 1039.4211\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0017, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0017, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0019 loss_fro: 20.9078 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 20.8964 delta_l1_norm: 1874.5837 loss_l1: 1609.9453 loss_total: nan loss_nuclear: 995.7770\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0018, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0018, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0020 loss_fro: 21.8816 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 20.0931 delta_l1_norm: 1949.2756 loss_l1: 1533.1378 loss_total: nan loss_nuclear: 953.6979\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0019, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0019, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0021 loss_fro: 22.8356 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 19.3002 delta_l1_norm: 2021.6852 loss_l1: 1458.6028 loss_total: nan loss_nuclear: 912.6661\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0020, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0020, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0022 loss_fro: 23.7710 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 18.5187 delta_l1_norm: 2091.8079 loss_l1: 1386.3475 loss_total: nan loss_nuclear: 872.5576\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0021, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0021, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0023 loss_fro: 24.6882 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 17.7486 delta_l1_norm: 2160.0222 loss_l1: 1316.3861 loss_total: nan loss_nuclear: 833.3079\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0022, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0022, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0024 loss_fro: 25.5912 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 16.9895 delta_l1_norm: 2226.5405 loss_l1: 1248.4053 loss_total: nan loss_nuclear: 794.7949\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0023, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0023, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0025 loss_fro: 26.4830 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 16.2408 delta_l1_norm: 2291.4622 loss_l1: 1182.1912 loss_total: nan loss_nuclear: 756.8319\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0024, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0024, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0026 loss_fro: 27.3631 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 15.5013 delta_l1_norm: 2354.5378 loss_l1: 1117.6414 loss_total: nan loss_nuclear: 719.4800\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0025, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0025, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0027 loss_fro: 28.2252 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 14.7732 delta_l1_norm: 2412.5381 loss_l1: 1054.9231 loss_total: nan loss_nuclear: 683.1851\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0026, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0026, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0028 loss_fro: 29.0275 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 14.0942 delta_l1_norm: 2468.1274 loss_l1: 995.5906 loss_total: nan loss_nuclear: 650.2440\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0027, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0027, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0029 loss_fro: 29.7968 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 13.4059 delta_l1_norm: 2521.1348 loss_l1: 939.5410 loss_total: nan loss_nuclear: 618.7380\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0028, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0028, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0030 loss_fro: 30.5382 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 12.7252 delta_l1_norm: 2571.7083 loss_l1: 886.2968 loss_total: nan loss_nuclear: 588.5265\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0029, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0029, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0031 loss_fro: 31.2564 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 12.0595 delta_l1_norm: 2620.5068 loss_l1: 835.4203 loss_total: nan loss_nuclear: 559.4050\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0030, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0030, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0032 loss_fro: 31.9600 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 11.4059 delta_l1_norm: 2667.7354 loss_l1: 786.3844 loss_total: nan loss_nuclear: 530.9453\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0031, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0031, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0033 loss_fro: 32.6506 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 10.7631 delta_l1_norm: 2713.2712 loss_l1: 739.0135 loss_total: nan loss_nuclear: 503.0836\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0032, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0032, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0034 loss_fro: 33.3269 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 10.1334 delta_l1_norm: 2757.3608 loss_l1: 693.3417 loss_total: nan loss_nuclear: 475.9896\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0033, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0033, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0035 loss_fro: 33.9919 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 9.5150 delta_l1_norm: 2800.4087 loss_l1: 649.1307 loss_total: nan loss_nuclear: 449.5370\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0034, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0034, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0036 loss_fro: 34.6507 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 8.9067 delta_l1_norm: 2842.6096 loss_l1: 606.0372 loss_total: nan loss_nuclear: 423.4507\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0035, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0035, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0037 loss_fro: 35.3061 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 8.3097 delta_l1_norm: 2883.9429 loss_l1: 563.8306 loss_total: nan loss_nuclear: 397.6189\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0036, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0036, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0038 loss_fro: 35.9581 loss_gcn: nan loss_feat: 0.0000 loss_symmetric: 7.7249 delta_l1_norm: 2924.0288 loss_l1: 522.4974 loss_total: nan loss_nuclear: 371.9798\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0037, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "False False\n",
      "False False\n",
      "False\n",
      "Epoch: 0037, train_loss=nan, train/struct_loss=nan, train/feat_loss=nan\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "False False\n",
      "False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[171], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m args\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      5\u001b[0m prognn \u001b[38;5;241m=\u001b[39m ProGNN(dom_model, args, device)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mprognn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#prognn.test(features, labels, idx_test)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[163], line 105\u001b[0m, in \u001b[0;36mProGNN.fit\u001b[1;34m(self, features, adj, labels, top_k, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# self.train_gcn(epoch, features, estimator.estimated_adj,\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;66;03m#         labels, idx_train, idx_val)\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(args\u001b[38;5;241m.\u001b[39mouter_steps)):\n\u001b[1;32m--> 105\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_adj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(args\u001b[38;5;241m.\u001b[39minner_steps)):\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_DOMINANT(epoch, estimator\u001b[38;5;241m.\u001b[39mestimated_adj)\n",
      "Cell \u001b[1;32mIn[163], line 249\u001b[0m, in \u001b[0;36mProGNN.train_adj\u001b[1;34m(self, epoch, features, adj)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mbeta \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_nuclear\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 249\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_nuclear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m     loss_nuclear \u001b[38;5;241m=\u001b[39m prox_operators\u001b[38;5;241m.\u001b[39mnuclear_norm\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_l1\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\grace\\anaconda3\\envs\\PyG2\\lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[162], line 70\u001b[0m, in \u001b[0;36mPGD.step\u001b[1;34m(self, delta, closure)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prox_operator, alpha \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(proxs, alphas):\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;66;03m# param.data.add_(lr, -param.grad.data)\u001b[39;00m\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;66;03m# param.data.add_(delta)\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m         param\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mprox_operator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[162], line 90\u001b[0m, in \u001b[0;36mProxOperators.prox_nuclear\u001b[1;34m(self, data, alpha)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Proximal operator for nuclear norm (trace norm).\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     89\u001b[0m device \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m---> 90\u001b[0m U, S, V \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m U, S, V \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(U)\u001b[38;5;241m.\u001b[39mto(device), torch\u001b[38;5;241m.\u001b[39mFloatTensor(S)\u001b[38;5;241m.\u001b[39mto(device), torch\u001b[38;5;241m.\u001b[39mFloatTensor(V)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnuclear_norm \u001b[38;5;241m=\u001b[39m S\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[1;32mc:\\Users\\grace\\anaconda3\\envs\\PyG2\\lib\\site-packages\\numpy\\linalg\\linalg.py:1681\u001b[0m, in \u001b[0;36msvd\u001b[1;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[0;32m   1678\u001b[0m         gufunc \u001b[38;5;241m=\u001b[39m _umath_linalg\u001b[38;5;241m.\u001b[39msvd_n_s\n\u001b[0;32m   1680\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->DdD\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->ddd\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m-> 1681\u001b[0m u, s, vh \u001b[38;5;241m=\u001b[39m \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1682\u001b[0m u \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1683\u001b[0m s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mastype(_realType(result_t), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#perturbed_adj, features, labels = preprocess(perturbed_adj, features, labels, preprocess_adj=False, device=device)\n",
    "\n",
    "args.debug = True\n",
    "\n",
    "prognn = ProGNN(dom_model, args, device)\n",
    "prognn.fit(features, adj, labels)\n",
    "\n",
    "\n",
    "#prognn.test(features, labels, idx_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(adj, features, labels, device='cpu'):\n",
    "    \"\"\"Convert adj, features, labels from array or sparse matrix to\n",
    "    torch Tensor, and normalize the input data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adj : scipy.sparse.csr_matrix\n",
    "        the adjacency matrix.\n",
    "    features : scipy.sparse.csr_matrix\n",
    "        node features\n",
    "    labels : numpy.array\n",
    "        node labels\n",
    "    device : str\n",
    "        'cpu' or 'cuda'\n",
    "    \"\"\"\n",
    "\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    if sp.issparse(features):\n",
    "        features = torch.FloatTensor(np.array(features.todense()))\n",
    "    else:\n",
    "        features = torch.FloatTensor(features)\n",
    "    adj = torch.FloatTensor(adj.todense())\n",
    "\n",
    "    return adj.to(device), features.to(device), labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyG2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
