{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/DSE-MSU/DeepRobust/blob/master/deeprobust/graph/utils.py\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.sparse as ts\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    \"\"\"Return accuracy of output compared to labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output : torch.Tensor\n",
    "        output from model\n",
    "    labels : torch.Tensor or numpy.array\n",
    "        node labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        accuracy\n",
    "    \"\"\"\n",
    "    if not hasattr(labels, '__len__'):\n",
    "        labels = [labels]\n",
    "    if type(labels) is not torch.Tensor:\n",
    "        labels = torch.LongTensor(labels)\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/DSE-MSU/DeepRobust/blob/master/deeprobust/graph/defense/pgd.py\n",
    "\n",
    "from torch.optim.sgd import SGD\n",
    "from torch.optim.optimizer import required\n",
    "from torch.optim import Optimizer\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "class PGD(Optimizer):\n",
    "    \"\"\"Proximal gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : iterable\n",
    "        iterable of parameters to optimize or dicts defining parameter groups\n",
    "    proxs : iterable\n",
    "        iterable of proximal operators\n",
    "    alpha : iterable\n",
    "        iterable of coefficients for proximal gradient descent\n",
    "    lr : float\n",
    "        learning rate\n",
    "    momentum : float\n",
    "        momentum factor (default: 0)\n",
    "    weight_decay : float\n",
    "        weight decay (L2 penalty) (default: 0)\n",
    "    dampening : float\n",
    "        dampening for momentum (default: 0)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, proxs, alphas, lr=required, momentum=0, dampening=0, weight_decay=0):\n",
    "        defaults = dict(lr=lr, momentum=0, dampening=0,\n",
    "                        weight_decay=0, nesterov=False)\n",
    "        \n",
    "        #Added such that it can be accessed in setstate, which previously gave errors\n",
    "        self.proxs = proxs\n",
    "        self.alphas = alphas\n",
    "        #End of added\n",
    "\n",
    "        super(PGD, self).__init__(params, defaults)\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('proxs', proxs)\n",
    "            group.setdefault('alphas', alphas)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(PGD, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('nesterov', False)\n",
    "            group.setdefault('proxs', self.proxs)\n",
    "            group.setdefault('alphas', self.alphas)\n",
    "\n",
    "    def step(self, delta=0, closure=None):\n",
    "         for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            dampening = group['dampening']\n",
    "            nesterov = group['nesterov']\n",
    "            proxs = group['proxs']\n",
    "            alphas = group['alphas']\n",
    "\n",
    "            # apply the proximal operator to each parameter in a group\n",
    "            for param in group['params']:\n",
    "                for prox_operator, alpha in zip(proxs, alphas):\n",
    "                    # param.data.add_(lr, -param.grad.data)\n",
    "                    # param.data.add_(delta)\n",
    "                    param.data = prox_operator(param.data, alpha=alpha*lr)\n",
    "\n",
    "\n",
    "class ProxOperators():\n",
    "    \"\"\"Proximal Operators.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nuclear_norm = None\n",
    "\n",
    "    def prox_l1(self, data, alpha):\n",
    "        \"\"\"Proximal operator for l1 norm.\n",
    "        \"\"\"\n",
    "        data = torch.mul(torch.sign(data), torch.clamp(torch.abs(data)-alpha, min=0))\n",
    "        return data\n",
    "\n",
    "    def prox_nuclear(self, data, alpha):\n",
    "        \"\"\"Proximal operator for nuclear norm (trace norm).\n",
    "        \"\"\"\n",
    "        device = data.device\n",
    "        U, S, V = np.linalg.svd(data.cpu())\n",
    "        U, S, V = torch.FloatTensor(U).to(device), torch.FloatTensor(S).to(device), torch.FloatTensor(V).to(device)\n",
    "        self.nuclear_norm = S.sum()\n",
    "        # print(\"nuclear norm: %.4f\" % self.nuclear_norm)\n",
    "\n",
    "        diag_S = torch.diag(torch.clamp(S-alpha, min=0))\n",
    "        return torch.matmul(torch.matmul(U, diag_S), V)\n",
    "\n",
    "    def prox_nuclear_truncated_2(self, data, alpha, k=50):\n",
    "        device = data.device\n",
    "        import tensorly as tl\n",
    "        tl.set_backend('pytorch')\n",
    "        U, S, V = tl.truncated_svd(data.cpu(), n_eigenvecs=k)\n",
    "        U, S, V = torch.FloatTensor(U).to(device), torch.FloatTensor(S).to(device), torch.FloatTensor(V).to(device)\n",
    "        self.nuclear_norm = S.sum()\n",
    "        # print(\"nuclear norm: %.4f\" % self.nuclear_norm)\n",
    "\n",
    "        S = torch.clamp(S-alpha, min=0)\n",
    "\n",
    "        # diag_S = torch.diag(torch.clamp(S-alpha, min=0))\n",
    "        # U = torch.spmm(U, diag_S)\n",
    "        # V = torch.matmul(U, V)\n",
    "\n",
    "        # make diag_S sparse matrix\n",
    "        indices = torch.tensor((range(0, len(S)), range(0, len(S)))).to(device)\n",
    "        values = S\n",
    "        diag_S = torch.sparse.FloatTensor(indices, values, torch.Size((len(S), len(S))))\n",
    "        V = torch.spmm(diag_S, V)\n",
    "        V = torch.matmul(U, V)\n",
    "        return V\n",
    "\n",
    "    def prox_nuclear_truncated(self, data, alpha, k=50):\n",
    "        device = data.device\n",
    "        indices = torch.nonzero(data).t()\n",
    "        values = data[indices[0], indices[1]] # modify this based on dimensionality\n",
    "        data_sparse = sp.csr_matrix((values.cpu().numpy(), indices.cpu().numpy()))\n",
    "        U, S, V = sp.linalg.svds(data_sparse, k=k)\n",
    "        U, S, V = torch.FloatTensor(U).to(device), torch.FloatTensor(S).to(device), torch.FloatTensor(V).to(device)\n",
    "        self.nuclear_norm = S.sum()\n",
    "        diag_S = torch.diag(torch.clamp(S-alpha, min=0))\n",
    "        return torch.matmul(torch.matmul(U, diag_S), V)\n",
    "\n",
    "    def prox_nuclear_cuda(self, data, alpha):\n",
    "\n",
    "        device = data.device\n",
    "        U, S, V = torch.svd(data)\n",
    "        # self.nuclear_norm = S.sum()\n",
    "        # print(f\"rank = {len(S.nonzero())}\")\n",
    "        self.nuclear_norm = S.sum()\n",
    "        S = torch.clamp(S-alpha, min=0)\n",
    "        indices = torch.tensor([range(0, U.shape[0]),range(0, U.shape[0])]).to(device)\n",
    "        values = S\n",
    "        diag_S = torch.sparse.FloatTensor(indices, values, torch.Size(U.shape))\n",
    "        # diag_S = torch.diag(torch.clamp(S-alpha, min=0))\n",
    "        # print(f\"rank_after = {len(diag_S.nonzero())}\")\n",
    "        V = torch.spmm(diag_S, V.t_())\n",
    "        V = torch.matmul(U, V)\n",
    "        return V\n",
    "\n",
    "prox_operators = ProxOperators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/DSE-MSU/DeepRobust/blob/master/deeprobust/graph/defense/prognn.py\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "from typing import Tuple\n",
    "\n",
    "def loss_func(adj: torch.Tensor, A_hat: torch.Tensor, attrs: torch.Tensor, X_hat: torch.Tensor, alpha: float) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    diff_attribute = torch.pow(X_hat - attrs, 2)\n",
    "    attribute_reconstruction_errors = torch.sqrt(torch.sum(diff_attribute, 1))\n",
    "    attribute_cost = torch.mean(attribute_reconstruction_errors)\n",
    "\n",
    "    diff_structure = torch.pow(A_hat - adj, 2)\n",
    "    structure_reconstruction_errors = torch.sqrt(torch.sum(diff_structure, 1))\n",
    "    structure_cost = torch.mean(structure_reconstruction_errors)\n",
    "\n",
    "    cost = alpha * attribute_reconstruction_errors + (1 - alpha) * structure_reconstruction_errors\n",
    "    return cost, structure_cost, attribute_cost\n",
    "\n",
    "class ProGNN:\n",
    "    \"\"\" ProGNN (Properties Graph Neural Network). See more details in Graph Structure Learning for Robust Graph Neural Networks, KDD 2020, https://arxiv.org/abs/2005.10203.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model:\n",
    "        model: The backbone GNN model in ProGNN\n",
    "    args:\n",
    "        model configs\n",
    "    device: str\n",
    "        'cpu' or 'cuda'.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    See details in https://github.com/ChandlerBang/Pro-GNN.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, args, device):\n",
    "        self.device = device\n",
    "        self.args = args\n",
    "        self.best_val_acc = 0\n",
    "        self.best_val_loss = 10\n",
    "        self.best_graph = None\n",
    "        self.weights = None\n",
    "        self.estimator = None\n",
    "        self.model = model.to(device)\n",
    "\n",
    "    def fit(self, features, adj, labels, top_k = 10, **kwargs):\n",
    "        \"\"\"Train Pro-GNN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        features :\n",
    "            node features\n",
    "        adj :\n",
    "            the adjacency matrix. The format is torch.tensor\n",
    "        labels :\n",
    "            node labels\n",
    "        top_k :\n",
    "            the number of top k nodes to get\n",
    "        \"\"\"\n",
    "        args = self.args\n",
    "\n",
    "        # self.optimizer = optim.Adam(self.model.parameters(),\n",
    "        #                        lr=args.lr, weight_decay=args.weight_decay)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=args.lr) #Our other versions do not use weight decay\n",
    "\n",
    "\n",
    "        estimator = EstimateAdj(adj, symmetric=args.symmetric, device=self.device).to(self.device)\n",
    "        self.estimator = estimator\n",
    "        self.optimizer_adj = optim.SGD(estimator.parameters(),\n",
    "                              momentum=0.9, lr=args.lr_adj)\n",
    "\n",
    "        self.optimizer_l1 = PGD(estimator.parameters(),\n",
    "                        proxs=[prox_operators.prox_l1],\n",
    "                        lr=args.lr_adj, alphas=[args.alpha])\n",
    "\n",
    "\n",
    "        # warnings.warn(\"If you find the nuclear proximal operator runs too slow on Pubmed, you can  uncomment line 67-71 and use prox_nuclear_cuda to perform the proximal on gpu.\")\n",
    "        # if args.dataset == \"pubmed\":\n",
    "        #     self.optimizer_nuclear = PGD(estimator.parameters(),\n",
    "        #               proxs=[prox_operators.prox_nuclear_cuda],\n",
    "        #               lr=args.lr_adj, alphas=[args.beta])\n",
    "        # else:\n",
    "        warnings.warn(\"If you find the nuclear proximal operator runs too slow, you can modify line 77 to use prox_operators.prox_nuclear_cuda instead of prox_operators.prox_nuclear to perform the proximal on GPU. See details in https://github.com/ChandlerBang/Pro-GNN/issues/1\")\n",
    "        self.optimizer_nuclear = PGD(estimator.parameters(),\n",
    "                  proxs=[prox_operators.prox_nuclear],\n",
    "                  lr=args.lr_adj, alphas=[args.beta])\n",
    "\n",
    "        # Train model\n",
    "        t_total = time.time()\n",
    "        for epoch in range(args.epochs):\n",
    "            if args.only_gcn:\n",
    "                self.train_DOMINANT(epoch, estimator.estimated_adj)\n",
    "                # self.train_gcn(epoch, features, estimator.estimated_adj,\n",
    "                #         labels, idx_train, idx_val)\n",
    "            else:\n",
    "                for i in range(int(args.outer_steps)):\n",
    "                    self.train_adj(epoch, features, adj)\n",
    "\n",
    "                for i in range(int(args.inner_steps)):\n",
    "                    self.train_DOMINANT(epoch, estimator.estimated_adj)\n",
    "                    # self.train_gcn(epoch, features, estimator.estimated_adj,\n",
    "                    #         labels, idx_train, idx_val)\n",
    "\n",
    "        # Get evals from DOMINANT model\n",
    "        self.eval()\n",
    "        A_hat, X_hat = self.forward(self.attrs, self.edge_index)\n",
    "        loss, struct_loss, feat_loss = loss_func(self.adj_label, A_hat, self.attrs, X_hat, args.alpha)\n",
    "        self.score = loss.detach().cpu().numpy()\n",
    "\n",
    "        # Identify and store the IDs of the nodes with the top K highest anomaly scores\n",
    "        \n",
    "        # Convert the anomaly scores to a PyTorch tensor\n",
    "        scores_tensor = torch.tensor(self.score)\n",
    "        # Use torch.topk to find the top K scores and their indices\n",
    "        topk_scores, topk_indices = torch.topk(scores_tensor, top_k, largest=True)\n",
    "        # Convert the indices and scores to lists and store them\n",
    "        top_k_AS_indices = topk_indices.tolist()\n",
    "        top_k_AS_scores = topk_scores.tolist()\n",
    "        self.model.top_k_AS = top_k_AS_indices\n",
    "        # Print the node IDs and their corresponding anomaly scores\n",
    "        if args.debug:\n",
    "            print(f\"Top {top_k} highest anomaly scores' node IDs and scores:\")\n",
    "            for idx, score in zip(top_k_AS_indices, top_k_AS_scores):\n",
    "                print(f\"Node ID: {idx}, Anomaly Score: {score}\")\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "        print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "        print(args)\n",
    "\n",
    "        # Testing\n",
    "        # print(\"picking the best model according to validation performance\")\n",
    "        # self.model.load_state_dict(self.weights)\n",
    "\n",
    "\n",
    "    def train_DOMINANT(self, epoch, estimated_adj):\n",
    "        args = self.args\n",
    "        if args.debug:\n",
    "            print(\"\\n=== train_model ===\")\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        edge_index = estimated_adj.clone().detach().to_sparse()\n",
    "\n",
    "        A_hat, X_hat = self.model.forward(self.model.attrs, edge_index)\n",
    "\n",
    "        loss, struct_loss, feat_loss = loss_func(self.model.adj_label, A_hat, self.model.attrs, X_hat, args.alpha)\n",
    "        loss = torch.mean(loss)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        if args.debug:\n",
    "            print(f\"Epoch: {epoch:04d}, train_loss={loss.item():.5f}, \"\n",
    "                f\"train/struct_loss={struct_loss.item():.5f}, train/feat_loss={feat_loss.item():.5f}\")\n",
    "\n",
    "        \n",
    "    def train_gcn(self, epoch, features, adj, labels, idx_train, idx_val):\n",
    "        args = self.args\n",
    "        estimator = self.estimator\n",
    "        adj = estimator.normalize()\n",
    "\n",
    "        t = time.time()\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        output = self.model(features, adj)\n",
    "        loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "        acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "        loss_train.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        self.model.eval()\n",
    "        output = self.model(features, adj)\n",
    "\n",
    "        loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "        acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "\n",
    "        if acc_val > self.best_val_acc:\n",
    "            self.best_val_acc = acc_val\n",
    "            self.best_graph = adj.detach()\n",
    "            self.weights = deepcopy(self.model.state_dict())\n",
    "            if args.debug:\n",
    "                print('\\t=== saving current graph/gcn, best_val_acc: %s' % self.best_val_acc.item())\n",
    "\n",
    "        if loss_val < self.best_val_loss:\n",
    "            self.best_val_loss = loss_val\n",
    "            self.best_graph = adj.detach()\n",
    "            self.weights = deepcopy(self.model.state_dict())\n",
    "            if args.debug:\n",
    "                print(f'\\t=== saving current graph/gcn, best_val_loss: %s' % self.best_val_loss.item())\n",
    "\n",
    "        if args.debug:\n",
    "            if epoch % 1 == 0:\n",
    "                print('Epoch: {:04d}'.format(epoch+1),\n",
    "                      'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "                      'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "                      'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "                      'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "                      'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "\n",
    "    def train_adj(self, epoch, features, adj):\n",
    "        estimator = self.estimator\n",
    "        args = self.args\n",
    "        if args.debug:\n",
    "            print(\"\\n=== train_adj ===\")\n",
    "        t = time.time()\n",
    "        estimator.train()\n",
    "        self.optimizer_adj.zero_grad()\n",
    "\n",
    "        loss_l1 = torch.norm(estimator.estimated_adj, 1)\n",
    "        loss_fro = torch.norm(estimator.estimated_adj - adj, p='fro')\n",
    "        normalized_adj = estimator.normalize()\n",
    "\n",
    "        if args.lambda_:\n",
    "            loss_smooth_feat = self.feature_smoothing(estimator.estimated_adj, features)\n",
    "        else:\n",
    "            loss_smooth_feat = 0 * loss_l1\n",
    "\n",
    "        # output = self.model(features, normalized_adj) #Forward func\n",
    "        # loss_gcn = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "        # acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "\n",
    "        edge_index = normalized_adj.clone().detach().to_sparse()\n",
    "\n",
    "        A_hat, X_hat = self.model.forward(self.model.attrs, edge_index)\n",
    "        loss, _, _ = loss_func(self.model.adj_label, A_hat, self.model.attrs, X_hat, args.alpha)\n",
    "        loss_gcn = torch.mean(loss)\n",
    "\n",
    "\n",
    "        loss_symmetric = torch.norm(estimator.estimated_adj \\\n",
    "                        - estimator.estimated_adj.t(), p=\"fro\")\n",
    "\n",
    "        loss_diffiential =  loss_fro + args.gamma * loss_gcn + args.lambda_ * loss_smooth_feat + args.phi * loss_symmetric\n",
    "\n",
    "        loss_diffiential.backward()\n",
    "\n",
    "        self.optimizer_adj.step()\n",
    "        loss_nuclear =  0 * loss_fro\n",
    "        if args.beta != 0:\n",
    "            self.optimizer_nuclear.zero_grad()\n",
    "            self.optimizer_nuclear.step()\n",
    "            loss_nuclear = prox_operators.nuclear_norm\n",
    "\n",
    "        self.optimizer_l1.zero_grad()\n",
    "        self.optimizer_l1.step()\n",
    "\n",
    "        total_loss = loss_fro \\\n",
    "                    + args.gamma * loss_gcn \\\n",
    "                    + args.alpha * loss_l1 \\\n",
    "                    + args.beta * loss_nuclear \\\n",
    "                    + args.phi * loss_symmetric\n",
    "\n",
    "        estimator.estimated_adj.data.copy_(torch.clamp(\n",
    "                  estimator.estimated_adj.data, min=0, max=1))\n",
    "\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "\n",
    "        if args.debug:\n",
    "            if epoch % 1 == 0:\n",
    "                print('Epoch: {:04d}'.format(epoch+1),\n",
    "                      'loss_fro: {:.4f}'.format(loss_fro.item()),\n",
    "                      'loss_gcn: {:.4f}'.format(loss_gcn.item()),\n",
    "                      'loss_feat: {:.4f}'.format(loss_smooth_feat.item()),\n",
    "                      'loss_symmetric: {:.4f}'.format(loss_symmetric.item()),\n",
    "                      'delta_l1_norm: {:.4f}'.format(torch.norm(estimator.estimated_adj-adj, 1).item()),\n",
    "                      'loss_l1: {:.4f}'.format(loss_l1.item()),\n",
    "                      'loss_total: {:.4f}'.format(total_loss.item()),\n",
    "                      'loss_nuclear: {:.4f}'.format(loss_nuclear.item()))\n",
    "\n",
    "\n",
    "    def test(self, features, labels, idx_test):\n",
    "        \"\"\"Evaluate the performance of ProGNN on test set\n",
    "        \"\"\"\n",
    "        print(\"\\t=== testing ===\")\n",
    "        self.model.eval()\n",
    "        adj = self.best_graph\n",
    "        if self.best_graph is None:\n",
    "            adj = self.estimator.normalize()\n",
    "        output = self.model(features, adj)\n",
    "        loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "        acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "        print(\"\\tTest set results:\",\n",
    "              \"loss= {:.4f}\".format(loss_test.item()),\n",
    "              \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "        return acc_test.item()\n",
    "\n",
    "    def feature_smoothing(self, adj, X):\n",
    "        adj = (adj.t() + adj)/2\n",
    "        rowsum = adj.sum(1)\n",
    "        r_inv = rowsum.flatten()\n",
    "        D = torch.diag(r_inv)\n",
    "        L = D - adj\n",
    "\n",
    "        r_inv = r_inv  + 1e-3\n",
    "        r_inv = r_inv.pow(-1/2).flatten()\n",
    "        r_inv[torch.isinf(r_inv)] = 0.\n",
    "        r_mat_inv = torch.diag(r_inv)\n",
    "        # L = r_mat_inv @ L\n",
    "        L = r_mat_inv @ L @ r_mat_inv\n",
    "\n",
    "        XLXT = torch.matmul(torch.matmul(X.t(), L), X)\n",
    "        loss_smooth_feat = torch.trace(XLXT)\n",
    "        return loss_smooth_feat\n",
    "\n",
    "\n",
    "class EstimateAdj(nn.Module):\n",
    "    \"\"\"Provide a pytorch parameter matrix for estimated\n",
    "    adjacency matrix and corresponding operations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, adj, symmetric=False, device='cpu'):\n",
    "        super(EstimateAdj, self).__init__()\n",
    "        n = len(adj)\n",
    "        self.estimated_adj = nn.Parameter(torch.FloatTensor(n, n))\n",
    "        self._init_estimation(adj)\n",
    "        self.symmetric = symmetric\n",
    "        self.device = device\n",
    "\n",
    "    def _init_estimation(self, adj):\n",
    "        with torch.no_grad():\n",
    "            n = len(adj)\n",
    "            self.estimated_adj.data.copy_(adj)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.estimated_adj\n",
    "\n",
    "    def normalize(self):\n",
    "\n",
    "        if self.symmetric:\n",
    "            adj = (self.estimated_adj + self.estimated_adj.t())/2\n",
    "        else:\n",
    "            adj = self.estimated_adj\n",
    "\n",
    "        normalized_adj = self._normalize(adj + torch.eye(adj.shape[0]).to(self.device))\n",
    "        return normalized_adj\n",
    "\n",
    "    def _normalize(self, mx):\n",
    "        rowsum = mx.sum(1)\n",
    "        r_inv = rowsum.pow(-1/2).flatten()\n",
    "        r_inv[torch.isinf(r_inv)] = 0.\n",
    "        r_mat_inv = torch.diag(r_inv)\n",
    "        mx = r_mat_inv @ mx\n",
    "        mx = mx @ r_mat_inv\n",
    "        return mx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.,  ..., 1., 1., 1.])\n",
      "node_attrs\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "adj\n",
      "SparseTensor(row=tensor([   0,    0,    0,  ..., 2707, 2707, 2707]),\n",
      "             col=tensor([   8,  435,  544,  ...,  774, 1389, 2344]),\n",
      "             val=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
      "             size=(2708, 2708), nnz=6268, density=0.09%)\n",
      "labels\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "#Insert own dataset here\n",
    "\n",
    "from pygod.generator import gen_contextual_outlier, gen_structural_outlier\n",
    "from torch_geometric.datasets import AttributedGraphDataset\n",
    "from typing import List\n",
    "\n",
    "string = \"Cora\"\n",
    "#string = \"Wiki\"\n",
    "#string = \"Facebook\"\n",
    "\n",
    "dataset = AttributedGraphDataset(root = \"data/\"+string, name = string)\n",
    "\n",
    "data = dataset[0]\n",
    "\n",
    "\n",
    "amount_of_nodes = data.num_nodes\n",
    "\n",
    "seed = 123\n",
    "num_nodes_to_inject = round(amount_of_nodes * 0.05)\n",
    "num_nodes_per_clique = 15\n",
    "num_cliques = round((num_nodes_to_inject / 2) / num_nodes_per_clique)\n",
    "num_contextual_outliers = num_nodes_to_inject - num_cliques * num_nodes_per_clique\n",
    "\n",
    "data, ya = gen_contextual_outlier(data, n = num_contextual_outliers, k = 50, seed = seed) \n",
    "#n (int) – Number of nodes converting to outliers.\n",
    "#k (int) – Number of candidate nodes for each outlier node.\n",
    "\n",
    "data, ys = gen_structural_outlier(data, m = num_nodes_per_clique, n = num_cliques, seed = seed)\n",
    "#m (int) - Number nodes in the outlier cliques.\n",
    "#n (int) - Number of outlier clique\n",
    "data_classes = data.y\n",
    "\n",
    "data.y = torch.logical_or(ys, ya).long()\n",
    "y_binary: List[int] = data.y.bool()\n",
    "anomaly_list = np.where(y_binary == True)[0]  # Used for list for which nodes to hide\n",
    "\n",
    "\n",
    "\n",
    "from gad_adversarial_robustness.utils.graph_utils import prepare_graph\n",
    "\n",
    "amount_of_nodes = data.num_nodes\n",
    "labels = data_classes\n",
    "features = data.x\n",
    "_, adj, _ = prepare_graph(data) #Get adjacency matrix\n",
    "\n",
    "\n",
    "# adj, features, labels = data.adj, data.features, data.labels\n",
    "# idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2824f801210>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Based on https://github.com/ChandlerBang/Pro-GNN/blob/master/train.py\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "import os\n",
    "#from torch_sparse import SparseTensor\n",
    "\n",
    "from gad_adversarial_robustness.utils.graph_utils import load_anomaly_detection_dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_torch_sparse_tensor\n",
    "\n",
    "script_dir = os.path.abspath('')\n",
    "yaml_path = os.path.join(script_dir, '..', 'configs', 'dominant_config.yaml')\n",
    "with open(yaml_path) as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "dataset_caching_path = os.path.join(script_dir, '..', '..', '..', 'data')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    config['model']['device'] = 'cuda'\n",
    "else:\n",
    "    config['model']['device'] = 'cpu'\n",
    "\n",
    "\n",
    "# Training settings\n",
    "class Args: #Has comments from what pro-gnn recommends for their own GCN system\n",
    "    def __init__(self, config):\n",
    "        self.debug = False\n",
    "        self.only_gcn = False\n",
    "        self.no_cuda = False\n",
    "        self.seed = 123\n",
    "        self.lr = config['model']['lr'] #0.01\n",
    "        self.weight_decay = 5e-4\n",
    "        self.hidden = 16\n",
    "        self.dropout = config['model']['dropout'] # 0.5\n",
    "        self.dataset = 'cora'\n",
    "        self.attack = 'meta'\n",
    "        self.ptb_rate = 0.05\n",
    "        self.epochs = config['model']['epochs'] #400\n",
    "        self.alpha = config['model']['alpha'] #5e-4\n",
    "        self.beta = 1.5\n",
    "        self.gamma = 1\n",
    "        self.lambda_ = 0\n",
    "        self.phi = 0\n",
    "        self.inner_steps = 2\n",
    "        self.outer_steps = 1\n",
    "        self.lr_adj = 0.01\n",
    "        self.symmetric = False\n",
    "\n",
    "args = Args(config)\n",
    "\n",
    "\n",
    "\n",
    "cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "if args.ptb_rate == 0:\n",
    "    attack = \"no\"\n",
    "\n",
    "\n",
    "# # Here the random seed is to split the train/val/test data,\n",
    "# # we need to set the random seed to be the same as that when you generate the perturbed graph\n",
    "# # but now change the setting from nettack to prognn which directly loads the prognn splits\n",
    "# # data = Dataset(root='/tmp/', name=args.dataset, setting='nettack', seed=15)\n",
    "# data = Dataset(root='/tmp/', name=args.dataset, setting='prognn')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make model\n",
    "\n",
    "adj, _, _, adj_label = load_anomaly_detection_dataset(data, config['model']['device'])\n",
    "\n",
    "#edge_index = torch.LongTensor(np.array(sp.coo_matrix(adj).nonzero()))\n",
    "adj_label = torch.FloatTensor(adj_label).to(config['model']['device'])\n",
    "#attrs = torch.FloatTensor(attrs)\n",
    "\n",
    "edge_index = dataset.edge_index.to(config['model']['device'])\n",
    "label = torch.Tensor(dataset.y.bool()).to(config['model']['device'])\n",
    "attrs = dataset.x.to(config['model']['device'])\n",
    "\n",
    "\n",
    "sparse_adj = to_torch_sparse_tensor(edge_index)\n",
    "\n",
    "from gad_adversarial_robustness.gad.dominant.dominant import Dominant \n",
    "\n",
    "dom_model = Dominant(feat_size=attrs.size(1), hidden_size=config['model']['hidden_dim'], dropout=config['model']['dropout'],\n",
    "                device=config['model']['device'], edge_index=sparse_adj, adj_label=adj_label, attrs=attrs, label=label)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2500, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2500]])\n",
      "tensor(indices=tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
      "                       [   8,  435,  544,  ...,  774, 1389, 2344]]),\n",
      "       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
      "       size=(2708, 2708), nnz=5429, layout=torch.sparse_coo)\n",
      "torch.Size([2708, 2708])\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "\n",
    "adj = torch.Tensor(adj)\n",
    "\n",
    "print(adj)\n",
    "print(sparse_adj)\n",
    "#print(torch.tensor(adj))\n",
    "\n",
    "print(adj.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create poison compatible adjacency matrix...\n"
     ]
    }
   ],
   "source": [
    "np_dense_adj = np.array(adj)\n",
    "print(\"Create poison compatible adjacency matrix...\")\n",
    "triple = []\n",
    "for i in range(amount_of_nodes):\n",
    "    for j in range(i + 1, amount_of_nodes):\n",
    "        triple.append([i, j, np_dense_adj[i,j]])  #Fill with 0, then insert actual after\n",
    "\n",
    "triple = np.array(triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "True AS\n",
      "2.4187380426707095\n",
      "[2.41873804]\n",
      "After AS\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::max' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::max' is only available for these backends: [CPU, Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen\\RegisterCPU.cpp:31188 [kernel]\nMeta: registered at /dev/null:228 [kernel]\nQuantizedCPU: registered at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen\\RegisterQuantizedCPU.cpp:944 [kernel]\nBackendSelect: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradCPU: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradCUDA: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradHIP: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradXLA: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradMPS: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradIPU: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradXPU: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradHPU: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradVE: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradLazy: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradMTIA: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradPrivateUse1: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradPrivateUse2: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradPrivateUse3: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradMeta: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradNestedTensor: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nTracer: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\functorch\\BatchRulesReduceOps.cpp:451 [kernel]\nFuncTorchVmapMode: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:157 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m multiple_AS(target_lst \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39my, n_node \u001b[38;5;241m=\u001b[39m amount_of_nodes, device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m budget \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m35\u001b[39m\n\u001b[1;32m----> 8\u001b[0m triple, AS, AS_DOM, AUC_DOM, ACC_DOM, perturb, edge_index \u001b[38;5;241m=\u001b[39m \u001b[43mgreedy_attack_with_statistics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtriple\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdom_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbudget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\grace\\anaconda3\\envs\\PyG2\\lib\\site-packages\\gad_adversarial_robustness\\poison\\greedy.py:236\u001b[0m, in \u001b[0;36mgreedy_attack_with_statistics\u001b[1;34m(model, triple, DOMINANT_model, config, target_list, B, CPI, print_stats)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter AS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    235\u001b[0m AS\u001b[38;5;241m.\u001b[39mappend(model\u001b[38;5;241m.\u001b[39mtrue_AS(triple_torch)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m--> 236\u001b[0m AS_DOM_temp, AUC_DOM_temp, ACC_DOM_temp, target_nodes_as \u001b[38;5;241m=\u001b[39m \u001b[43mget_DOMINANT_eval_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDOMINANT_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperturb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m CHANGE_IN_AS_TARGET_NODE_AS\u001b[38;5;241m.\u001b[39mappend(target_nodes_as)\n\u001b[0;32m    238\u001b[0m AS_DOM\u001b[38;5;241m.\u001b[39mappend(AS_DOM_temp)\n",
      "File \u001b[1;32mc:\\Users\\grace\\anaconda3\\envs\\PyG2\\lib\\site-packages\\gad_adversarial_robustness\\poison\\greedy.py:180\u001b[0m, in \u001b[0;36mget_DOMINANT_eval_values\u001b[1;34m(model, config, target_list, perturb, iteration)\u001b[0m\n\u001b[0;32m    175\u001b[0m deepcopy_model \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(model)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m#torch.save(deepcopy_model.state_dict(), 'model.pt')\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m#model.edge_index = update_edge_data_with_perturb(model.edge_index, perturb)\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m deepcopy_model\u001b[38;5;241m.\u001b[39medge_index \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_adj_matrix_with_perturb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeepcopy_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperturb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m deepcopy_model\u001b[38;5;241m.\u001b[39mto(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    183\u001b[0m deepcopy_model\u001b[38;5;241m.\u001b[39mfit(config, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\grace\\anaconda3\\envs\\PyG2\\lib\\site-packages\\gad_adversarial_robustness\\poison\\greedy.py:108\u001b[0m, in \u001b[0;36mupdate_adj_matrix_with_perturb\u001b[1;34m(adj_matrix, perturb)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_adj_matrix_with_perturb\u001b[39m(adj_matrix, perturb):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m        A faster way of converting perturbations to the edge_data\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m        - adj_matrix: The updated adjacency matrix in sparse\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     adj_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mto_dense_adj\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m#adj_matrix = adj_matrix.to_dense()\u001b[39;00m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m adj_matrix\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\grace\\anaconda3\\envs\\PyG2\\lib\\site-packages\\torch_geometric\\utils\\_to_dense_adj.py:64\u001b[0m, in \u001b[0;36mto_dense_adj\u001b[1;34m(edge_index, batch, edge_attr, max_num_nodes, batch_size)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Converts batched sparse adjacency matrices given by edge indices and\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03medge attributes to a single dense batched adjacency matrix.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m            [5., 0.]]])\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m     max_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m edge_index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     65\u001b[0m     batch \u001b[38;5;241m=\u001b[39m edge_index\u001b[38;5;241m.\u001b[39mnew_zeros(max_index)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Could not run 'aten::max' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::max' is only available for these backends: [CPU, Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen\\RegisterCPU.cpp:31188 [kernel]\nMeta: registered at /dev/null:228 [kernel]\nQuantizedCPU: registered at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen\\RegisterQuantizedCPU.cpp:944 [kernel]\nBackendSelect: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:290 [backend fallback]\nNamed: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradCPU: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradCUDA: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradHIP: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradXLA: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradMPS: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradIPU: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradXPU: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradHPU: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradVE: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradLazy: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradMTIA: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradPrivateUse1: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradPrivateUse2: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradPrivateUse3: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradMeta: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nAutogradNestedTensor: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\VariableType_0.cpp:16790 [autograd kernel]\nTracer: registered at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\generated\\TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\functorch\\BatchRulesReduceOps.cpp:451 [kernel]\nFuncTorchVmapMode: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:157 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "from gad_adversarial_robustness.poison.greedy import greedy_attack_with_statistics, multiple_AS\n",
    "\n",
    "print(data.y)\n",
    "model = multiple_AS(target_lst = data.y, n_node = amount_of_nodes, device = 'cpu')\n",
    "\n",
    "\n",
    "budget = 35\n",
    "triple, AS, AS_DOM, AUC_DOM, ACC_DOM, perturb, edge_index = greedy_attack_with_statistics(model, triple, dom_model, config, data.y, budget, print_stats=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== train_adj ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grace\\AppData\\Local\\Temp\\ipykernel_6796\\3869390901.py:91: UserWarning: If you find the nuclear proximal operator runs too slow, you can modify line 77 to use prox_operators.prox_nuclear_cuda instead of prox_operators.prox_nuclear to perform the proximal on GPU. See details in https://github.com/ChandlerBang/Pro-GNN/issues/1\n",
      "  warnings.warn(\"If you find the nuclear proximal operator runs too slow, you can modify line 77 to use prox_operators.prox_nuclear_cuda instead of prox_operators.prox_nuclear to perform the proximal on GPU. See details in https://github.com/ChandlerBang/Pro-GNN/issues/1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\grace\\anaconda3\\envs\\PyG2\\lib\\site-packages\\torch_geometric\\utils\\_spmm.py:60: UserWarning: Converting sparse tensor to CSR format for more efficient processing. Consider converting your sparse tensor to CSR format beforehand to avoid repeated conversion (got 'torch.sparse_coo')\n",
      "  warnings.warn(f\"Converting sparse tensor to CSR format for more \"\n",
      "c:\\Users\\grace\\anaconda3\\envs\\PyG2\\lib\\site-packages\\torch_geometric\\utils\\_spmm.py:64: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\SparseCsrTensorImpl.cpp:55.)\n",
      "  src = src.to_sparse_csr()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_fro: 0.0000 loss_gcn: 5.4465 loss_feat: 0.0000 loss_symmetric: 37.2644 delta_l1_norm: 116.8474 loss_l1: 3406.4385 loss_total: 5270.1572 loss_nuclear: 1920.1359\n",
      "\n",
      "=== train_model ===\n",
      "False False\n",
      "Epoch: 0000, train_loss=5.81015, train/struct_loss=9.53732, train/feat_loss=4.21279\n",
      "\n",
      "=== train_model ===\n",
      "False False\n",
      "Epoch: 0000, train_loss=3.56211, train/struct_loss=2.13405, train/feat_loss=4.17414\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "Epoch: 0002 loss_fro: 1.3000 loss_gcn: 3.4004 loss_feat: 0.0000 loss_symmetric: 36.2567 delta_l1_norm: 232.7221 loss_l1: 3289.6260 loss_total: 5101.1558 loss_nuclear: 1862.4781\n",
      "\n",
      "=== train_model ===\n",
      "False False\n",
      "Epoch: 0001, train_loss=3.40489, train/struct_loss=1.65241, train/feat_loss=4.15595\n",
      "\n",
      "=== train_model ===\n",
      "False False\n",
      "Epoch: 0001, train_loss=3.38102, train/struct_loss=1.60800, train/feat_loss=4.14088\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "Epoch: 0003 loss_fro: 2.5889 loss_gcn: 3.3717 loss_feat: 0.0000 loss_symmetric: 35.2618 delta_l1_norm: 347.5591 loss_l1: 3173.7837 loss_total: 4935.6387 loss_nuclear: 1805.3531\n",
      "\n",
      "=== train_model ===\n",
      "False False\n",
      "Epoch: 0002, train_loss=3.37376, train/struct_loss=1.60466, train/feat_loss=4.13194\n",
      "\n",
      "=== train_model ===\n",
      "False False\n",
      "Epoch: 0002, train_loss=3.36895, train/struct_loss=1.60443, train/feat_loss=4.12518\n",
      "\n",
      "=== train_adj ===\n",
      "False False\n",
      "Epoch: 0004 loss_fro: 3.8666 loss_gcn: 3.3609 loss_feat: 0.0000 loss_symmetric: 34.2792 delta_l1_norm: 461.2324 loss_l1: 3058.9741 loss_total: 4771.6997 loss_nuclear: 1748.7937\n",
      "\n",
      "=== train_model ===\n",
      "False False\n",
      "Epoch: 0003, train_loss=3.36312, train/struct_loss=1.60459, train/feat_loss=4.11678\n",
      "\n",
      "=== train_model ===\n",
      "False False\n",
      "Epoch: 0003, train_loss=3.35927, train/struct_loss=1.60485, train/feat_loss=4.11116\n",
      "\n",
      "=== train_adj ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m args\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      5\u001b[0m prognn \u001b[38;5;241m=\u001b[39m ProGNN(dom_model, args, device)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mprognn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#prognn.test(features, labels, idx_test)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 105\u001b[0m, in \u001b[0;36mProGNN.fit\u001b[1;34m(self, features, adj, labels, top_k, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# self.train_gcn(epoch, features, estimator.estimated_adj,\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;66;03m#         labels, idx_train, idx_val)\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(args\u001b[38;5;241m.\u001b[39mouter_steps)):\n\u001b[1;32m--> 105\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_adj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(args\u001b[38;5;241m.\u001b[39minner_steps)):\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_DOMINANT(epoch, estimator\u001b[38;5;241m.\u001b[39mestimated_adj)\n",
      "Cell \u001b[1;32mIn[10], line 223\u001b[0m, in \u001b[0;36mProGNN.train_adj\u001b[1;34m(self, epoch, features, adj)\u001b[0m\n\u001b[0;32m    221\u001b[0m loss_l1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(estimator\u001b[38;5;241m.\u001b[39mestimated_adj, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    222\u001b[0m loss_fro \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(estimator\u001b[38;5;241m.\u001b[39mestimated_adj \u001b[38;5;241m-\u001b[39m adj, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 223\u001b[0m normalized_adj \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mlambda_:\n\u001b[0;32m    226\u001b[0m     loss_smooth_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_smoothing(estimator\u001b[38;5;241m.\u001b[39mestimated_adj, features)\n",
      "Cell \u001b[1;32mIn[10], line 346\u001b[0m, in \u001b[0;36mEstimateAdj.normalize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    344\u001b[0m     adj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimated_adj\n\u001b[1;32m--> 346\u001b[0m normalized_adj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m normalized_adj\n",
      "Cell \u001b[1;32mIn[10], line 354\u001b[0m, in \u001b[0;36mEstimateAdj._normalize\u001b[1;34m(self, mx)\u001b[0m\n\u001b[0;32m    352\u001b[0m r_inv[torch\u001b[38;5;241m.\u001b[39misinf(r_inv)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m    353\u001b[0m r_mat_inv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(r_inv)\n\u001b[1;32m--> 354\u001b[0m mx \u001b[38;5;241m=\u001b[39m \u001b[43mr_mat_inv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmx\u001b[49m\n\u001b[0;32m    355\u001b[0m mx \u001b[38;5;241m=\u001b[39m mx \u001b[38;5;241m@\u001b[39m r_mat_inv\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mx\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#perturbed_adj, features, labels = preprocess(perturbed_adj, features, labels, preprocess_adj=False, device=device)\n",
    "\n",
    "args.debug = True\n",
    "\n",
    "prognn = ProGNN(dom_model, args, device)\n",
    "prognn.fit(features, adj, labels)\n",
    "\n",
    "\n",
    "#prognn.test(features, labels, idx_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.0, 1], [0.1, 0.2, 1], [0.2, 0.4, 1], [0.30000000000000004, 0.6000000000000001, 1], [0.4, 0.8, 1], [0.5, 1.0, 1], [0.6000000000000001, 1.2000000000000002, 1], [0.7000000000000001, 1.4000000000000001, 1], [0.8, 1.6, 1], [0.9, 1.8, 1]]\n",
      "[[0.0, 0.0], [0.1, 0.2], [0.2, 0.4], [0.30000000000000004, 0.6000000000000001], [0.4, 0.8], [0.5, 1.0], [0.6000000000000001, 1.2000000000000002], [0.7000000000000001, 1.4000000000000001], [0.8, 1.6], [0.9, 1.8]]\n"
     ]
    }
   ],
   "source": [
    "# Testing environment\n",
    "\n",
    "perturb = []\n",
    "for i in range(10):\n",
    "    perturb.append([0.1 * i, 0.2*i, 1])\n",
    "\n",
    "slice = [perturb[i][0:2] for i in range(len(perturb))]\n",
    "\n",
    "print(perturb)\n",
    "print(slice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(adj, features, labels, device='cpu'):\n",
    "    \"\"\"Convert adj, features, labels from array or sparse matrix to\n",
    "    torch Tensor, and normalize the input data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adj : scipy.sparse.csr_matrix\n",
    "        the adjacency matrix.\n",
    "    features : scipy.sparse.csr_matrix\n",
    "        node features\n",
    "    labels : numpy.array\n",
    "        node labels\n",
    "    device : str\n",
    "        'cpu' or 'cuda'\n",
    "    \"\"\"\n",
    "\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    if sp.issparse(features):\n",
    "        features = torch.FloatTensor(np.array(features.todense()))\n",
    "    else:\n",
    "        features = torch.FloatTensor(features)\n",
    "    adj = torch.FloatTensor(adj.todense())\n",
    "\n",
    "    return adj.to(device), features.to(device), labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyG2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
