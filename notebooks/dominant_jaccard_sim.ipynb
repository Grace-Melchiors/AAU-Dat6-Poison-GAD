{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pygod\n",
    "from pygod.utils import load_data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "import torch_sparse\n",
    "from torch_sparse import SparseTensor\n",
    "from typing import List\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from gad_adversarial_robustness.utils.graph_utils import prepare_graph, get_n_anomaly_indexes, load_anomaly_detection_dataset\n",
    "from gad_adversarial_robustness.poison.greedy import multiple_AS, poison_attack\n",
    "\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import coo_matrix\n",
    "from torch_geometric.utils.convert import from_scipy_sparse_matrix\n",
    "\n",
    "# --------- related to dataset import\n",
    "from pygod.generator import gen_contextual_outlier, gen_structural_outlier\n",
    "from torch_geometric.datasets import AttributedGraphDataset\n",
    "\n",
    "# --------- jaccard\n",
    "from gad_adversarial_robustness.gad.dominant.dominant_cuda import Dominant\n",
    "from gad_adversarial_robustness.gad.dominant.dominant_cuda_Jaccard_similarity import Dominant as DominantJaccard\n",
    "\n",
    "# --------- setup\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "PRELOADED_EDGE_INDEX = True\n",
    "EDGE_INDEX_PT = \"100_budget_facebook_greedy_edge_index.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- datasets: \n",
    "data_name = [\"Cora\", \"Wiki\", \"Facebook\", \"Blogcatalog\"] # blogcataog not an option yet\n",
    "string = data_name[2]\n",
    "\n",
    "dataset = AttributedGraphDataset(root = \"data/\"+string, name = string)\n",
    "data = dataset[0]\n",
    "clean_data = data.clone() # save for later use\n",
    "\n",
    "# ------- inject dataset through pygod:\n",
    "seed = None\n",
    "num_nodes_to_inject = 20\n",
    "num_nodes_per_clique = 10\n",
    "num_cliques = (num_nodes_to_inject // 2) // num_nodes_per_clique\n",
    "num_contextual_outliers = num_nodes_to_inject - num_cliques * num_nodes_per_clique\n",
    "\n",
    "data, ya = gen_contextual_outlier(data, n = num_contextual_outliers, k = 50, seed = seed) \n",
    "#n (int) â€“ Number of nodes converting to outliers.\n",
    "#k (int) â€“ Number of candidate nodes for each outlier node.\n",
    "\n",
    "data, ys = gen_structural_outlier(data, m = num_nodes_per_clique, n = num_cliques, seed = seed)\n",
    "#m (int) - Number nodes in the outlier cliques.\n",
    "#n (int) - Number of outlier clique\n",
    "\n",
    "data.y = torch.logical_or(ys, ya).long() # where ys = y structural and ya = y attribute\n",
    "\n",
    "y_binary: List[int] = data.y.bool()\n",
    "anomaly_list = np.where(y_binary == True)[0]  # Used for list for which nodes to hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poisoned data:    Data(x=[4039, 1283], edge_index=[2, 88324], y=[4039])\n",
      "clean data:       Data(x=[4039, 1283], edge_index=[2, 88234], y=[4039, 193])\n",
      "edge_index diff:  90\n"
     ]
    }
   ],
   "source": [
    "print('poisoned data:   ', data)\n",
    "print('clean data:      ', clean_data)\n",
    "print('edge_index diff: ', len(data.edge_index[1]) - len(clean_data.edge_index[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adj matrix from edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----> to obtain the dense_adj matrix from edge_index tensor \n",
    "edge_weight = torch.ones(data.edge_index.size(1))\n",
    "edge_weight = edge_weight.cpu()\n",
    "adj = sp.csr_matrix((edge_weight, data.edge_index), (data.num_nodes, data.num_nodes))\n",
    "adj = torch_sparse.SparseTensor.from_scipy(adj).coalesce().to(\"cpu\")\n",
    "\n",
    "# adj matrix based on edge_index\n",
    "data.adj = adj.to_dense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posion (compute OR load in poisoned data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truth, of type int list, is instantiated to the T/F labels indicating whether a node is an anomalous node\n",
    "if PRELOADED_EDGE_INDEX is False :\n",
    "    truth: List[int] = data.y.bool()\n",
    "    print('truth: ', truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRELOADED_EDGE_INDEX is False :\n",
    "    print(\"Create poison compatible adjacency matrix...\") # based on code from: https://github.com/zhuyulin-tony/BinarizedAttack/blob/main/src/Greedy.py\n",
    "    triple = []\n",
    "    for i in range(data.num_nodes): # for all nodes...\n",
    "        for j in range(i + 1, data.num_nodes):\n",
    "            triple.append([i, j, data.adj[i,j]])  # Fill with 0, then insert actual after\n",
    "\n",
    "    triple = np.array(triple) # convert to numpy array\n",
    "    print('tripple: ', triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRELOADED_EDGE_INDEX is False :\n",
    "    # ------- Identify Anomalous Nodes \n",
    "    # The nodes we try reduce the \"Active Subnetwork\" score for (i.e. disguise anonomalous nodes)\n",
    "    target_node_lst = get_n_anomaly_indexes(truth, 999) # the indexes of the anomalies (999 is just a flag, if not 999 then it gets from 0 to n anomaly indexes)\n",
    "\n",
    "    # ------- Making Model\n",
    "    model = multiple_AS(target_lst = target_node_lst, n_node = data.num_nodes, device = 'cpu')\n",
    "    budget = 1  # The amount of edges to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Attack (GradMaxSearch, i.e. the greedy attack)\n",
    "if PRELOADED_EDGE_INDEX is False :\n",
    "    adj_adversary, _, _ = poison_attack(model, triple, budget) # returns modified/ poisoned adj matrix\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- post-attack processing --> i.e. \"Converting to compatible tensor\"\n",
    "if PRELOADED_EDGE_INDEX is False :\n",
    "    edge_index = torch.tensor([[],[]]) # Create new Edge Index\n",
    "\n",
    "    # Transpose it to make shape compatible\n",
    "    transposed_adj_adversary = torch.transpose(adj_adversary, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRELOADED_EDGE_INDEX is False :\n",
    "    for i in range(len(adj_adversary)):\n",
    "        if(adj_adversary[i][2] != 0):   # If edge value is NOT 0 (0 meaning no edge)\n",
    "            # Add edge to edge index, choosing first 2 elements (edges), and then the ith edge\n",
    "            edge_index = torch.cat((edge_index, transposed_adj_adversary[:2, i:i+1]), -1)\n",
    "\n",
    "            # Dataset uses edges both ways so add reverse edge as well\n",
    "            edge_index = torch.cat((edge_index, torch.flip(transposed_adj_adversary[:2, i:i+1], dims=[0])), -1)\n",
    "\n",
    "    edge_index = edge_index.type(torch.int64)\n",
    "    data.edge_index = edge_index # assign to dataset obj\n",
    "\n",
    "    # ---- SAVE the edge index data: \n",
    "    torch.save(data.edge_index, '100_budget_facebook_greedy_edge_index.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRELOADED_EDGE_INDEX is True  :\n",
    "    data.edge_index = torch.load(EDGE_INDEX_PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poisoned data:    Data(x=[4039, 1283], edge_index=[2, 176556], y=[4039], adj=[4039, 4039])\n",
      "clean data:       Data(x=[4039, 1283], edge_index=[2, 88234], y=[4039, 193])\n",
      "edge_index diff:  88322\n"
     ]
    }
   ],
   "source": [
    "print('poisoned data:   ', data)\n",
    "print('clean data:      ', clean_data)\n",
    "print('edge_index diff: ', len(data.edge_index[1]) - len(clean_data.edge_index[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAD: DOMINANT JACCARD SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = os.path.abspath('')\n",
    "yaml_path = os.path.join(script_dir, '..', 'configs', 'dominant_config.yaml')\n",
    "with open(yaml_path) as file:\n",
    "        config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DELETE CELL AFTER DEBUGGING --> move changes to utils #####\n",
    "from gad_adversarial_robustness.utils.graph_utils import to_dense_adj, normalize_adj\n",
    "def load_anomaly_detection_dataset(dataset, datadir='data'):\n",
    "    # import dataset and extract its parts\n",
    "    #dataset = load_data(\"inj_cora\")\n",
    "    edge_index = dataset.edge_index\n",
    "    adj = to_dense_adj(edge_index)[0].detach().cpu().numpy() # size of adj = 16313521\n",
    "    print('adj size after to_dense_adj call: ', adj.size)\n",
    "    \n",
    "    feat= dataset.x.detach().cpu().numpy()\n",
    "    # remember to use .bool() if the dataset is an injected dataset, to enable binary labels.\n",
    "    # TODO: handle the case where we inject ourselves\n",
    "    print('dataset.y.size : ', len(dataset.y)) # 4039\n",
    "    print('detached y: ', len(dataset.y.bool().detach().cpu().numpy())) # 4039\n",
    "\n",
    "    truth = dataset.y.bool().detach().cpu().numpy()\n",
    "    # truth = truth.flatten() # len: 779527\n",
    "    \n",
    "\n",
    "    adj_norm = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    adj_norm = adj_norm.toarray()\n",
    "    adj = adj + np.eye(adj.shape[0])\n",
    "    return adj_norm, feat, truth, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOMINANT on CLEAN data:\n",
      "Data(x=[4039, 1283], edge_index=[2, 88234], y=[4039, 193])\n",
      "adj size after to_dense_adj call:  16313521\n",
      "dataset.y.size :  4039\n",
      "detached y:  4039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "4039\n"
     ]
    }
   ],
   "source": [
    "print(\"DOMINANT on CLEAN data:\")\n",
    "\n",
    "print(clean_data)\n",
    "\n",
    "adj, attrs, label, adj_label = load_anomaly_detection_dataset(clean_data)\n",
    "edge_index = torch.LongTensor(np.array(sp.coo_matrix(adj).nonzero()))\n",
    "adj_label = torch.FloatTensor(adj_label)\n",
    "attrs = torch.FloatTensor(attrs)\n",
    "\n",
    "print(type(adj))\n",
    "\n",
    "# print(adj)\n",
    "# print(attrs)\n",
    "print(len(label)) # label \n",
    "# print(adj_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"DOMINANT on CLEAN data:\")\n",
    "# adj, attrs, label, adj_label = load_anomaly_detection_dataset(clean_data)\n",
    "# edge_index = torch.LongTensor(np.array(sp.coo_matrix(adj).nonzero()))\n",
    "# adj_label = torch.FloatTensor(adj_label)\n",
    "# attrs = torch.FloatTensor(attrs)\n",
    "\n",
    "# model = Dominant(feat_size=attrs.size(1), hidden_size=config['model']['hidden_dim'], dropout=config['model']['dropout'],\n",
    "#                     device='cpu', edge_index=edge_index, adj_label=adj_label, attrs=attrs, label=label)\n",
    "\n",
    "# model.fit(config, verbose=True)\n",
    "\n",
    "# # -------------------------------------------------------------------------------------\n",
    "\n",
    "# print(\"DOMINANT on POISONED data:\")\n",
    "# dataset = data\n",
    "# adj, attrs, label, adj_label = load_anomaly_detection_dataset(dataset)\n",
    "# edge_index = torch.LongTensor(np.array(sp.coo_matrix(adj).nonzero()))\n",
    "# adj_label = torch.FloatTensor(adj_label)\n",
    "# attrs = torch.FloatTensor(attrs)\n",
    "\n",
    "# model = Dominant(feat_size=attrs.size(1), hidden_size=config['model']['hidden_dim'], dropout=config['model']['dropout'],\n",
    "#                     device='cpu', edge_index=edge_index, adj_label=adj_label, attrs=attrs, label=label)\n",
    "\n",
    "# model.fit(config, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOMINANT modified w/ Jaccard on CLEAN data:\n",
      "adj size after to_dense_adj call:  16313521\n",
      "dataset.y.size :  4039\n",
      "detached y:  4039\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m DominantJaccard(feat_size\u001b[38;5;241m=\u001b[39mattrs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), hidden_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], dropout\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     18\u001b[0m                     device\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m], edge_index\u001b[38;5;241m=\u001b[39medge_index, adj_label\u001b[38;5;241m=\u001b[39madj_label, attrs\u001b[38;5;241m=\u001b[39mattrs, label\u001b[38;5;241m=\u001b[39mlabel, adj\u001b[38;5;241m=\u001b[39madj)\n\u001b[0;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mto(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 20\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjaccard_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDOMINANT modified w/ Jaccard on POISONED data:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\karenscode\\dat6_gad\\aau-dat6-poison-gad\\gad_adversarial_robustness\\gad\\dominant\\dominant_cuda_Jaccard_similarity.py:93\u001b[0m, in \u001b[0;36mDominant.fit\u001b[1;34m(self, config, verbose, top_k, threshold)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m## JACCARD related: !---------!---------!---------!---------!--------- ##---------!--------- ##\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold \u001b[38;5;241m=\u001b[39m threshold\n\u001b[1;32m---> 93\u001b[0m modified_adj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop_dissimilar_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# modified_adj = drop_dissimilar_edges_independentVersion(self.attrs, adj)\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs, modified_adj, labels \u001b[38;5;241m=\u001b[39m to_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs, modified_adj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32md:\\karenscode\\dat6_gad\\aau-dat6-poison-gad\\gad_adversarial_robustness\\gad\\dominant\\dominant_cuda_Jaccard_similarity.py:162\u001b[0m, in \u001b[0;36mDominant.drop_dissimilar_edges\u001b[1;34m(self, features, adj, metric)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary_feature:\n\u001b[1;32m--> 162\u001b[0m         removed_cnt \u001b[38;5;241m=\u001b[39m \u001b[43mdropedge_jaccard\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj_triu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_triu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_triu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m         removed_cnt \u001b[38;5;241m=\u001b[39m dropedge_cosine(adj_triu\u001b[38;5;241m.\u001b[39mdata, adj_triu\u001b[38;5;241m.\u001b[39mindptr, adj_triu\u001b[38;5;241m.\u001b[39mindices, features, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold)\n",
      "File \u001b[1;32md:\\karenscode\\dat6_gad\\aau-dat6-poison-gad\\gad_adversarial_robustness\\gad\\dominant\\dominant_cuda_Jaccard_similarity.py:192\u001b[0m, in \u001b[0;36mdropedge_jaccard\u001b[1;34m(A, iA, jA, features, threshold)\u001b[0m\n\u001b[0;32m    189\u001b[0m intersection \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcount_nonzero((a\u001b[38;5;241m*\u001b[39mb)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# J = intersection * 1.0 / (np.count_nonzero(a) + np.count_nonzero(b) - intersection)\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m J \u001b[38;5;241m=\u001b[39m \u001b[43mintersection\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount_nonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount_nonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mintersection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m J \u001b[38;5;241m<\u001b[39m threshold:\n\u001b[0;32m    196\u001b[0m     A[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dominant Jaccard Similarity on poisoned data\n",
    "\"\"\"\n",
    "jaccard_threshold = 0.01\n",
    "\n",
    "print(\"DOMINANT modified w/ Jaccard on CLEAN data:\")\n",
    "adj, _, _, adj_label = load_anomaly_detection_dataset(clean_data, config['model']['device'])\n",
    "\n",
    "#edge_index = torch.LongTensor(np.array(sp.coo_matrix(adj).nonzero()))\n",
    "adj_label = torch.FloatTensor(adj_label).to(config['model']['device'])\n",
    "#attrs = torch.FloatTensor(attrs)\n",
    "\n",
    "edge_index = clean_data.edge_index.to(config['model']['device'])\n",
    "label = torch.Tensor(clean_data.y.bool()).to(config['model']['device'])\n",
    "attrs = clean_data.x.to(config['model']['device'])\n",
    "\n",
    "model = DominantJaccard(feat_size=attrs.size(1), hidden_size=config['model']['hidden_dim'], dropout=config['model']['dropout'],\n",
    "                    device=config['model']['device'], edge_index=edge_index, adj_label=adj_label, attrs=attrs, label=label, adj=adj)\n",
    "model.to(config['model']['device'])\n",
    "model.fit(config, threshold=jaccard_threshold, verbose=True)\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"DOMINANT modified w/ Jaccard on POISONED data:\")\n",
    "dataset = data\n",
    "adj, _, _, adj_label = load_anomaly_detection_dataset(dataset, config['model']['device'])\n",
    "#edge_index = torch.LongTensor(np.array(sp.coo_matrix(adj).nonzero()))\n",
    "adj_label = torch.FloatTensor(adj_label).to(config['model']['device'])\n",
    "#attrs = torch.FloatTensor(attrs)\n",
    "\n",
    "edge_index = dataset.edge_index.to(config['model']['device'])\n",
    "label = torch.Tensor(dataset.y.bool()).to(config['model']['device'])\n",
    "attrs = dataset.x.to(config['model']['device'])\n",
    "\n",
    "model = DominantJaccard(feat_size=attrs.size(1), hidden_size=config['model']['hidden_dim'], dropout=config['model']['dropout'],\n",
    "                    device=config['model']['device'], edge_index=edge_index, adj_label=adj_label, attrs=attrs, label=label, adj=adj)\n",
    "model.to(config['model']['device'])\n",
    "model.fit(config, threshold=jaccard_threshold, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----- compute new or load in poisoned data\n",
    "# if PRELOADED_EDGE_INDEX is False :\n",
    "\n",
    "#     # truth, of type int list, is instantiated to the T/F labels indicating whether a node is an anomalous node\n",
    "#     truth: List[int] = data.y.bool()\n",
    "#     print('truth: ', truth)\n",
    "\n",
    "\n",
    "#     print(\"Create poison compatible adjacency matrix...\") # based on code from: https://github.com/zhuyulin-tony/BinarizedAttack/blob/main/src/Greedy.py\n",
    "#     triple = []\n",
    "#     for i in range(data.num_nodes): # for all nodes...\n",
    "#         for j in range(i + 1, data.num_nodes):\n",
    "#             triple.append([i, j, data.adj[i,j]])  # Fill with 0, then insert actual after\n",
    "\n",
    "#     triple = np.array(triple) # convert to numpy array\n",
    "#     print('tripple: ', triple)\n",
    "\n",
    "#     # These are the nodes we try reduce the \"active subnetwork score\" for (i.e. disguising anonomalous nodes)\n",
    "#     target_node_lst = get_n_anomaly_indexes(truth, 999) # the indexes of the anomalies (999 is just a flag, if not 999 then it gets from 0 to n anomaly indexes)\n",
    "\n",
    "#     print(\"Making model...\")\n",
    "#     model = multiple_AS(target_lst = target_node_lst, n_node = data.num_nodes, device = 'cpu')\n",
    "#     budget = 1  # The amount of edges to change\n",
    "\n",
    "#     print(\"Starting attack...\")\n",
    "#     adj_adversary, _, _ = poison_attack(model, triple, budget) # returns modified/ poisoned adj matrix as \n",
    "\n",
    "#     print(\"Converting to compatible tensor...\")\n",
    "#     edge_index = torch.tensor([[],[]]) # Create new Edge Index\n",
    "\n",
    "#     # Transpose it to make shape compatible\n",
    "#     transposed_adj_adversary = torch.transpose(adj_adversary, 0, 1)\n",
    "\n",
    "#     for i in range(len(adj_adversary)):\n",
    "#         if(adj_adversary[i][2] != 0):   # If edge value is NOT 0 (0 meaning no edge)\n",
    "#             #Add edge to edge index, choosing first 2 elements (edges), and then the ith edge\n",
    "#             edge_index = torch.cat((edge_index, transposed_adj_adversary[:2, i:i+1]), -1)\n",
    "#             # Dataset uses edges both ways so add reverse edge as well\n",
    "#             edge_index = torch.cat((edge_index, torch.flip(transposed_adj_adversary[:2, i:i+1], dims=[0])), -1)\n",
    "\n",
    "\n",
    "#     edge_index = edge_index.type(torch.int64)\n",
    "#     data.edge_index = edge_index # assign to dataset obj\n",
    "\n",
    "#     # ---- SAVE the edge index data: \n",
    "#     torch.save(data.edge_index, '100_budget_facebook_greedy_edge_index.pt')\n",
    "\n",
    "# else : \n",
    "#     data.edge_index = torch.load(EDGE_INDEX_PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_nodes = data.y.shape[0] # note: data already has attribute .num_nodes\n",
    "\n",
    "# # ---------- make adj matrix from data in coo format\n",
    "# edge_weight = torch.ones(data.edge_index.size(1))\n",
    "# edge_weight = edge_weight.cpu()\n",
    "\n",
    "# adj = sp.csr_matrix((edge_weight, data.edge_index), (data.num_nodes, data.num_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make a train/validation/test split #\n",
    "# labels = data.y\n",
    "\n",
    "# # ----------- create new masks with specified split \n",
    "# split = [0.6, 0.2, 0.2]\n",
    "# train_ratio = split[0]\n",
    "# val_ratio = split[1]\n",
    "# test_ratio = split[2]\n",
    "\n",
    "# # Create a random permutation of node indices\n",
    "# node_indices = torch.randperm(num_nodes)\n",
    "# print(node_indices)\n",
    "\n",
    "# # Calculate the split indices\n",
    "# train_size = int(num_nodes * train_ratio)\n",
    "# val_size = int(num_nodes * val_ratio)\n",
    "# test_size = num_nodes - train_size - val_size\n",
    "\n",
    "#     # Create new masks based on the split indices\n",
    "# new_train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "# new_train_mask[node_indices[:train_size]] = True\n",
    "\n",
    "# new_val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "# new_val_mask[node_indices[train_size:train_size+val_size]] = True\n",
    "\n",
    "# new_test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "# new_test_mask[node_indices[train_size+val_size:]] = True\n",
    "\n",
    "# # Assign the new masks to the data object\n",
    "# data.train_mask = new_train_mask\n",
    "# data.val_mask = new_val_mask\n",
    "# data.test_mask = new_test_mask\n",
    "\n",
    "# # Extract the new indices for the training, validation, and test sets\n",
    "# idx_train = np.where(data.train_mask == True)[0]\n",
    "# idx_val = np.where(data.val_mask == True)[0]\n",
    "# idx_test = np.where(data.test_mask == True)[0]\n",
    "\n",
    "# # from the RTGNN github ----------------------------------------------\n",
    "# train_labels = labels[idx_train]\n",
    "# val_labels = labels[idx_val]\n",
    "\n",
    "# # Concatenating training and validation labels\n",
    "# train_val_labels = np.concatenate([train_labels, val_labels],axis=0)\n",
    "\n",
    "# # Concatenating training and validation masks\n",
    "# idx = np.concatenate([idx_train, idx_val],axis=0)\n",
    "\n",
    "\n",
    "# # # convert the T/F labels to integers --> used in the following call of \"noisyfy_with_P\"\n",
    "# train_val_labels_int = train_val_labels.astype(int)\n",
    "# # idx_int = idx.astype(int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
