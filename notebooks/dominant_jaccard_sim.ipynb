{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pygod\n",
    "from pygod.utils import load_data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "import torch_sparse\n",
    "from torch_sparse import SparseTensor\n",
    "from typing import List\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from gad_adversarial_robustness.utils.graph_utils import prepare_graph, get_n_anomaly_indexes, load_anomaly_detection_dataset\n",
    "from gad_adversarial_robustness.poison.greedy import multiple_AS, poison_attack\n",
    "\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import coo_matrix\n",
    "from torch_geometric.utils.convert import from_scipy_sparse_matrix\n",
    "\n",
    "# --------- related to dataset import\n",
    "from pygod.generator import gen_contextual_outlier, gen_structural_outlier\n",
    "from torch_geometric.datasets import AttributedGraphDataset\n",
    "\n",
    "# --------- jaccard\n",
    "from gad_adversarial_robustness.gad.dominant.dominant_cuda import Dominant\n",
    "from gad_adversarial_robustness.gad.dominant.dominant_cuda_Jaccard_similarity import Dominant as DominantJaccard\n",
    "\n",
    "# --------- setup\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "PRELOADED_EDGE_INDEX = False\n",
    "EDGE_INDEX_PT = \"100_budget_facebook_greedy_edge_index.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- datasets: \n",
    "data_name = [\"Cora\", \"Wiki\", \"Facebook\", \"Blogcatalog\"] # blogcataog not an option yet\n",
    "string = data_name[2]\n",
    "\n",
    "dataset = AttributedGraphDataset(root = \"data/\"+string, name = string)\n",
    "data = dataset[0]\n",
    "clean_data = data.clone() # save for later use\n",
    "\n",
    "# ------- inject dataset through pygod:\n",
    "seed = None\n",
    "num_nodes_to_inject = 20\n",
    "num_nodes_per_clique = 10\n",
    "num_cliques = (num_nodes_to_inject // 2) // num_nodes_per_clique\n",
    "num_contextual_outliers = num_nodes_to_inject - num_cliques * num_nodes_per_clique\n",
    "\n",
    "data, ya = gen_contextual_outlier(data, n = num_contextual_outliers, k = 50, seed = seed) \n",
    "#n (int) – Number of nodes converting to outliers.\n",
    "#k (int) – Number of candidate nodes for each outlier node.\n",
    "\n",
    "data, ys = gen_structural_outlier(data, m = num_nodes_per_clique, n = num_cliques, seed = seed)\n",
    "#m (int) - Number nodes in the outlier cliques.\n",
    "#n (int) - Number of outlier clique\n",
    "\n",
    "data.y = torch.logical_or(ys, ya).long() # where ys = y structural and ya = y attribute\n",
    "\n",
    "y_binary: List[int] = data.y.bool()\n",
    "anomaly_list = np.where(y_binary == True)[0]  # Used for list for which nodes to hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poisoned data:    Data(x=[4039, 1283], edge_index=[2, 88324], y=[4039])\n",
      "clean data:       Data(x=[4039, 1283], edge_index=[2, 88234], y=[4039, 193])\n",
      "edge_index diff:  90\n"
     ]
    }
   ],
   "source": [
    "print('poisoned data:   ', data)\n",
    "print('clean data:      ', clean_data)\n",
    "print('edge_index diff: ', len(data.edge_index[1]) - len(clean_data.edge_index[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adj matrix from edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----> to obtain the dense_adj matrix from edge_index tensor \n",
    "edge_weight = torch.ones(data.edge_index.size(1))\n",
    "edge_weight = edge_weight.cpu()\n",
    "adj = sp.csr_matrix((edge_weight, data.edge_index), (data.num_nodes, data.num_nodes))\n",
    "adj = torch_sparse.SparseTensor.from_scipy(adj).coalesce().to(\"cpu\")\n",
    "\n",
    "# adj matrix based on edge_index\n",
    "data.adj = adj.to_dense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posion (compute OR load in poisoned data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth:  tensor([False, False, False,  ..., False, False, False])\n"
     ]
    }
   ],
   "source": [
    "# truth, of type int list, is instantiated to the T/F labels indicating whether a node is an anomalous node\n",
    "if PRELOADED_EDGE_INDEX is False :\n",
    "    truth: List[int] = data.y.bool()\n",
    "    print('truth: ', truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create poison compatible adjacency matrix...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tripple:  [[0.000e+00 1.000e+00 1.000e+00]\n",
      " [0.000e+00 2.000e+00 1.000e+00]\n",
      " [0.000e+00 3.000e+00 1.000e+00]\n",
      " ...\n",
      " [4.036e+03 4.037e+03 0.000e+00]\n",
      " [4.036e+03 4.038e+03 0.000e+00]\n",
      " [4.037e+03 4.038e+03 0.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "if PRELOADED_EDGE_INDEX is False :\n",
    "    print(\"Create poison compatible adjacency matrix...\") # based on code from: https://github.com/zhuyulin-tony/BinarizedAttack/blob/main/src/Greedy.py\n",
    "    triple = []\n",
    "    for i in range(data.num_nodes): # for all nodes...\n",
    "        for j in range(i + 1, data.num_nodes):\n",
    "            triple.append([i, j, data.adj[i,j]])  # Fill with 0, then insert actual after\n",
    "\n",
    "    triple = np.array(triple) # convert to numpy array\n",
    "    print('tripple: ', triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies indexes: [ 148  426  523  836  899 1153 1212 1365 1490 1521 1792 1906 1973 2192\n",
      " 2403 2590 2888 3807 3811 4007]\n"
     ]
    }
   ],
   "source": [
    "if PRELOADED_EDGE_INDEX is False :\n",
    "    # ------- Identify Anomalous Nodes \n",
    "    # The nodes we try reduce the \"Active Subnetwork\" score for (i.e. disguise anonomalous nodes)\n",
    "    target_node_lst = get_n_anomaly_indexes(truth, 999) # the indexes of the anomalies (999 is just a flag, if not 999 then it gets from 0 to n anomaly indexes)\n",
    "\n",
    "    # ------- Making Model\n",
    "    model = multiple_AS(target_lst = target_node_lst, n_node = data.num_nodes, device = 'cpu')\n",
    "    budget = 1  # The amount of edges to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial anomaly score: 93.62191966375084\n",
      "Iteration: 1 --- Anomaly score: 93.52029468209719\n"
     ]
    }
   ],
   "source": [
    "# ------- Attack (GradMaxSearch, i.e. the greedy attack)\n",
    "if PRELOADED_EDGE_INDEX is False :\n",
    "    adj_adversary, _, _ = poison_attack(model, triple, budget) # returns modified/ poisoned adj matrix\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- post-attack processing --> i.e. \"Converting to compatible tensor\"\n",
    "if PRELOADED_EDGE_INDEX is False :\n",
    "    edge_index = torch.tensor([[],[]]) # Create new Edge Index\n",
    "\n",
    "    # Transpose it to make shape compatible\n",
    "    transposed_adj_adversary = torch.transpose(adj_adversary, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRELOADED_EDGE_INDEX is False :\n",
    "    for i in range(len(adj_adversary)):\n",
    "        if(adj_adversary[i][2] != 0):   # If edge value is NOT 0 (0 meaning no edge)\n",
    "            # Add edge to edge index, choosing first 2 elements (edges), and then the ith edge\n",
    "            edge_index = torch.cat((edge_index, transposed_adj_adversary[:2, i:i+1]), -1)\n",
    "\n",
    "            # Dataset uses edges both ways so add reverse edge as well\n",
    "            edge_index = torch.cat((edge_index, torch.flip(transposed_adj_adversary[:2, i:i+1], dims=[0])), -1)\n",
    "\n",
    "    edge_index = edge_index.type(torch.int64)\n",
    "    data.edge_index = edge_index # assign to dataset obj\n",
    "\n",
    "    # ---- SAVE the edge index data: \n",
    "    torch.save(data.edge_index, '100_budget_facebook_greedy_edge_index.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRELOADED_EDGE_INDEX is True  :\n",
    "    data.edge_index = torch.load(EDGE_INDEX_PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poisoned data:    Data(x=[4039, 1283], edge_index=[2, 176556], y=[4039], adj=[4039, 4039])\n",
      "clean data:       Data(x=[4039, 1283], edge_index=[2, 88234], y=[4039, 193])\n",
      "edge_index diff:  88322\n"
     ]
    }
   ],
   "source": [
    "print('poisoned data:   ', data)\n",
    "print('clean data:      ', clean_data)\n",
    "print('edge_index diff: ', len(data.edge_index[1]) - len(clean_data.edge_index[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAD: DOMINANT JACCARD SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = os.path.abspath('')\n",
    "yaml_path = os.path.join(script_dir, '..', 'configs', 'dominant_config.yaml')\n",
    "with open(yaml_path) as file:\n",
    "        config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOMINANT on CLEAN data:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m attrs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(attrs)\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m Dominant(feat_size\u001b[38;5;241m=\u001b[39mattrs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), hidden_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], dropout\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      8\u001b[0m                     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, edge_index\u001b[38;5;241m=\u001b[39medge_index, adj_label\u001b[38;5;241m=\u001b[39madj_label, attrs\u001b[38;5;241m=\u001b[39mattrs, label\u001b[38;5;241m=\u001b[39mlabel)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDOMINANT on POISONED data:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\karenscode\\dat6_gad\\aau-dat6-poison-gad\\gad_adversarial_robustness\\gad\\dominant\\dominant_cuda.py:93\u001b[0m, in \u001b[0;36mDominant.fit\u001b[1;34m(self, config, verbose, top_k)\u001b[0m\n\u001b[0;32m     91\u001b[0m loss, struct_loss, feat_loss \u001b[38;5;241m=\u001b[39m loss_func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madj_label, A_hat, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs, X_hat, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     92\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(loss)\n\u001b[1;32m---> 93\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\PyG\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\PyG\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"DOMINANT on CLEAN data:\")\n",
    "adj, attrs, label, adj_label = load_anomaly_detection_dataset(clean_data)\n",
    "edge_index = torch.LongTensor(np.array(sp.coo_matrix(adj).nonzero()))\n",
    "adj_label = torch.FloatTensor(adj_label)\n",
    "attrs = torch.FloatTensor(attrs)\n",
    "\n",
    "model = Dominant(feat_size=attrs.size(1), hidden_size=config['model']['hidden_dim'], dropout=config['model']['dropout'],\n",
    "                    device='cpu', edge_index=edge_index, adj_label=adj_label, attrs=attrs, label=label)\n",
    "\n",
    "model.fit(config, verbose=False)\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"DOMINANT on POISONED data:\")\n",
    "dataset = data\n",
    "adj, attrs, label, adj_label = load_anomaly_detection_dataset(dataset)\n",
    "edge_index = torch.LongTensor(np.array(sp.coo_matrix(adj).nonzero()))\n",
    "adj_label = torch.FloatTensor(adj_label)\n",
    "attrs = torch.FloatTensor(attrs)\n",
    "\n",
    "model = Dominant(feat_size=attrs.size(1), hidden_size=config['model']['hidden_dim'], dropout=config['model']['dropout'],\n",
    "                    device='cpu', edge_index=edge_index, adj_label=adj_label, attrs=attrs, label=label)\n",
    "\n",
    "model.fit(config, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dominant Jaccard Similarity on poisoned data\n",
    "\"\"\"\n",
    "jaccard_threshold = 0.01\n",
    "\n",
    "print(\"DOMINANT modified w/ Jaccard on CLEAN data:\")\n",
    "adj, _, _, adj_label = load_anomaly_detection_dataset(clean_data, config['model']['device'])\n",
    "#edge_index = torch.LongTensor(np.array(sp.coo_matrix(adj).nonzero()))\n",
    "adj_label = torch.FloatTensor(adj_label).to(config['model']['device'])\n",
    "#attrs = torch.FloatTensor(attrs)\n",
    "\n",
    "edge_index = clean_data.edge_index.to(config['model']['device'])\n",
    "label = torch.Tensor(clean_data.y.bool()).to(config['model']['device'])\n",
    "attrs = clean_data.x.to(config['model']['device'])\n",
    "\n",
    "\n",
    "model = DominantJaccard(feat_size=attrs.size(1), hidden_size=config['model']['hidden_dim'], dropout=config['model']['dropout'],\n",
    "                    device=config['model']['device'], edge_index=edge_index, adj_label=adj_label, attrs=attrs, label=label)\n",
    "model.to(config['model']['device'])\n",
    "model.fit(config, threshold=jaccard_threshold, verbose=False)\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"DOMINANT modified w/ Jaccard on POISONED data:\")\n",
    "dataset = data\n",
    "adj, _, _, adj_label = load_anomaly_detection_dataset(dataset, config['model']['device'])\n",
    "#edge_index = torch.LongTensor(np.array(sp.coo_matrix(adj).nonzero()))\n",
    "adj_label = torch.FloatTensor(adj_label).to(config['model']['device'])\n",
    "#attrs = torch.FloatTensor(attrs)\n",
    "\n",
    "edge_index = dataset.edge_index.to(config['model']['device'])\n",
    "label = torch.Tensor(dataset.y.bool()).to(config['model']['device'])\n",
    "attrs = dataset.x.to(config['model']['device'])\n",
    "\n",
    "model = DominantJaccard(feat_size=attrs.size(1), hidden_size=config['model']['hidden_dim'], dropout=config['model']['dropout'],\n",
    "                    device=config['model']['device'], edge_index=edge_index, adj_label=adj_label, attrs=attrs, label=label)\n",
    "model.to(config['model']['device'])\n",
    "model.fit(config, threshold=jaccard_threshold, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----- compute new or load in poisoned data\n",
    "# if PRELOADED_EDGE_INDEX is False :\n",
    "\n",
    "#     # truth, of type int list, is instantiated to the T/F labels indicating whether a node is an anomalous node\n",
    "#     truth: List[int] = data.y.bool()\n",
    "#     print('truth: ', truth)\n",
    "\n",
    "\n",
    "#     print(\"Create poison compatible adjacency matrix...\") # based on code from: https://github.com/zhuyulin-tony/BinarizedAttack/blob/main/src/Greedy.py\n",
    "#     triple = []\n",
    "#     for i in range(data.num_nodes): # for all nodes...\n",
    "#         for j in range(i + 1, data.num_nodes):\n",
    "#             triple.append([i, j, data.adj[i,j]])  # Fill with 0, then insert actual after\n",
    "\n",
    "#     triple = np.array(triple) # convert to numpy array\n",
    "#     print('tripple: ', triple)\n",
    "\n",
    "#     # These are the nodes we try reduce the \"active subnetwork score\" for (i.e. disguising anonomalous nodes)\n",
    "#     target_node_lst = get_n_anomaly_indexes(truth, 999) # the indexes of the anomalies (999 is just a flag, if not 999 then it gets from 0 to n anomaly indexes)\n",
    "\n",
    "#     print(\"Making model...\")\n",
    "#     model = multiple_AS(target_lst = target_node_lst, n_node = data.num_nodes, device = 'cpu')\n",
    "#     budget = 1  # The amount of edges to change\n",
    "\n",
    "#     print(\"Starting attack...\")\n",
    "#     adj_adversary, _, _ = poison_attack(model, triple, budget) # returns modified/ poisoned adj matrix as \n",
    "\n",
    "#     print(\"Converting to compatible tensor...\")\n",
    "#     edge_index = torch.tensor([[],[]]) # Create new Edge Index\n",
    "\n",
    "#     # Transpose it to make shape compatible\n",
    "#     transposed_adj_adversary = torch.transpose(adj_adversary, 0, 1)\n",
    "\n",
    "#     for i in range(len(adj_adversary)):\n",
    "#         if(adj_adversary[i][2] != 0):   # If edge value is NOT 0 (0 meaning no edge)\n",
    "#             #Add edge to edge index, choosing first 2 elements (edges), and then the ith edge\n",
    "#             edge_index = torch.cat((edge_index, transposed_adj_adversary[:2, i:i+1]), -1)\n",
    "#             # Dataset uses edges both ways so add reverse edge as well\n",
    "#             edge_index = torch.cat((edge_index, torch.flip(transposed_adj_adversary[:2, i:i+1], dims=[0])), -1)\n",
    "\n",
    "\n",
    "#     edge_index = edge_index.type(torch.int64)\n",
    "#     data.edge_index = edge_index # assign to dataset obj\n",
    "\n",
    "#     # ---- SAVE the edge index data: \n",
    "#     torch.save(data.edge_index, '100_budget_facebook_greedy_edge_index.pt')\n",
    "\n",
    "# else : \n",
    "#     data.edge_index = torch.load(EDGE_INDEX_PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_nodes = data.y.shape[0] # note: data already has attribute .num_nodes\n",
    "\n",
    "# # ---------- make adj matrix from data in coo format\n",
    "# edge_weight = torch.ones(data.edge_index.size(1))\n",
    "# edge_weight = edge_weight.cpu()\n",
    "\n",
    "# adj = sp.csr_matrix((edge_weight, data.edge_index), (data.num_nodes, data.num_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make a train/validation/test split #\n",
    "# labels = data.y\n",
    "\n",
    "# # ----------- create new masks with specified split \n",
    "# split = [0.6, 0.2, 0.2]\n",
    "# train_ratio = split[0]\n",
    "# val_ratio = split[1]\n",
    "# test_ratio = split[2]\n",
    "\n",
    "# # Create a random permutation of node indices\n",
    "# node_indices = torch.randperm(num_nodes)\n",
    "# print(node_indices)\n",
    "\n",
    "# # Calculate the split indices\n",
    "# train_size = int(num_nodes * train_ratio)\n",
    "# val_size = int(num_nodes * val_ratio)\n",
    "# test_size = num_nodes - train_size - val_size\n",
    "\n",
    "#     # Create new masks based on the split indices\n",
    "# new_train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "# new_train_mask[node_indices[:train_size]] = True\n",
    "\n",
    "# new_val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "# new_val_mask[node_indices[train_size:train_size+val_size]] = True\n",
    "\n",
    "# new_test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "# new_test_mask[node_indices[train_size+val_size:]] = True\n",
    "\n",
    "# # Assign the new masks to the data object\n",
    "# data.train_mask = new_train_mask\n",
    "# data.val_mask = new_val_mask\n",
    "# data.test_mask = new_test_mask\n",
    "\n",
    "# # Extract the new indices for the training, validation, and test sets\n",
    "# idx_train = np.where(data.train_mask == True)[0]\n",
    "# idx_val = np.where(data.val_mask == True)[0]\n",
    "# idx_test = np.where(data.test_mask == True)[0]\n",
    "\n",
    "# # from the RTGNN github ----------------------------------------------\n",
    "# train_labels = labels[idx_train]\n",
    "# val_labels = labels[idx_val]\n",
    "\n",
    "# # Concatenating training and validation labels\n",
    "# train_val_labels = np.concatenate([train_labels, val_labels],axis=0)\n",
    "\n",
    "# # Concatenating training and validation masks\n",
    "# idx = np.concatenate([idx_train, idx_val],axis=0)\n",
    "\n",
    "\n",
    "# # # convert the T/F labels to integers --> used in the following call of \"noisyfy_with_P\"\n",
    "# train_val_labels_int = train_val_labels.astype(int)\n",
    "# # idx_int = idx.astype(int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
