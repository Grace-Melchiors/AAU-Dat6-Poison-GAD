{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RTGNN vs. greedy binarized attack model\n",
    "\n",
    "- this model should utilize the greedy binarized attack (white box poison) on a dataset to hide (potential) anomalies or poison in a datset, such as CORA. \n",
    "-   This should produce a dataset which contains hidden anomalies\n",
    "- After obtaining a dataset with disguised poison (hidden anomalies) --> use RTGNN model to create a model that is unaffected/ robust to these anomalies\n",
    "-   Like DOMINANT, RTGNN also makes use of an Autoencoder (encoder - decoder)?\n",
    "-   \n",
    "\n",
    "Goal: See if RTGNN is able to withstand or counteract a binarized attack on the input data\n",
    "\n",
    "trying to figure out whether RTGNN would be useful or not, against a dataset with disguised poison (hidden anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pygod\n",
    "from pygod.utils import load_data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "import torch_sparse\n",
    "from torch_sparse import SparseTensor\n",
    "from typing import List\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# from gad_adversarial_robustness.gad.dominant import dominant\n",
    "from gad_adversarial_robustness.utils.graph_utils import prepare_graph, get_n_anomaly_indexes, load_anomaly_detection_dataset\n",
    "from gad_adversarial_robustness.poison.greedy import multiple_AS, poison_attack\n",
    "\n",
    "import argparse\n",
    "import scipy.sparse as sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gad_adversarial_robustness.gad.RTGNN.utils import noisify_with_P\n",
    "from gad_adversarial_robustness.gad.RTGNN.dataset import Dataset\n",
    "from gad_adversarial_robustness.gad.RTGNN.model.RTGNN import RTGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = os.path.abspath('')\n",
    "dataset_caching_path = os.path.join(script_dir, '..', '..', '..', 'data')\n",
    "\n",
    "# import dataset from pygod\n",
    "clean_data: Data = load_data(\"inj_cora\", dataset_caching_path)\n",
    "poisoned_data: Data = load_data(\"inj_cora\", dataset_caching_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 11060], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "print(poisoned_data)\n",
    "\n",
    "# x = [2708, 1433] : our node feature matrix of the shape [number of nodes, number of features]\n",
    "# edge_index = [2, 11060] : our graph connectivity matrix of the shape [2, number of edges]\n",
    "# y = [2708] : the node ground truth labels \n",
    "# train_mask = [2708] : an optional attribute that says which node should be used for training, with a list of True or False statements\n",
    "# etc... in this case, the train_mask, vel_mask and test_mask have the same size...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges are directed: True\n",
      "Graph has isolated nodes: False\n",
      "Graph has loops: False\n"
     ]
    }
   ],
   "source": [
    "print(f'Edges are directed: {poisoned_data.is_directed()}')\n",
    "print(f'Graph has isolated nodes: {poisoned_data.has_isolated_nodes()}')\n",
    "print(f'Graph has loops: {poisoned_data.has_self_loops()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10862], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], adj=[2708, 2708])\n",
      "------------\n",
      "Number of graphs: 7\n",
      "Number of features: 1433\n"
     ]
    }
   ],
   "source": [
    "print(poisoned_data)\n",
    "print('------------')\n",
    "print(f'Number of graphs: {len(poisoned_data)}')\n",
    "print(f'Number of features: {poisoned_data.num_features}')\n",
    "# print(f'Number of classes: {poisoned_data.num_classes}') ### does not have an attribute for the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index = torch.Size([2, 11060])\n",
      "tensor([[   0,    0,    0,  ...,  869,  127, 1674],\n",
      "        [ 633, 1862, 2582,  ..., 1732,  214,  438]])\n"
     ]
    }
   ],
   "source": [
    "print(f'edge_index = {poisoned_data.edge_index.shape}')\n",
    "print(poisoned_data.edge_index)\n",
    "\n",
    "# the graph's connections are stored in two lists (11060 directed edges, which equate to 5530 bidirectional edges)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### -->  the method of obtaining dense_adj matrix from #####\n",
    "edge_weight = torch.ones(poisoned_data.edge_index.size(1))\n",
    "edge_weight = edge_weight.cpu()\n",
    "adj = sp.csr_matrix((edge_weight, poisoned_data.edge_index), (poisoned_data.num_nodes, poisoned_data.num_nodes))\n",
    "adj = torch_sparse.SparseTensor.from_scipy(adj).coalesce().to(\"cpu\")\n",
    "\n",
    "# adj matrix based on edge_index\n",
    "poisoned_data.adj = adj.to_dense() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "torch.Size([2708, 2708])\n"
     ]
    }
   ],
   "source": [
    "print(poisoned_data.adj)\n",
    "print(poisoned_data.adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truth, of type int list, is instantiated to the T/F labels indicating whether a node is an anomalous node\n",
    "truth: List[int] = poisoned_data.y.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create poison compatible adjacency matrix...\n"
     ]
    }
   ],
   "source": [
    "print(\"Create poison compatible adjacency matrix...\") # based on code from: https://github.com/zhuyulin-tony/BinarizedAttack/blob/main/src/Greedy.py\n",
    "triple = []\n",
    "for i in range(poisoned_data.num_nodes): # Cora has 2708 nodes\n",
    "    for j in range(i + 1, poisoned_data.num_nodes):\n",
    "        triple.append([i, j, poisoned_data.adj[i,j]])  #Fill with 0, then insert actual after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tripple to numpy array\n",
    "triple = np.array(triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies indexes: [  10   50   70   76  104  124  127  143  151  170  179  181  196  214\n",
      "  217  224  227  287  289  294  311  333  425  438  451  454  459  539\n",
      "  565  572  578  581  615  619  641  652  654  660  670  674  692  711\n",
      "  722  738  781  833  843  869  874  878  882  891  895  915  923  938\n",
      "  980  982  996 1002 1014 1035 1053 1079 1090 1096 1133 1135 1206 1211\n",
      " 1224 1229 1235 1287 1293 1310 1362 1391 1414 1426 1533 1540 1543 1547\n",
      " 1570 1573 1575 1606 1623 1633 1656 1674 1728 1730 1732 1783 1808 1818\n",
      " 1833 1854 1881 1885 1901 1918 1946 1999 2004 2041 2052 2055 2056 2078\n",
      " 2089 2121 2126 2198 2215 2234 2263 2265 2294 2307 2336 2340 2375 2382\n",
      " 2386 2397 2475 2479 2506 2518 2551 2600 2624 2654 2658 2693]\n",
      "Making model...\n"
     ]
    }
   ],
   "source": [
    "# These are the nodes we try reduce the \"active subnetwork score\" for (i.e. disguising anonomalous nodes)\n",
    "target_node_lst = get_n_anomaly_indexes(truth, 999) # the indexes of the anomalies\n",
    "\n",
    "# print(type(target_node_lst)), print(f'target node list: {target_node_lst}'), print(target_node_lst)\n",
    "\n",
    "print(\"Making model...\")\n",
    "model = multiple_AS(target_lst = target_node_lst, n_node = poisoned_data.num_nodes, device = 'cpu')\n",
    "budget = 100  # The amount of edges to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting attack...\n",
      "triple copy type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting attack...\")\n",
    "adj_adversary, _, _ = poison_attack(model, triple, budget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to compatible tensor...\n"
     ]
    }
   ],
   "source": [
    "print(\"Converting to compatible tensor...\")\n",
    "\n",
    "# Create Edge Index'\n",
    "edge_index = torch.tensor([[],[]])\n",
    "\n",
    "# Transpose it to make shape compatible\n",
    "transposed_adj_adversary = torch.transpose(adj_adversary, 0, 1)\n",
    "\n",
    "for i in range(len(adj_adversary)):\n",
    "    if(adj_adversary[i][2] != 0):   #If edge value is not 0 (no edge)\n",
    "        #Add edge to edge index, choosing first 2 elements (edges), and then the ith edge\n",
    "        edge_index = torch.cat((edge_index, transposed_adj_adversary[:2, i:i+1]), -1)\n",
    "        # Dataset uses edges both ways so add reverse edge as well\n",
    "        edge_index = torch.cat((edge_index, torch.flip(transposed_adj_adversary[:2, i:i+1], dims=[0])), -1)\n",
    "\n",
    "\n",
    "edge_index = edge_index.type(torch.int64)\n",
    "poisoned_data.edge_index = edge_index # assign to dataset obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10862], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], adj=[2708, 2708])\n",
      "Data(x=[2708, 1433], edge_index=[2, 11060], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "print(poisoned_data)\n",
    "print(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RTGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--co_lambda'], dest='co_lambda', nargs=None, const=None, default=0.1, type=<class 'float'>, choices=None, required=False, help='weight for consistency regularization term', metavar=None)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the args for the model\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "######## the following are the parameters defined in the RTGNN main file ########\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=12, help='Random seed.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=128,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--edge_hidden', type=int, default=64,\n",
    "                    help='Number of hidden units of MLP graph constructor')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "# parser.add_argument('--dataset', type=str, default=\"cora\",\n",
    "#                     choices=['cora', 'citeseer','blogcatalog'], help='dataset')\n",
    "parser.add_argument('--ptb_rate', type=float, default=0.3,\n",
    "                    help=\"noise ptb_rate\")\n",
    "parser.add_argument('--epochs', type=int,  default=200,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.001,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--alpha', type=float, default=1,\n",
    "                    help='loss weight of graph reconstruction')\n",
    "parser.add_argument('--tau',type=float, default=0.05,\n",
    "                    help='threshold of filtering noisy edges')\n",
    "parser.add_argument('--th',type=float, default=0.95,\n",
    "                    help='threshold of adding pseudo labels')\n",
    "parser.add_argument(\"--K\", type=int, default=100,\n",
    "                    help='number of KNN search for each node')\n",
    "parser.add_argument(\"--n_neg\", type=int, default=100,\n",
    "                    help='number of negitive sampling for each node')\n",
    "parser.add_argument('--noise', type=str, default='uniform', choices=['uniform', 'pair'],\n",
    "                    help='type of noises')\n",
    "parser.add_argument('--decay_w', type=float, default=0.1,\n",
    "                    help='down-weighted factor')\n",
    "parser.add_argument('--co_lambda',type=float,default=0.1,\n",
    "                     help='weight for consistency regularization term')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(seed=12, weight_decay=0.0005, hidden=128, edge_hidden=64, dropout=0.5, ptb_rate=0.3, epochs=200, lr=0.001, alpha=1, tau=0.05, th=0.95, K=100, n_neg=100, noise='uniform', decay_w=0.1, co_lambda=0.1)\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_known_args()[0]\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# data = Dataset(root='./data', name=args.dataset)\n",
    "\n",
    "### insert the data that was attacked by greedy BinarizedAttack\n",
    "data = poisoned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize adj matrix, node features and labels\n",
    "adj = data.adj\n",
    "features = data.x\n",
    "labels = data.y.bool() # converts to true / false\n",
    "\n",
    "# noise perturbation rate --> it controls the amount of noise added to the training and validation labels\n",
    "ptb = args.ptb_rate \n",
    "\n",
    "# initalize number of classes\n",
    "nclass = NUM_CLASSES + 1 \n",
    "args.class_num=nclass\n",
    "\n",
    "##################################################### from the original code\n",
    "# # initialize data split values: train, validation, test indexes\n",
    "# idx_train = len(data.train_mask)\n",
    "# idx_val = len(data.val_mask)\n",
    "# idx_test = len(data.test_mask)\n",
    "\n",
    "# # Extracting labels for training and validation data\n",
    "# train_labels = labels[idx_train] \n",
    "# val_labels = labels[idx_val]\n",
    "\n",
    "# # concatenating values to np arrays\n",
    "# train_val_labels = np.concatenate([train_labels,val_labels],axis=0) \n",
    "# idx = np.concatenate([idx_train,idx_val],axis=0)\n",
    "\n",
    "##################################################################\n",
    "######################   modified version:   #####################\n",
    "################################################################## \n",
    "\n",
    "# Assuming data.y.shape == (2708,)\n",
    "num_nodes = data.y.shape[0]\n",
    "\n",
    "# Determine the split ratios (e.g., 0.6, 0.2, 0.2)\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Create a random permutation of node indices\n",
    "node_indices = torch.randperm(num_nodes)\n",
    "\n",
    "# Calculate the split indices\n",
    "train_size = int(num_nodes * train_ratio)\n",
    "val_size = int(num_nodes * val_ratio)\n",
    "test_size = num_nodes - train_size - val_size\n",
    "\n",
    "# Create new masks based on the split indices\n",
    "new_train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "new_train_mask[node_indices[:train_size]] = True\n",
    "\n",
    "new_val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "new_val_mask[node_indices[train_size:train_size+val_size]] = True\n",
    "\n",
    "new_test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "new_test_mask[node_indices[train_size+val_size:]] = True\n",
    "\n",
    "# Assign the new masks to the data object\n",
    "data.train_mask = new_train_mask\n",
    "data.val_mask = new_val_mask\n",
    "data.test_mask = new_test_mask\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# Extracting labels for training, validation, and test data using masks\n",
    "train_labels = labels[data.train_mask]\n",
    "val_labels = labels[data.val_mask]\n",
    "test_labels = labels[data.test_mask]\n",
    "\n",
    "# Concatenating training and validation labels\n",
    "train_val_labels = torch.cat([train_labels, val_labels], dim=0)\n",
    "\n",
    "# Concatenating training and validation masks\n",
    "idx = torch.cat([data.train_mask, data.val_mask], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tensors to numpy arrays\n",
    "train_val_labels = train_val_labels.cpu().detach().numpy()\n",
    "idx = idx.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform noise\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "object too deep for desired array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[393], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Adding noise to the concatenated labels and getting the noise indices and clean indices\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m noise_y, P, noise_idx, clean_idx \u001b[38;5;241m=\u001b[39m \u001b[43mnoisify_with_P\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_val_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnclass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m args\u001b[38;5;241m.\u001b[39mnoise_idx, args\u001b[38;5;241m.\u001b[39mclean_idx \u001b[38;5;241m=\u001b[39m noise_idx, clean_idx\n\u001b[0;32m      5\u001b[0m noise_labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32md:\\karenscode\\dat6_gad\\aau-dat6-poison-gad\\gad_adversarial_robustness\\gad\\RTGNN\\utils.py:114\u001b[0m, in \u001b[0;36mnoisify_with_P\u001b[1;34m(y_train, train_num, nb_classes, noise, random_state, noise_type)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNoise type have implemented\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# seed the random numbers with #run\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m y_train_noisy \u001b[38;5;241m=\u001b[39m \u001b[43mmulticlass_noisify\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m actual_noise \u001b[38;5;241m=\u001b[39m (y_train_noisy \u001b[38;5;241m!=\u001b[39m y_train)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m    117\u001b[0m y_train_noisy_l \u001b[38;5;241m=\u001b[39m y_train_noisy[:train_num]\n",
      "File \u001b[1;32md:\\karenscode\\dat6_gad\\aau-dat6-poison-gad\\gad_adversarial_robustness\\gad\\RTGNN\\utils.py:57\u001b[0m, in \u001b[0;36mmulticlass_noisify\u001b[1;34m(y, P, random_state)\u001b[0m\n\u001b[0;32m     55\u001b[0m     i \u001b[38;5;241m=\u001b[39m y[idx]\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# draw a vector with only an 1\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     flipped \u001b[38;5;241m=\u001b[39m \u001b[43mflipper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     58\u001b[0m     new_y[idx] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(flipped \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_y\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:4348\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.multinomial\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: object too deep for desired array"
     ]
    }
   ],
   "source": [
    "# Adding noise to the concatenated labels and getting the noise indices and clean indices\n",
    "noise_y, P, noise_idx, clean_idx = noisify_with_P(train_val_labels, data.train_mask.shape[0], nclass, ptb, 10, args.noise)\n",
    "args.noise_idx, args.clean_idx = noise_idx, clean_idx\n",
    "\n",
    "noise_labels = labels.copy()\n",
    "noise_labels[idx] = noise_y # set the noisy labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instatiate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
