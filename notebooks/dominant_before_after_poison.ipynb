{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gad_adversarial_robustness.gad.dominant import dominant\n",
    "from pygod.utils import load_data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import os\n",
    "from gad_adversarial_robustness.utils.graph_utils import prepare_graph\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "from gad_adversarial_robustness.utils.graph_utils import get_n_anomaly_indexes\n",
    "from typing import List\n",
    "from gad_adversarial_robustness.poison.greedy import multiple_AS\n",
    "\n",
    "script_dir = os.path.abspath('')\n",
    "\n",
    "\n",
    "dataset_caching_path = os.path.join(script_dir, '..', '..', '..', 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run the poison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.,  ..., 1., 1., 1.])\n",
      "node_attrs\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "adj\n",
      "SparseTensor(row=tensor([   0,    0,    0,  ..., 2707, 2707, 2707]),\n",
      "             col=tensor([ 633, 1862, 2582,  ...,  598, 1473, 2706]),\n",
      "             val=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
      "             size=(2708, 2708), nnz=11054, density=0.15%)\n",
      "labels\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "Create poison compatible adjacency matrix...\n",
      "Anomalies indexes: [  10   50   70   76  104  124  127  143  151  170  179  181  196  214\n",
      "  217  224  227  287  289  294  311  333  425  438  451  454  459  539\n",
      "  565  572  578  581  615  619  641  652  654  660  670  674  692  711\n",
      "  722  738  781  833  843  869  874  878  882  891  895  915  923  938\n",
      "  980  982  996 1002 1014 1035 1053 1079 1090 1096 1133 1135 1206 1211\n",
      " 1224 1229 1235 1287 1293 1310 1362 1391 1414 1426 1533 1540 1543 1547\n",
      " 1570 1573 1575 1606 1623 1633 1656 1674 1728 1730 1732 1783 1808 1818\n",
      " 1833 1854 1881 1885 1901 1918 1946 1999 2004 2041 2052 2055 2056 2078\n",
      " 2089 2121 2126 2198 2215 2234 2263 2265 2294 2307 2336 2340 2375 2382\n",
      " 2386 2397 2475 2479 2506 2518 2551 2600 2624 2654 2658 2693]\n",
      "<class 'numpy.ndarray'>\n",
      "target node list: [  10   50   70   76  104  124  127  143  151  170  179  181  196  214\n",
      "  217  224  227  287  289  294  311  333  425  438  451  454  459  539\n",
      "  565  572  578  581  615  619  641  652  654  660  670  674  692  711\n",
      "  722  738  781  833  843  869  874  878  882  891  895  915  923  938\n",
      "  980  982  996 1002 1014 1035 1053 1079 1090 1096 1133 1135 1206 1211\n",
      " 1224 1229 1235 1287 1293 1310 1362 1391 1414 1426 1533 1540 1543 1547\n",
      " 1570 1573 1575 1606 1623 1633 1656 1674 1728 1730 1732 1783 1808 1818\n",
      " 1833 1854 1881 1885 1901 1918 1946 1999 2004 2041 2052 2055 2056 2078\n",
      " 2089 2121 2126 2198 2215 2234 2263 2265 2294 2307 2336 2340 2375 2382\n",
      " 2386 2397 2475 2479 2506 2518 2551 2600 2624 2654 2658 2693]\n",
      "[  10   50   70   76  104  124  127  143  151  170  179  181  196  214\n",
      "  217  224  227  287  289  294  311  333  425  438  451  454  459  539\n",
      "  565  572  578  581  615  619  641  652  654  660  670  674  692  711\n",
      "  722  738  781  833  843  869  874  878  882  891  895  915  923  938\n",
      "  980  982  996 1002 1014 1035 1053 1079 1090 1096 1133 1135 1206 1211\n",
      " 1224 1229 1235 1287 1293 1310 1362 1391 1414 1426 1533 1540 1543 1547\n",
      " 1570 1573 1575 1606 1623 1633 1656 1674 1728 1730 1732 1783 1808 1818\n",
      " 1833 1854 1881 1885 1901 1918 1946 1999 2004 2041 2052 2055 2056 2078\n",
      " 2089 2121 2126 2198 2215 2234 2263 2265 2294 2307 2336 2340 2375 2382\n",
      " 2386 2397 2475 2479 2506 2518 2551 2600 2624 2654 2658 2693]\n",
      "Making model...\n",
      "Starting attack...\n",
      "triple copy type: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\andwh\\documents\\p6\\aau-dat6-poison-gad\\gad_adversarial_robustness\\poison\\greedy.py:35: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\SparseCsrTensorImpl.cpp:55.)\n",
      "  return torch.sparse.mm(torch.sparse.mm(A_sp, A_sp), A_sp).to_dense()\n",
      "c:\\users\\andwh\\documents\\p6\\aau-dat6-poison-gad\\gad_adversarial_robustness\\poison\\greedy.py:39: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3618.)\n",
      "  E = torch.sum(A, 1) + 0.5 * torch.diag(self.sparse_matrix_power(A, 3)).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to compatible tensor...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import copy\n",
    "from gad_adversarial_robustness.poison.greedy import poison_attack\n",
    "\n",
    "\n",
    "clean_data: Data = load_data(\"inj_cora\", dataset_caching_path)\n",
    "poisoned_data: Data = load_data(\"inj_cora\", dataset_caching_path)\n",
    "\n",
    "truth: List[int] = poisoned_data.y.bool()\n",
    "amount_of_nodes = poisoned_data.num_nodes\n",
    "_, adj, _ = prepare_graph(poisoned_data)\n",
    "dense_adj = adj.to_dense()  #Fill in zeroes where there are no edges\n",
    "\n",
    "\n",
    "print(\"Create poison compatible adjacency matrix...\")\n",
    "triple = []\n",
    "for i in range(amount_of_nodes):\n",
    "    for j in range(i + 1, amount_of_nodes):\n",
    "        triple.append([i, j, dense_adj[i,j]])  #Fill with 0, then insert actual after\n",
    "\n",
    "\n",
    "\n",
    "triple = np.array(triple)\n",
    "\n",
    "# These are the nodes we try reduce the active subnetwork score for (disguising anonomaly nodes)\n",
    "target_node_lst = get_n_anomaly_indexes(truth, 999)\n",
    "#target_node_lst = get_n_anomaly_indexes(truth, 10)\n",
    "print(type(target_node_lst)), print(f'target node list: {target_node_lst}'), print(target_node_lst)\n",
    "\n",
    "print(\"Making model...\")\n",
    "model = multiple_AS(target_lst = target_node_lst, n_node = amount_of_nodes, device = 'cpu')\n",
    "budget = 2  # The amount of edges to change\n",
    "\n",
    "\n",
    "print(\"Starting attack...\")\n",
    "#triple = triple.detach().cpu().numpy()\n",
    "\n",
    "#triple = torch.tensor(triple, requires_grad=True)\n",
    "\n",
    "\n",
    "adj_adversary, _, _ = poison_attack(model, triple, budget)\n",
    "\n",
    "print(\"Converting to compatible tensor...\")\n",
    "\n",
    "# Create Edge Index'\n",
    "edge_index = torch.tensor([[],[]])\n",
    "\n",
    "# Transpose it to make shape compatible\n",
    "transposed_adj_adversary = torch.transpose(adj_adversary, 0, 1)\n",
    "\n",
    "for i in range(len(adj_adversary)):\n",
    "    if(adj_adversary[i][2] != 0):   #If edge value is not 0 (no edge)\n",
    "        #Add edge to edge index, choosing first 2 elements (edges), and then the ith edge\n",
    "        edge_index = torch.cat((edge_index, transposed_adj_adversary[:2, i:i+1]), -1)\n",
    "        # Dataset uses edges both ways so add reverse edge as well\n",
    "        edge_index = torch.cat((edge_index, torch.flip(transposed_adj_adversary[:2, i:i+1], dims=[0])), -1)\n",
    "\n",
    "\n",
    "edge_index = edge_index.type(torch.int64)\n",
    "poisoned_data.edge_index = edge_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000, train_loss=2.17160, train/struct_loss=8.73512, train/feat_loss=0.53073\n",
      "Epoch: 0000, Auc: 0.8461822590650201\n",
      "Epoch: 0001, train_loss=1.32507, train/struct_loss=5.39569, train/feat_loss=0.30742\n",
      "Epoch: 0002, train_loss=0.91094, train/struct_loss=3.51874, train/feat_loss=0.25899\n",
      "Epoch: 0003, train_loss=0.71144, train/struct_loss=2.52575, train/feat_loss=0.25786\n",
      "Epoch: 0004, train_loss=0.64340, train/struct_loss=2.18490, train/feat_loss=0.25802\n",
      "Epoch: 0005, train_loss=0.63391, train/struct_loss=2.13861, train/feat_loss=0.25774\n",
      "Epoch: 0006, train_loss=0.63444, train/struct_loss=2.14121, train/feat_loss=0.25774\n",
      "Epoch: 0007, train_loss=0.63428, train/struct_loss=2.14026, train/feat_loss=0.25778\n",
      "Epoch: 0008, train_loss=0.63376, train/struct_loss=2.13779, train/feat_loss=0.25776\n",
      "Epoch: 0009, train_loss=0.63376, train/struct_loss=2.13790, train/feat_loss=0.25773\n",
      "Epoch: 0010, train_loss=0.63387, train/struct_loss=2.13870, train/feat_loss=0.25766\n",
      "Epoch: 0010, Auc: 0.8389358822534257\n",
      "Epoch: 0011, train_loss=0.63400, train/struct_loss=2.13919, train/feat_loss=0.25770\n",
      "Epoch: 0012, train_loss=0.63398, train/struct_loss=2.13908, train/feat_loss=0.25771\n",
      "Epoch: 0013, train_loss=0.63391, train/struct_loss=2.13903, train/feat_loss=0.25763\n",
      "Epoch: 0014, train_loss=0.63374, train/struct_loss=2.13832, train/feat_loss=0.25759\n",
      "Epoch: 0015, train_loss=0.63387, train/struct_loss=2.13870, train/feat_loss=0.25767\n",
      "Epoch: 0016, train_loss=0.63392, train/struct_loss=2.13892, train/feat_loss=0.25767\n",
      "Epoch: 0017, train_loss=0.63373, train/struct_loss=2.13837, train/feat_loss=0.25757\n",
      "Epoch: 0018, train_loss=0.63366, train/struct_loss=2.13803, train/feat_loss=0.25757\n",
      "Epoch: 0019, train_loss=0.63385, train/struct_loss=2.13878, train/feat_loss=0.25762\n",
      "Epoch: 0020, train_loss=0.63386, train/struct_loss=2.13893, train/feat_loss=0.25760\n",
      "Epoch: 0020, Auc: 0.8390655839395477\n",
      "Epoch: 0021, train_loss=0.63379, train/struct_loss=2.13870, train/feat_loss=0.25757\n",
      "Epoch: 0022, train_loss=0.63367, train/struct_loss=2.13820, train/feat_loss=0.25753\n",
      "Epoch: 0023, train_loss=0.63376, train/struct_loss=2.13847, train/feat_loss=0.25759\n",
      "Epoch: 0024, train_loss=0.63360, train/struct_loss=2.13773, train/feat_loss=0.25757\n",
      "Epoch: 0025, train_loss=0.63367, train/struct_loss=2.13812, train/feat_loss=0.25756\n",
      "Epoch: 0026, train_loss=0.63366, train/struct_loss=2.13813, train/feat_loss=0.25754\n",
      "Epoch: 0027, train_loss=0.63358, train/struct_loss=2.13774, train/feat_loss=0.25754\n",
      "Epoch: 0028, train_loss=0.63352, train/struct_loss=2.13750, train/feat_loss=0.25753\n",
      "Epoch: 0029, train_loss=0.63355, train/struct_loss=2.13763, train/feat_loss=0.25753\n",
      "Epoch: 0030, train_loss=0.63349, train/struct_loss=2.13741, train/feat_loss=0.25751\n",
      "Epoch: 0030, Auc: 0.8389584390684035\n",
      "Epoch: 0031, train_loss=0.63358, train/struct_loss=2.13779, train/feat_loss=0.25753\n",
      "Epoch: 0032, train_loss=0.63357, train/struct_loss=2.13777, train/feat_loss=0.25752\n",
      "Epoch: 0033, train_loss=0.63363, train/struct_loss=2.13808, train/feat_loss=0.25752\n",
      "Epoch: 0034, train_loss=0.63353, train/struct_loss=2.13758, train/feat_loss=0.25752\n",
      "Epoch: 0035, train_loss=0.63362, train/struct_loss=2.13807, train/feat_loss=0.25751\n",
      "Epoch: 0036, train_loss=0.63348, train/struct_loss=2.13735, train/feat_loss=0.25751\n",
      "Epoch: 0037, train_loss=0.63359, train/struct_loss=2.13764, train/feat_loss=0.25758\n",
      "Epoch: 0038, train_loss=0.63348, train/struct_loss=2.13743, train/feat_loss=0.25749\n",
      "Epoch: 0039, train_loss=0.63348, train/struct_loss=2.13735, train/feat_loss=0.25751\n",
      "Epoch: 0040, train_loss=0.63346, train/struct_loss=2.13728, train/feat_loss=0.25750\n",
      "Epoch: 0040, Auc: 0.8389542096655952\n",
      "Epoch: 0041, train_loss=0.63346, train/struct_loss=2.13730, train/feat_loss=0.25750\n",
      "Epoch: 0042, train_loss=0.63345, train/struct_loss=2.13727, train/feat_loss=0.25750\n",
      "Epoch: 0043, train_loss=0.63352, train/struct_loss=2.13761, train/feat_loss=0.25750\n",
      "Epoch: 0044, train_loss=0.63349, train/struct_loss=2.13745, train/feat_loss=0.25750\n",
      "Epoch: 0045, train_loss=0.63357, train/struct_loss=2.13786, train/feat_loss=0.25750\n",
      "Epoch: 0046, train_loss=0.63348, train/struct_loss=2.13739, train/feat_loss=0.25750\n",
      "Epoch: 0047, train_loss=0.63347, train/struct_loss=2.13733, train/feat_loss=0.25750\n",
      "Epoch: 0048, train_loss=0.63348, train/struct_loss=2.13742, train/feat_loss=0.25750\n",
      "Epoch: 0049, train_loss=0.63340, train/struct_loss=2.13698, train/feat_loss=0.25750\n",
      "Epoch: 0050, train_loss=0.63419, train/struct_loss=2.14097, train/feat_loss=0.25750\n",
      "Epoch: 0050, Auc: 0.8389471606609147\n",
      "Epoch: 0051, train_loss=0.63350, train/struct_loss=2.13750, train/feat_loss=0.25749\n",
      "Epoch: 0052, train_loss=0.63379, train/struct_loss=2.13897, train/feat_loss=0.25750\n",
      "Epoch: 0053, train_loss=0.63373, train/struct_loss=2.13871, train/feat_loss=0.25749\n",
      "Epoch: 0054, train_loss=0.63356, train/struct_loss=2.13784, train/feat_loss=0.25749\n",
      "Epoch: 0055, train_loss=0.63401, train/struct_loss=2.13997, train/feat_loss=0.25752\n",
      "Epoch: 0056, train_loss=0.63354, train/struct_loss=2.13774, train/feat_loss=0.25749\n",
      "Epoch: 0057, train_loss=0.63367, train/struct_loss=2.13843, train/feat_loss=0.25748\n",
      "Epoch: 0058, train_loss=0.63368, train/struct_loss=2.13849, train/feat_loss=0.25748\n",
      "Epoch: 0059, train_loss=0.63366, train/struct_loss=2.13840, train/feat_loss=0.25748\n",
      "Epoch: 0060, train_loss=0.63365, train/struct_loss=2.13827, train/feat_loss=0.25750\n",
      "Epoch: 0060, Auc: 0.8389781762815092\n",
      "Epoch: 0061, train_loss=0.63361, train/struct_loss=2.13808, train/feat_loss=0.25749\n",
      "Epoch: 0062, train_loss=0.63360, train/struct_loss=2.13809, train/feat_loss=0.25748\n",
      "Epoch: 0063, train_loss=0.63345, train/struct_loss=2.13738, train/feat_loss=0.25747\n",
      "Epoch: 0064, train_loss=0.63352, train/struct_loss=2.13764, train/feat_loss=0.25749\n",
      "Epoch: 0065, train_loss=0.63341, train/struct_loss=2.13716, train/feat_loss=0.25747\n",
      "Epoch: 0066, train_loss=0.63361, train/struct_loss=2.13814, train/feat_loss=0.25748\n",
      "Epoch: 0067, train_loss=0.63342, train/struct_loss=2.13718, train/feat_loss=0.25748\n",
      "Epoch: 0068, train_loss=0.63348, train/struct_loss=2.13745, train/feat_loss=0.25748\n",
      "Epoch: 0069, train_loss=0.63344, train/struct_loss=2.13731, train/feat_loss=0.25748\n",
      "Epoch: 0070, train_loss=0.63355, train/struct_loss=2.13782, train/feat_loss=0.25748\n",
      "Epoch: 0070, Auc: 0.8389612586702757\n",
      "Epoch: 0071, train_loss=0.63340, train/struct_loss=2.13715, train/feat_loss=0.25747\n",
      "Epoch: 0072, train_loss=0.63344, train/struct_loss=2.13730, train/feat_loss=0.25747\n",
      "Epoch: 0073, train_loss=0.63343, train/struct_loss=2.13731, train/feat_loss=0.25746\n",
      "Epoch: 0074, train_loss=0.63346, train/struct_loss=2.13754, train/feat_loss=0.25745\n",
      "Epoch: 0075, train_loss=0.63343, train/struct_loss=2.13733, train/feat_loss=0.25746\n",
      "Epoch: 0076, train_loss=0.63338, train/struct_loss=2.13703, train/feat_loss=0.25747\n",
      "Epoch: 0077, train_loss=0.63347, train/struct_loss=2.13754, train/feat_loss=0.25746\n",
      "Epoch: 0078, train_loss=0.63341, train/struct_loss=2.13726, train/feat_loss=0.25745\n",
      "Epoch: 0079, train_loss=0.63338, train/struct_loss=2.13715, train/feat_loss=0.25744\n",
      "Epoch: 0080, train_loss=0.63343, train/struct_loss=2.13741, train/feat_loss=0.25743\n",
      "Epoch: 0080, Auc: 0.8389527998646592\n",
      "Epoch: 0081, train_loss=0.63331, train/struct_loss=2.13695, train/feat_loss=0.25740\n",
      "Epoch: 0082, train_loss=0.63329, train/struct_loss=2.13682, train/feat_loss=0.25741\n",
      "Epoch: 0083, train_loss=0.63342, train/struct_loss=2.13733, train/feat_loss=0.25744\n",
      "Epoch: 0084, train_loss=0.63335, train/struct_loss=2.13715, train/feat_loss=0.25740\n",
      "Epoch: 0085, train_loss=0.63331, train/struct_loss=2.13708, train/feat_loss=0.25737\n",
      "Epoch: 0086, train_loss=0.63326, train/struct_loss=2.13675, train/feat_loss=0.25738\n",
      "Epoch: 0087, train_loss=0.63326, train/struct_loss=2.13686, train/feat_loss=0.25736\n",
      "Epoch: 0088, train_loss=0.63331, train/struct_loss=2.13722, train/feat_loss=0.25733\n",
      "Epoch: 0089, train_loss=0.63328, train/struct_loss=2.13695, train/feat_loss=0.25736\n",
      "Epoch: 0090, train_loss=0.63327, train/struct_loss=2.13697, train/feat_loss=0.25734\n",
      "Epoch: 0090, Auc: 0.8390345683189533\n",
      "Epoch: 0091, train_loss=0.63318, train/struct_loss=2.13672, train/feat_loss=0.25730\n",
      "Epoch: 0092, train_loss=0.63312, train/struct_loss=2.13648, train/feat_loss=0.25728\n",
      "Epoch: 0093, train_loss=0.63331, train/struct_loss=2.13746, train/feat_loss=0.25727\n",
      "Epoch: 0094, train_loss=0.63315, train/struct_loss=2.13692, train/feat_loss=0.25720\n",
      "Epoch: 0095, train_loss=0.63315, train/struct_loss=2.13688, train/feat_loss=0.25722\n",
      "Epoch: 0096, train_loss=0.63314, train/struct_loss=2.13689, train/feat_loss=0.25721\n",
      "Epoch: 0097, train_loss=0.63311, train/struct_loss=2.13692, train/feat_loss=0.25715\n",
      "Epoch: 0098, train_loss=0.63311, train/struct_loss=2.13683, train/feat_loss=0.25718\n",
      "Epoch: 0099, train_loss=0.63303, train/struct_loss=2.13648, train/feat_loss=0.25717\n",
      "Epoch: 0099, Auc: 0.8390402075226978\n",
      "Epoch: 0000, train_loss=1.97862, train/struct_loss=7.80730, train/feat_loss=0.52145\n",
      "Epoch: 0000, Auc: 0.860018045451982\n",
      "Epoch: 0001, train_loss=1.22892, train/struct_loss=4.91761, train/feat_loss=0.30675\n",
      "Epoch: 0002, train_loss=0.83482, train/struct_loss=3.13710, train/feat_loss=0.25925\n",
      "Epoch: 0003, train_loss=0.67540, train/struct_loss=2.34415, train/feat_loss=0.25821\n",
      "Epoch: 0004, train_loss=0.63712, train/struct_loss=2.15194, train/feat_loss=0.25842\n",
      "Epoch: 0005, train_loss=0.63406, train/struct_loss=2.13712, train/feat_loss=0.25829\n",
      "Epoch: 0006, train_loss=0.63461, train/struct_loss=2.14079, train/feat_loss=0.25806\n",
      "Epoch: 0007, train_loss=0.63474, train/struct_loss=2.14004, train/feat_loss=0.25841\n",
      "Epoch: 0008, train_loss=0.63397, train/struct_loss=2.13737, train/feat_loss=0.25812\n",
      "Epoch: 0009, train_loss=0.63398, train/struct_loss=2.13784, train/feat_loss=0.25802\n",
      "Epoch: 0010, train_loss=0.63422, train/struct_loss=2.13862, train/feat_loss=0.25812\n",
      "Epoch: 0010, Auc: 0.8382140641741386\n",
      "Epoch: 0011, train_loss=0.63435, train/struct_loss=2.13885, train/feat_loss=0.25822\n",
      "Epoch: 0012, train_loss=0.63411, train/struct_loss=2.13875, train/feat_loss=0.25795\n",
      "Epoch: 0013, train_loss=0.63390, train/struct_loss=2.13796, train/feat_loss=0.25788\n",
      "Epoch: 0014, train_loss=0.63476, train/struct_loss=2.14167, train/feat_loss=0.25803\n",
      "Epoch: 0015, train_loss=0.63402, train/struct_loss=2.13828, train/feat_loss=0.25796\n",
      "Epoch: 0016, train_loss=0.63385, train/struct_loss=2.13780, train/feat_loss=0.25787\n",
      "Epoch: 0017, train_loss=0.63399, train/struct_loss=2.13852, train/feat_loss=0.25786\n",
      "Epoch: 0018, train_loss=0.63415, train/struct_loss=2.13893, train/feat_loss=0.25796\n",
      "Epoch: 0019, train_loss=0.63412, train/struct_loss=2.13912, train/feat_loss=0.25787\n",
      "Epoch: 0020, train_loss=0.63407, train/struct_loss=2.13923, train/feat_loss=0.25778\n",
      "Epoch: 0020, Auc: 0.8377149946427564\n",
      "Epoch: 0021, train_loss=0.63413, train/struct_loss=2.13928, train/feat_loss=0.25784\n",
      "Epoch: 0022, train_loss=0.63412, train/struct_loss=2.13930, train/feat_loss=0.25783\n",
      "Epoch: 0023, train_loss=0.63409, train/struct_loss=2.13933, train/feat_loss=0.25777\n",
      "Epoch: 0024, train_loss=0.63410, train/struct_loss=2.13936, train/feat_loss=0.25779\n",
      "Epoch: 0025, train_loss=0.63409, train/struct_loss=2.13936, train/feat_loss=0.25778\n",
      "Epoch: 0026, train_loss=0.63405, train/struct_loss=2.13936, train/feat_loss=0.25773\n",
      "Epoch: 0027, train_loss=0.63406, train/struct_loss=2.13937, train/feat_loss=0.25774\n",
      "Epoch: 0028, train_loss=0.63408, train/struct_loss=2.13936, train/feat_loss=0.25776\n",
      "Epoch: 0029, train_loss=0.63405, train/struct_loss=2.13937, train/feat_loss=0.25772\n",
      "Epoch: 0030, train_loss=0.63404, train/struct_loss=2.13936, train/feat_loss=0.25771\n",
      "Epoch: 0030, Auc: 0.8378559747363672\n",
      "Epoch: 0031, train_loss=0.63404, train/struct_loss=2.13936, train/feat_loss=0.25770\n",
      "Epoch: 0032, train_loss=0.63402, train/struct_loss=2.13936, train/feat_loss=0.25769\n",
      "Epoch: 0033, train_loss=0.63401, train/struct_loss=2.13937, train/feat_loss=0.25767\n",
      "Epoch: 0034, train_loss=0.63401, train/struct_loss=2.13931, train/feat_loss=0.25769\n",
      "Epoch: 0035, train_loss=0.63395, train/struct_loss=2.13905, train/feat_loss=0.25767\n",
      "Epoch: 0036, train_loss=0.63373, train/struct_loss=2.13799, train/feat_loss=0.25767\n",
      "Epoch: 0037, train_loss=0.63583, train/struct_loss=2.14839, train/feat_loss=0.25769\n",
      "Epoch: 0038, train_loss=0.63377, train/struct_loss=2.13814, train/feat_loss=0.25768\n",
      "Epoch: 0039, train_loss=0.63393, train/struct_loss=2.13896, train/feat_loss=0.25767\n",
      "Epoch: 0040, train_loss=0.63399, train/struct_loss=2.13930, train/feat_loss=0.25766\n",
      "Epoch: 0040, Auc: 0.8378136807082839\n",
      "Epoch: 0041, train_loss=0.63398, train/struct_loss=2.13928, train/feat_loss=0.25765\n",
      "Epoch: 0042, train_loss=0.63382, train/struct_loss=2.13847, train/feat_loss=0.25766\n",
      "Epoch: 0043, train_loss=0.63387, train/struct_loss=2.13873, train/feat_loss=0.25766\n",
      "Epoch: 0044, train_loss=0.63365, train/struct_loss=2.13759, train/feat_loss=0.25767\n",
      "Epoch: 0045, train_loss=0.63379, train/struct_loss=2.13838, train/feat_loss=0.25765\n",
      "Epoch: 0046, train_loss=0.63381, train/struct_loss=2.13848, train/feat_loss=0.25764\n",
      "Epoch: 0047, train_loss=0.63370, train/struct_loss=2.13792, train/feat_loss=0.25764\n",
      "Epoch: 0048, train_loss=0.63365, train/struct_loss=2.13776, train/feat_loss=0.25762\n",
      "Epoch: 0049, train_loss=0.63379, train/struct_loss=2.13841, train/feat_loss=0.25763\n",
      "Epoch: 0050, train_loss=0.63362, train/struct_loss=2.13758, train/feat_loss=0.25764\n",
      "Epoch: 0050, Auc: 0.8378418767270062\n",
      "Epoch: 0051, train_loss=0.63365, train/struct_loss=2.13772, train/feat_loss=0.25763\n",
      "Epoch: 0052, train_loss=0.63372, train/struct_loss=2.13805, train/feat_loss=0.25764\n",
      "Epoch: 0053, train_loss=0.63365, train/struct_loss=2.13764, train/feat_loss=0.25765\n",
      "Epoch: 0054, train_loss=0.63358, train/struct_loss=2.13733, train/feat_loss=0.25764\n",
      "Epoch: 0055, train_loss=0.63365, train/struct_loss=2.13768, train/feat_loss=0.25764\n",
      "Epoch: 0056, train_loss=0.63360, train/struct_loss=2.13739, train/feat_loss=0.25765\n",
      "Epoch: 0057, train_loss=0.63364, train/struct_loss=2.13762, train/feat_loss=0.25764\n",
      "Epoch: 0058, train_loss=0.63361, train/struct_loss=2.13751, train/feat_loss=0.25763\n",
      "Epoch: 0059, train_loss=0.63360, train/struct_loss=2.13748, train/feat_loss=0.25763\n",
      "Epoch: 0060, train_loss=0.63363, train/struct_loss=2.13766, train/feat_loss=0.25762\n",
      "Epoch: 0060, Auc: 0.837878531551345\n",
      "Epoch: 0061, train_loss=0.63357, train/struct_loss=2.13735, train/feat_loss=0.25763\n",
      "Epoch: 0062, train_loss=0.63358, train/struct_loss=2.13738, train/feat_loss=0.25763\n",
      "Epoch: 0063, train_loss=0.63352, train/struct_loss=2.13708, train/feat_loss=0.25763\n",
      "Epoch: 0064, train_loss=0.63353, train/struct_loss=2.13719, train/feat_loss=0.25762\n",
      "Epoch: 0065, train_loss=0.63349, train/struct_loss=2.13695, train/feat_loss=0.25762\n",
      "Epoch: 0066, train_loss=0.63354, train/struct_loss=2.13718, train/feat_loss=0.25762\n",
      "Epoch: 0067, train_loss=0.63353, train/struct_loss=2.13717, train/feat_loss=0.25762\n",
      "Epoch: 0068, train_loss=0.63355, train/struct_loss=2.13726, train/feat_loss=0.25762\n",
      "Epoch: 0069, train_loss=0.63352, train/struct_loss=2.13716, train/feat_loss=0.25761\n",
      "Epoch: 0070, train_loss=0.63351, train/struct_loss=2.13706, train/feat_loss=0.25763\n",
      "Epoch: 0070, Auc: 0.8379180059775561\n",
      "Epoch: 0071, train_loss=0.63346, train/struct_loss=2.13683, train/feat_loss=0.25762\n",
      "Epoch: 0072, train_loss=0.63347, train/struct_loss=2.13680, train/feat_loss=0.25764\n",
      "Epoch: 0073, train_loss=0.63348, train/struct_loss=2.13696, train/feat_loss=0.25761\n",
      "Epoch: 0074, train_loss=0.63347, train/struct_loss=2.13690, train/feat_loss=0.25761\n",
      "Epoch: 0075, train_loss=0.63345, train/struct_loss=2.13679, train/feat_loss=0.25761\n",
      "Epoch: 0076, train_loss=0.63347, train/struct_loss=2.13692, train/feat_loss=0.25760\n",
      "Epoch: 0077, train_loss=0.63350, train/struct_loss=2.13687, train/feat_loss=0.25765\n",
      "Epoch: 0078, train_loss=0.63347, train/struct_loss=2.13676, train/feat_loss=0.25765\n",
      "Epoch: 0079, train_loss=0.63345, train/struct_loss=2.13677, train/feat_loss=0.25762\n",
      "Epoch: 0080, train_loss=0.63347, train/struct_loss=2.13685, train/feat_loss=0.25763\n",
      "Epoch: 0080, Auc: 0.8378954491625784\n",
      "Epoch: 0081, train_loss=0.63347, train/struct_loss=2.13688, train/feat_loss=0.25762\n",
      "Epoch: 0082, train_loss=0.63346, train/struct_loss=2.13679, train/feat_loss=0.25763\n",
      "Epoch: 0083, train_loss=0.63346, train/struct_loss=2.13680, train/feat_loss=0.25763\n",
      "Epoch: 0084, train_loss=0.63341, train/struct_loss=2.13661, train/feat_loss=0.25761\n",
      "Epoch: 0085, train_loss=0.63343, train/struct_loss=2.13664, train/feat_loss=0.25763\n",
      "Epoch: 0086, train_loss=0.63342, train/struct_loss=2.13672, train/feat_loss=0.25760\n",
      "Epoch: 0087, train_loss=0.63343, train/struct_loss=2.13678, train/feat_loss=0.25760\n",
      "Epoch: 0088, train_loss=0.63347, train/struct_loss=2.13693, train/feat_loss=0.25760\n",
      "Epoch: 0089, train_loss=0.63345, train/struct_loss=2.13689, train/feat_loss=0.25759\n",
      "Epoch: 0090, train_loss=0.63345, train/struct_loss=2.13690, train/feat_loss=0.25759\n",
      "Epoch: 0090, Auc: 0.8378616139401117\n",
      "Epoch: 0091, train_loss=0.63350, train/struct_loss=2.13709, train/feat_loss=0.25760\n",
      "Epoch: 0092, train_loss=0.63336, train/struct_loss=2.13642, train/feat_loss=0.25759\n",
      "Epoch: 0093, train_loss=0.63339, train/struct_loss=2.13661, train/feat_loss=0.25758\n",
      "Epoch: 0094, train_loss=0.63330, train/struct_loss=2.13615, train/feat_loss=0.25759\n",
      "Epoch: 0095, train_loss=0.63330, train/struct_loss=2.13622, train/feat_loss=0.25757\n",
      "Epoch: 0096, train_loss=0.63332, train/struct_loss=2.13640, train/feat_loss=0.25755\n",
      "Epoch: 0097, train_loss=0.63329, train/struct_loss=2.13628, train/feat_loss=0.25755\n",
      "Epoch: 0098, train_loss=0.63327, train/struct_loss=2.13611, train/feat_loss=0.25756\n",
      "Epoch: 0099, train_loss=0.63334, train/struct_loss=2.13651, train/feat_loss=0.25755\n",
      "Epoch: 0099, Auc: 0.8378291885185813\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from gad_adversarial_robustness.gad.dominant.dominant import Dominant\n",
    "from gad_adversarial_robustness.utils.graph_utils import load_anomaly_detection_dataset\n",
    "\n",
    "yaml_path = os.path.join(script_dir, '..', 'configs', 'dominant_config.yaml')\n",
    "with open(yaml_path) as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "\"\"\"\n",
    "Dominant on clean data\n",
    "\"\"\"\n",
    "\n",
    "adj, attrs, label, adj_label = load_anomaly_detection_dataset(clean_data)\n",
    "adj = torch.FloatTensor(adj)\n",
    "adj_label = torch.FloatTensor(adj_label)\n",
    "attrs = torch.FloatTensor(attrs)\n",
    "\n",
    "\n",
    "model = Dominant(feat_size=attrs.size(1), hidden_size=config['hidden_dim'], dropout=config['dropout'], \n",
    "                 device=config['device'], adj=adj, adj_label=adj_label, attrs=attrs, label=label)\n",
    "model.fit(config)\n",
    "\n",
    "\"\"\"\n",
    "Dominant on poisoned data\n",
    "\"\"\"\n",
    "\n",
    "adj, attrs, label, adj_label = load_anomaly_detection_dataset(poisoned_data)\n",
    "adj = torch.FloatTensor(adj)\n",
    "adj_label = torch.FloatTensor(adj_label)\n",
    "attrs = torch.FloatTensor(attrs)\n",
    "\n",
    "\n",
    "model = Dominant(feat_size=attrs.size(1), hidden_size=config['hidden_dim'], dropout=config['dropout'], \n",
    "                 device=config['device'], adj=adj, adj_label=adj_label, attrs=attrs, label=label)\n",
    "model.fit(config)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
