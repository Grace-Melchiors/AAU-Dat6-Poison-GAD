{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RTGNN vs. greedy binarized attack model\n",
    "\n",
    "- this model should utilize the greedy binarized attack (white box poison) on a dataset to hide (potential) anomalies or poison in a datset, such as CORA. \n",
    "-   This should produce a dataset which contains hidden anomalies\n",
    "- After obtaining a dataset with disguised poison (hidden anomalies) --> use RTGNN model to create a model that is unaffected/ robust to these anomalies\n",
    "-   Like DOMINANT, RTGNN also makes use of an Autoencoder (encoder - decoder)?\n",
    "-   \n",
    "\n",
    "Goal: See if RTGNN is able to withstand or counteract a binarized attack on the input data\n",
    "\n",
    "trying to figure out whether RTGNN would be useful or not, against a dataset with disguised poison (hidden anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pygod\n",
    "from pygod.utils import load_data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "import torch_sparse\n",
    "from torch_sparse import SparseTensor\n",
    "from typing import List\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# from gad_adversarial_robustness.gad.dominant import dominant\n",
    "from gad_adversarial_robustness.utils.graph_utils import prepare_graph, get_n_anomaly_indexes, load_anomaly_detection_dataset\n",
    "from gad_adversarial_robustness.poison.greedy import multiple_AS, poison_attack\n",
    "\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import coo_matrix\n",
    "from torch_geometric.utils.convert import from_scipy_sparse_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "\n",
    "PRELOADED_EDGE_INDEX = True\n",
    "EDGE_INDEX_PT = \"300_budget_greedy_edge_index.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gad_adversarial_robustness.gad.RTGNN.utils import noisify_with_P\n",
    "from gad_adversarial_robustness.gad.RTGNN.dataset import Dataset\n",
    "from gad_adversarial_robustness.gad.RTGNN.model.RTGNN import RTGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = os.path.abspath('')\n",
    "dataset_caching_path = os.path.join(script_dir, '..', '..', '..', 'data')\n",
    "\n",
    "# import dataset from pygod\n",
    "clean_data: Data = load_data(\"inj_cora\", dataset_caching_path)\n",
    "poisoned_data: Data = load_data(\"inj_cora\", dataset_caching_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 11060], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "print(poisoned_data)\n",
    "\n",
    "# x = [2708, 1433] : our node feature matrix of the shape [number of nodes, number of features]\n",
    "# edge_index = [2, 11060] : our graph connectivity matrix of the shape [2, number of edges]\n",
    "# y = [2708] : the node ground truth labels \n",
    "# train_mask = [2708] : an optional attribute that says which node should be used for training, with a list of True or False statements\n",
    "# etc... in this case, the train_mask, vel_mask and test_mask have the same size...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges are directed: True\n",
      "Graph has isolated nodes: False\n",
      "Graph has loops: False\n"
     ]
    }
   ],
   "source": [
    "print(f'Edges are directed: {poisoned_data.is_directed()}')\n",
    "print(f'Graph has isolated nodes: {poisoned_data.has_isolated_nodes()}')\n",
    "print(f'Graph has loops: {poisoned_data.has_self_loops()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 11060], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "------------\n",
      "Number of graphs: 6\n",
      "Number of features: 1433\n"
     ]
    }
   ],
   "source": [
    "print(poisoned_data)\n",
    "print('------------')\n",
    "print(f'Number of graphs: {len(poisoned_data)}')\n",
    "print(f'Number of features: {poisoned_data.num_features}')\n",
    "# print(f'Number of classes: {poisoned_data.num_classes}') ### does not have an attribute for the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index = torch.Size([2, 11060])\n",
      "tensor([[   0,    0,    0,  ...,  869,  127, 1674],\n",
      "        [ 633, 1862, 2582,  ..., 1732,  214,  438]])\n"
     ]
    }
   ],
   "source": [
    "print(f'edge_index = {poisoned_data.edge_index.shape}')\n",
    "print(poisoned_data.edge_index)\n",
    "\n",
    "# the graph's connections are stored in two lists (11060 directed edges, which equate to 5530 bidirectional edges)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11060])\n",
      "11054\n"
     ]
    }
   ],
   "source": [
    "##### -->  the method of obtaining dense_adj matrix from edge_index tensor #####\n",
    "edge_weight = torch.ones(poisoned_data.edge_index.size(1))\n",
    "edge_weight = edge_weight.cpu()\n",
    "print(edge_weight.shape) # 11060\n",
    "\n",
    "adj = sp.csr_matrix((edge_weight, poisoned_data.edge_index), (poisoned_data.num_nodes, poisoned_data.num_nodes))\n",
    "print(adj.size) # 11054\n",
    "\n",
    "adj = torch_sparse.SparseTensor.from_scipy(adj).coalesce().to(\"cpu\")\n",
    "# adj at this point:\n",
    "#   SparseTensor(row=tensor([   0,    0,    0,  ..., 2707, 2707, 2707]), \n",
    "#       col=tensor([ 633, 1862, 2582,  ...,  598, 1473, 2706]), \n",
    "#       val=tensor([1., 1., 1.,  ..., 1., 1., 1.]), size=(2708, 2708), nnz=11054, density=0.15%)\n",
    "\n",
    "# adj matrix based on edge_index\n",
    "poisoned_data.adj = adj.to_dense() # produces tensor of shape: [2708, 2708]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "\n",
    "# np_adj = poisoned_data.adj.detach().cpu().numpy()\n",
    "# sparse_adj = coo_matrix(np_adj)\n",
    "# edge_index = from_scipy_sparse_matrix(sparse_adj)[0]\n",
    "# edge_index.shape\n",
    "\n",
    "##############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "# # Convert the dense tensor to a sparse tensor\n",
    "# sparse_adj = poisoned_data.adj.to_sparse()\n",
    "# print(\"sparse adj: \",sparse_adj.shape) # shape: [2708, 2708]\n",
    "\n",
    "# # Convert to a sparse COO tensor\n",
    "# sparse_COO = torch.sparse_coo_tensor(sparse_adj._indices(), sparse_adj._values(), sparse_adj.size())\n",
    "# print(\"sparse COO: \",sparse_COO.shape)\n",
    "\n",
    "# # Convert the sparse COO tensor to a dense edge_index tensor\n",
    "# edge_index2 = sparse_COO.to_dense()\n",
    "# print(\"dense coo: \", edge_index2.shape)\n",
    "\n",
    "# # Reshape the edge_index tensor to [2, num_edges]\n",
    "# edge_index2 = edge_index2.view(2, -1)\n",
    "\n",
    "# edge_index2.shape\n",
    "##############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "torch.Size([2708, 2708])\n"
     ]
    }
   ],
   "source": [
    "print(poisoned_data.adj)\n",
    "print(poisoned_data.adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compute new or load in poisoned data\n",
    "if PRELOADED_EDGE_INDEX is False :\n",
    "\n",
    "    # truth, of type int list, is instantiated to the T/F labels indicating whether a node is an anomalous node\n",
    "    truth: List[int] = poisoned_data.y.bool()\n",
    "\n",
    "    print(\"Create poison compatible adjacency matrix...\") # based on code from: https://github.com/zhuyulin-tony/BinarizedAttack/blob/main/src/Greedy.py\n",
    "    triple = []\n",
    "    for i in range(poisoned_data.num_nodes): # Cora has 2708 nodes\n",
    "        for j in range(i + 1, poisoned_data.num_nodes):\n",
    "            triple.append([i, j, poisoned_data.adj[i,j]])  #Fill with 0, then insert actual after\n",
    "\n",
    "    # convert tripple to numpy array\n",
    "    triple = np.array(triple)\n",
    "\n",
    "    # These are the nodes we try reduce the \"active subnetwork score\" for (i.e. disguising anonomalous nodes)\n",
    "    target_node_lst = get_n_anomaly_indexes(truth, 999) # the indexes of the anomalies\n",
    "\n",
    "    # print(type(target_node_lst)), print(f'target node list: {target_node_lst}'), print(target_node_lst)\n",
    "\n",
    "    print(\"Making model...\")\n",
    "    model = multiple_AS(target_lst = target_node_lst, n_node = poisoned_data.num_nodes, device = 'cpu')\n",
    "    budget = 100  # The amount of edges to change\n",
    "\n",
    "\n",
    "    print(\"Starting attack...\")\n",
    "    adj_adversary, _, _ = poison_attack(model, triple, budget)\n",
    "\n",
    "\n",
    "    print(\"Converting to compatible tensor...\")\n",
    "\n",
    "    # Create Edge Index'\n",
    "    edge_index = torch.tensor([[],[]])\n",
    "\n",
    "    # Transpose it to make shape compatible\n",
    "    transposed_adj_adversary = torch.transpose(adj_adversary, 0, 1)\n",
    "\n",
    "    for i in range(len(adj_adversary)):\n",
    "        if(adj_adversary[i][2] != 0):   #If edge value is not 0 (no edge)\n",
    "            #Add edge to edge index, choosing first 2 elements (edges), and then the ith edge\n",
    "            edge_index = torch.cat((edge_index, transposed_adj_adversary[:2, i:i+1]), -1)\n",
    "            # Dataset uses edges both ways so add reverse edge as well\n",
    "            edge_index = torch.cat((edge_index, torch.flip(transposed_adj_adversary[:2, i:i+1], dims=[0])), -1)\n",
    "\n",
    "\n",
    "    edge_index = edge_index.type(torch.int64)\n",
    "    poisoned_data.edge_index = edge_index # assign to dataset obj\n",
    "\n",
    "else : \n",
    "    poisoned_data.edge_index = torch.load(EDGE_INDEX_PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10486], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], adj=[2708, 2708])\n",
      "Data(x=[2708, 1433], edge_index=[2, 11060], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "print(poisoned_data)\n",
    "print(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RTGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--co_lambda'], dest='co_lambda', nargs=None, const=None, default=0.1, type=<class 'float'>, choices=None, required=False, help='weight for consistency regularization term', metavar=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the args for the model\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "######## the following are the parameters defined in the RTGNN main file ########\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=12, help='Random seed.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=128,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--edge_hidden', type=int, default=64,\n",
    "                    help='Number of hidden units of MLP graph constructor')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "# parser.add_argument('--dataset', type=str, default=\"cora\",\n",
    "#                     choices=['cora', 'citeseer','blogcatalog'], help='dataset')\n",
    "parser.add_argument('--ptb_rate', type=float, default=0.3,\n",
    "                    help=\"noise ptb_rate\")\n",
    "parser.add_argument('--epochs', type=int,  default=200,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.001,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--alpha', type=float, default=1,\n",
    "                    help='loss weight of graph reconstruction')\n",
    "parser.add_argument('--tau',type=float, default=0.05,\n",
    "                    help='threshold of filtering noisy edges')\n",
    "parser.add_argument('--th',type=float, default=0.95,\n",
    "                    help='threshold of adding pseudo labels')\n",
    "parser.add_argument(\"--K\", type=int, default=100,\n",
    "                    help='number of KNN search for each node')\n",
    "parser.add_argument(\"--n_neg\", type=int, default=100,\n",
    "                    help='number of negitive sampling for each node')\n",
    "parser.add_argument('--noise', type=str, default='uniform', choices=['uniform', 'pair'],\n",
    "                    help='type of noises')\n",
    "parser.add_argument('--decay_w', type=float, default=0.1,\n",
    "                    help='down-weighted factor')\n",
    "parser.add_argument('--co_lambda',type=float,default=0.1,\n",
    "                     help='weight for consistency regularization term')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(seed=12, weight_decay=0.0005, hidden=128, edge_hidden=64, dropout=0.5, ptb_rate=0.3, epochs=200, lr=0.001, alpha=1, tau=0.05, th=0.95, K=100, n_neg=100, noise='uniform', decay_w=0.1, co_lambda=0.1)\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_known_args()[0]\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "# data = Dataset(root='./data', name=args.dataset)\n",
    "\n",
    "### insert the data that was attacked by greedy BinarizedAttack\n",
    "data = poisoned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize adj matrix, node features and labels\n",
    "adj = data.adj\n",
    "features = data.x\n",
    "labels = data.y.bool() # converts to true / false\n",
    "\n",
    "# noise perturbation rate --> it controls the amount of noise added to the training and validation labels\n",
    "ptb = args.ptb_rate \n",
    "\n",
    "# initalize number of classes in dataset ---> hardcoded\n",
    "nclass = NUM_CLASSES + 1 \n",
    "args.class_num=nclass\n",
    "\n",
    "\n",
    "#### train validation test split\n",
    "\n",
    "# Assuming data.y.shape == (2708,)\n",
    "num_nodes = data.y.shape[0]\n",
    "\n",
    "# --- create new masks with 6/2/2 split ---\n",
    "    # Determine the split ratios (e.g., 0.6, 0.2, 0.2)\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "\n",
    "    # Create a random permutation of node indices\n",
    "node_indices = torch.randperm(num_nodes)\n",
    "\n",
    "    # Calculate the split indices\n",
    "train_size = int(num_nodes * train_ratio)\n",
    "val_size = int(num_nodes * val_ratio)\n",
    "test_size = num_nodes - train_size - val_size\n",
    "\n",
    "    # Create new masks based on the split indices\n",
    "new_train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "new_train_mask[node_indices[:train_size]] = True\n",
    "\n",
    "new_val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "new_val_mask[node_indices[train_size:train_size+val_size]] = True\n",
    "\n",
    "new_test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "new_test_mask[node_indices[train_size+val_size:]] = True\n",
    "\n",
    "# Assign the new masks to the data object\n",
    "data.train_mask = new_train_mask\n",
    "data.val_mask = new_val_mask\n",
    "data.test_mask = new_test_mask\n",
    "\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# Extract the new indices for the training, validation, and test sets\n",
    "idx_train = np.where(data.train_mask == True)[0]\n",
    "idx_val = np.where(data.val_mask == True)[0]\n",
    "idx_test = np.where(data.test_mask == True)[0]\n",
    "\n",
    "# from the RTGNN github ----------------------------------------------\n",
    "train_labels = labels[idx_train]\n",
    "val_labels = labels[idx_val]\n",
    "\n",
    "# Concatenating training and validation labels\n",
    "train_val_labels = np.concatenate([train_labels, val_labels],axis=0)\n",
    "\n",
    "# Concatenating training and validation masks\n",
    "idx = np.concatenate([idx_train, idx_val],axis=0)\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# OLD CODE ----------------------------------------------------------------\n",
    "# # Extracting labels for training, validation, and test data using masks\n",
    "# train_labels = labels[data.train_mask]\n",
    "# val_labels = labels[data.val_mask]\n",
    "# test_labels = labels[data.test_mask]\n",
    "\n",
    "# # Concatenating training and validation labels\n",
    "# train_val_labels = torch.cat([train_labels, val_labels], dim=0)\n",
    "\n",
    "# # Concatenating training and validation masks\n",
    "# idx = torch.cat([data.train_mask, data.val_mask], dim=0)\n",
    "\n",
    "# # convert tensors to numpy arrays\n",
    "# train_val_labels = train_val_labels.cpu().detach().numpy()\n",
    "# idx = idx.cpu().detach().numpy()\n",
    "\n",
    "# # convert the T/F labels to integers --> used in the following call of \"noisyfy_with_P\"\n",
    "train_val_labels_int = train_val_labels.astype(int)\n",
    "# idx_int = idx.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1    3    5 ... 2687 2691 2698]\n",
      "(2165,)\n",
      "[False False False ... False False False]\n",
      "(2165,)\n",
      "torch.Size([2708])\n",
      "torch.Size([2708])\n",
      "\n",
      "[   1    3    5 ... 2703 2705 2707]\n",
      "(1624,)\n"
     ]
    }
   ],
   "source": [
    "print(idx)\n",
    "print(idx.shape)\n",
    "print(train_val_labels)\n",
    "print(train_val_labels.shape)\n",
    "print(data.val_mask.shape)\n",
    "print(data.train_mask.shape)\n",
    "\n",
    "print()\n",
    "\n",
    "print(idx_train)\n",
    "print(idx_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform noise\n",
      "probability matrix P:\n",
      " [[0.7  0.15 0.15]\n",
      " [0.15 0.7  0.15]\n",
      " [0.15 0.15 0.7 ]]\n",
      "count: 651\n"
     ]
    }
   ],
   "source": [
    "# Adding noise to the concatenated labels and getting the noise indices and clean indices\n",
    "# noise_y, P, noise_idx, clean_idx = noisify_with_P(train_val_labels, data.train_mask.shape[0], nclass, ptb, 10, args.noise)\n",
    "noise_y, P, noise_idx, clean_idx = noisify_with_P(train_val_labels_int, idx_train.shape[0], nclass, ptb, 10, args.noise)\n",
    "\n",
    "args.noise_idx, args.clean_idx = noise_idx, clean_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_labels = labels.to(torch.int).clone() # size = 2708\n",
    "\n",
    "# size of noisy_y = 2165 --> which is the total amount of labels (2708) minus the test_labels (543)\n",
    "# # set the noisy labels\n",
    "# for i in range(noise_y.shape[0]):\n",
    "    \n",
    "#     noise_labels[idx[i]] = noise_y[i]\n",
    "\n",
    "# set the noisy labels (need to convert from np to torch tensor)\n",
    "noise_labels[idx] = torch.from_numpy(noise_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "# torch.cuda.manual_seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "\n",
    "# instatiate the model\n",
    "model = RTGNN(args, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the dense tensor to a sparse tensor\n",
    "# sparse_adj = adj.to_sparse()\n",
    "\n",
    "# # Convert to a sparse COO tensor\n",
    "# sparse_COO = torch.sparse_coo_tensor(sparse_adj._indices(), sparse_adj._values(), adj.size())\n",
    "\n",
    "# # Convert the sparse COO tensor to a dense edge_index tensor\n",
    "# edge_index = sparse_COO.to_dense()\n",
    "\n",
    "# # Reshape the edge_index tensor to [2, num_edges]\n",
    "# edge_index = edge_index.view(2, -1)\n",
    "\n",
    "# edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj = adj.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index: \n",
      " torch.Size([2, 11054])\n",
      "=====Train Accuray=====\n",
      "Epoch 0: #1 = 0.501232, #2= 0.479064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karen\\anaconda3\\envs\\PyG\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [281854]], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# train --------------------------------------------------\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# model fit : features, adj, noise_labels, real labels, \u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_idx\u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[1;32mc:\\p6-workspaces\\p6-repo\\aau-dat6-poison-gad\\gad_adversarial_robustness\\gad\\RTGNN\\model\\RTGNN.py:205\u001b[0m, in \u001b[0;36mRTGNN.fit\u001b[1;34m(self, features, adj, labels, true_labels, idx_train, idx_val, noise_idx, clean_idx)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mparameters()),\n\u001b[0;32m    202\u001b[0m                             lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mweight_decay)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimization Finished!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpicking the best model according to validation performance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\p6-workspaces\\p6-repo\\aau-dat6-poison-gad\\gad_adversarial_robustness\\gad\\RTGNN\\model\\RTGNN.py:262\u001b[0m, in \u001b[0;36mRTGNN.train\u001b[1;34m(self, epoch, features, edge_index, idx_train, idx_val, noise_idx, clean_idx)\u001b[0m\n\u001b[0;32m    260\u001b[0m neighbor_kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintra_reg(log_pred,log_pred_1,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx_train, pred_edge_index,predictor_weights)\n\u001b[0;32m    261\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m loss_pred \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m rec_loss \u001b[38;5;241m+\u001b[39m loss_add \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mco_lambda\u001b[38;5;241m*\u001b[39m(neighbor_kl_loss)\n\u001b[1;32m--> 262\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\karen\\anaconda3\\envs\\PyG\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\karen\\anaconda3\\envs\\PyG\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [281854]], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "# train --------------------------------------------------\n",
    "# model fit : features, adj, noise_labels, real labels, \n",
    "model.fit(features, adj, noise_labels, labels, idx_train, idx_val, noise_idx, clean_idx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model test\n",
    "model.test(idx_test) # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
