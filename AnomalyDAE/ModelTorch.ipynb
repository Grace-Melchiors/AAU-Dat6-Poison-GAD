{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.utils as pyg_utils\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch_geometric.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the [0] at the end is used to access the attributes\n",
    "data = Planetoid('./data/Cora', 'Cora', transform=T.NormalizeFeatures())[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset name ('Cora', 'CiteSeer', or 'PubMed')\n",
    "dataset_name = 'Cora'\n",
    "\n",
    "# Load the dataset\n",
    "dataset = Planetoid(root='./data', name=dataset_name)\n",
    "\n",
    "# Access the dataset attributes\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_nodes': 2708, 'num_edges': 10556, 'num_edge_features': 0, 'num_node_features': 1433, 'num_node_classes': 7}\n"
     ]
    }
   ],
   "source": [
    "# Dataset information\n",
    "# print(f'Dataset: {dataset_name}')\n",
    "# print(f'Number of nodes: {data.num_nodes}')\n",
    "# print(f'Number of edges: {data.num_edges}')\n",
    "# print(f'Number of node features: {data.num_node_features}')\n",
    "# print(f'Number of node classes: {dataset.num_classes}')\n",
    "\n",
    "data_details = {\n",
    "    \"num_nodes\" : data.num_nodes, \n",
    "    \"num_edges\" : data.num_edges, \n",
    "    \"num_edge_features\": dataset.num_edge_features,\n",
    "    \"num_node_features\": dataset.num_node_features,\n",
    "    \"num_node_classes\": dataset.num_classes   \n",
    "}\n",
    "\n",
    "print(data_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Visualization ####################\n",
    "# Convert PyTorch Geometric data to NetworkX graph ---\n",
    "\n",
    "# graph = pyg_utils.to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Draws the graph ----\n",
    "\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# nx.draw(graph, node_size=10, node_color='b', edge_color='gray', with_labels=False)\n",
    "# plt.title('CORA Dataset Graph')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnomalyDAE (Anomaly Detection Autoencoder) model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Data preprocessing: Convert the PyTorch Geometric Data object into a format suitable for training an autoencoder. \n",
    "\n",
    "# 2) Define the Autoencoder Architecture: Define the architecture of the AnomalyDAE model. This typically involves defining the encoder and decoder networks.\n",
    "\n",
    "# 3) Loss function: Define a suitable loss function for training the AnomalyDAE model. \n",
    "\n",
    "# 4) Training: Train the AnomalyDAE model using the CORA dataset. \n",
    "\n",
    "# 5) Anomaly Detection: Use the trained AnomalyDAE model to detect anomalies in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygod.generator import gen_contextual_outlier, gen_structural_outlier\n",
    "\n",
    "data, ya = gen_contextual_outlier(data, n=100, k=50)\n",
    "data, ys = gen_structural_outlier(data, m=10, n=10)\n",
    "data.y = torch.logical_or(ys, ya).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sigmoid\n",
    "from pygod import detector\n",
    "from pygod.detector import AnomalyDAE\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from pygod.detector import DeepDetector\n",
    "from pygod.nn import AnomalyDAEBase\n",
    "\n",
    "# # from: https://docs.pygod.org/en/latest/generated/pygod.detector.AnomalyDAE.html\n",
    "# class AnomalyDAE(DeepDetector):\n",
    "#     def __init__(self,\n",
    "#                  emb_dim=64,\n",
    "#                  hid_dim=64,\n",
    "#                  num_layers=4,\n",
    "#                  dropout=0.,\n",
    "#                  weight_decay=0.,\n",
    "#                  act=F.relu,\n",
    "#                  backbone=None,\n",
    "#                  alpha=0.5,\n",
    "#                  theta=1.,\n",
    "#                  eta=1.,\n",
    "#                  contamination=0.1,\n",
    "#                  lr=0.004,\n",
    "#                  epoch=5,\n",
    "#                  gpu=-1,\n",
    "#                  batch_size=0,    \n",
    "#                  num_neigh=-1,\n",
    "#                  verbose=0,\n",
    "#                  save_emb=False,\n",
    "#                  compile_model=False,\n",
    "#                  **kwargs):\n",
    "\n",
    "#         if backbone is not None or num_layers != 4:\n",
    "#             warnings.warn(\"Backbone and num_layers are not used in AnomalyDAE\")\n",
    "\n",
    "#         super(AnomalyDAE, self).__init__(hid_dim=hid_dim,\n",
    "#                                          num_layers=num_layers,\n",
    "#                                          dropout=dropout,\n",
    "#                                          weight_decay=weight_decay,\n",
    "#                                          act=act,\n",
    "#                                          backbone=backbone,\n",
    "#                                          contamination=contamination,\n",
    "#                                          lr=lr,\n",
    "#                                          epoch=epoch,\n",
    "#                                          gpu=gpu,\n",
    "#                                          batch_size=batch_size,\n",
    "#                                          num_neigh=num_neigh,\n",
    "#                                          verbose=verbose,\n",
    "#                                          save_emb=save_emb,\n",
    "#                                          compile_model=compile_model,\n",
    "#                                          **kwargs)\n",
    "\n",
    "#         self.emb_dim = emb_dim\n",
    "#         self.alpha = alpha\n",
    "#         self.theta = theta\n",
    "#         self.eta = eta\n",
    "\n",
    "#     def process_graph(self, data):\n",
    "#         AnomalyDAEBase.process_graph(data)\n",
    "\n",
    "#     def init_model(self, **kwargs):\n",
    "#         if self.save_emb:\n",
    "#             self.emb = torch.zeros(self.num_nodes,\n",
    "#                                    self.hid_dim)\n",
    "\n",
    "#         return AnomalyDAEBase(in_dim=self.in_dim,\n",
    "#                               num_nodes=self.num_nodes,\n",
    "#                               emb_dim=self.emb_dim,\n",
    "#                               hid_dim=self.hid_dim,\n",
    "#                               dropout=self.dropout,\n",
    "#                               act=self.act,\n",
    "#                               **kwargs).to(self.device)\n",
    "\n",
    "#     def forward_model(self, data):\n",
    "#         # batch_size = data.batch_size              # changed from \"data.batch_size\".. assuming it was refering to the data points it should be including\n",
    "#         node_idx = data.n_id                      # not sure what n_id or node_idx is..?\n",
    "#         batch_size = self.batch_size    \n",
    "#         # node_idx = data.test_mask            \n",
    "\n",
    "\n",
    "\n",
    "#         x = data.x.to(self.device)                  # not sure what this is trying to pull from the data\n",
    "#         s = data.s.to(self.device)                  # not sure what this is trying to pull from the data either...\n",
    "#         edge_index = data.edge_index.to(self.device)\n",
    "\n",
    "#         x_, s_ = self.model(x, edge_index, batch_size)\n",
    "\n",
    "#         # positive weight conversion\n",
    "#         weight = 1 - self.alpha\n",
    "#         pos_weight_a = self.eta / (1 + self.eta)\n",
    "#         pos_weight_s = self.theta / (1 + self.theta)\n",
    "\n",
    "#         score = self.model.loss_func(x[:batch_size],\n",
    "#                                      x_[:batch_size],\n",
    "#                                      s[:batch_size, node_idx],  # batch size and node_idx used here\n",
    "#                                      s_[:batch_size],\n",
    "#                                      weight,\n",
    "#                                      pos_weight_a,\n",
    "#                                      pos_weight_s)\n",
    "\n",
    "#         loss = torch.mean(score)\n",
    "\n",
    "#         return loss, score.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "'NeighborSampler' requires either 'pyg-lib' or 'torch-sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AnomalyDAE()\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# ---------- having trouble running the statement below, bc I need to resolve the uncertainties about what the code is referencing from the dataset above\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#                   Possibly reference: https://github.com/haoyfan/AnomalyDAE/blob/master/src/run.py   for a solution\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# model.forward_model(data)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pygod\\detector\\base.py:461\u001b[0m, in \u001b[0;36mDeepDetector.fit\u001b[1;34m(self, data, label)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgan:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_loss_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 461\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sampled_data \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m    462\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m sampled_data\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m    463\u001b[0m     node_idx \u001b[38;5;241m=\u001b[39m sampled_data\u001b[38;5;241m.\u001b[39mn_id\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch_geometric\\loader\\node_loader.py:134\u001b[0m, in \u001b[0;36mNodeLoader.collate_fn\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Samples a subgraph from a batch of input nodes.\"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m input_data: NodeSamplerInput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_data[index]\n\u001b[1;32m--> 134\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_sampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_per_worker:  \u001b[38;5;66;03m# Execute `filter_fn` in the worker process\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_fn(out)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch_geometric\\sampler\\neighbor_sampler.py:246\u001b[0m, in \u001b[0;36mNeighborSampler.sample_from_nodes\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_from_nodes\u001b[39m(\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    244\u001b[0m     inputs: NodeSamplerInput,\n\u001b[0;32m    245\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[SamplerOutput, HeteroSamplerOutput]:\n\u001b[1;32m--> 246\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mnode_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubgraph_type \u001b[38;5;241m==\u001b[39m SubgraphType\u001b[38;5;241m.\u001b[39mbidirectional:\n\u001b[0;32m    248\u001b[0m         out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto_bidirectional()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch_geometric\\sampler\\neighbor_sampler.py:504\u001b[0m, in \u001b[0;36mnode_sample\u001b[1;34m(inputs, sample_fn)\u001b[0m\n\u001b[0;32m    501\u001b[0m     seed \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mnode\n\u001b[0;32m    502\u001b[0m     seed_time \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mtime\n\u001b[1;32m--> 504\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43msample_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m out\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m (inputs\u001b[38;5;241m.\u001b[39minput_id, inputs\u001b[38;5;241m.\u001b[39mtime)\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch_geometric\\sampler\\neighbor_sampler.py:421\u001b[0m, in \u001b[0;36mNeighborSampler._sample\u001b[1;34m(self, seed, seed_time, **kwargs)\u001b[0m\n\u001b[0;32m    418\u001b[0m     num_sampled_nodes \u001b[38;5;241m=\u001b[39m num_sampled_edges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m requires \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    422\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meither \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyg-lib\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch-sparse\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m SamplerOutput(\n\u001b[0;32m    425\u001b[0m     node\u001b[38;5;241m=\u001b[39mnode,\n\u001b[0;32m    426\u001b[0m     row\u001b[38;5;241m=\u001b[39mrow,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    431\u001b[0m     num_sampled_edges\u001b[38;5;241m=\u001b[39mnum_sampled_edges,\n\u001b[0;32m    432\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: 'NeighborSampler' requires either 'pyg-lib' or 'torch-sparse'"
     ]
    }
   ],
   "source": [
    "from pygod.detector import AnomalyDAE\n",
    "import torch.sparse \n",
    "\n",
    "model = AnomalyDAE()\n",
    "\n",
    "model.fit(data=data)\n",
    "\n",
    "# ---------- having trouble running the statement below, bc I need to resolve the uncertainties about what the code is referencing from the dataset above\n",
    "#                   Possibly reference: https://github.com/haoyfan/AnomalyDAE/blob/master/src/run.py   for a solution\n",
    "\n",
    "# model.forward_model(data)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
